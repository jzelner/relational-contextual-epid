[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Field Guide to Relational and Systems Epidemiology",
    "section": "",
    "text": "1 Inspirations and Influences\nWhat - if any - kind of book needs to be written about relational and systems epidemiology?"
  },
  {
    "objectID": "index.html#why-relational-and-contextual-epidemiology",
    "href": "index.html#why-relational-and-contextual-epidemiology",
    "title": "Field Guide to Relational and Systems Epidemiology",
    "section": "1.1 Why relational and contextual epidemiology?",
    "text": "1.1 Why relational and contextual epidemiology?\nThere is a hole in the epidemiological literature that limits our ability to come to grips with the relational aspects of health and illness. For the purposes of this book, relationships are sources of non-independence. These could be social influences, e.g. from our friends and family. But they could also be spatial in nature, e.g. a result of some physically proximate environmental exposure. Often, the relationships we care about are temporal in nature, as is evident in the lifecourse perspective in which early-life exposures are understood to impact later-life outcomes.\nThere are many good books and papers out there about these topics as well as some of their conjunctions with each other (e.g. spatiotemporal analysis). But there is less written about their overlaps and commonalities, why knowing one teaches you so much about the others, and what having access to this set of tools might mean for a working epidemiologist.\nIn many ways, my goal in writing this is selfish, to satisfy a personal curiosity I suspect is of some greater import to at least a few people. Specifically: What does it mean to do contextual or relational epidemiology? Are these meaningless terms that just appeal to the social and natural science centers of my brain, or does this tap into something more meaningful?\n\nMore than methods\nPart of my inspiration is a reaction to the technocratic impulse and imperative in modern epidemiology. In particular, I find myself a bit paralyzed by the worry about what happens when we operate under the assumption that escalating methodological complexity is an imperative and that the road out of socio-epidemiological problems is paved with technological solutions.\n\n\nBut also, methods…\nOn the other side, simply put, I love the methodological tools of spatial epidemiology, Bayesian hierarchical analysis, and systems modeling. I have learned more than I ever could have hoped through learning, tinkering with, and applying these tools to problems in the real world and in my own head. But for me, the large majority of lessons learned have been from their conceptual isomorphisms (or conflicts) with the world as it appears to us through qualitative and quantitative data, rather than in the exact values of their parameter values and quantitative predictions.\nFor me, this book is about resolving this cognitive dissonance while providing useful ‘how-to’ pointers along the way. I hope to articulate the affirmative case for a systems-based, contextually-sensitive, justice-oriented, morally and ethically opinionated, and theoretically driven approach to epidemiology. Along the way, I hope to show why the tools of such an approach are necessarily heterogeneous in nature and require us to accept uncertaintities quantitative and epistemic."
  },
  {
    "objectID": "index.html#influences",
    "href": "index.html#influences",
    "title": "Field Guide to Relational and Systems Epidemiology",
    "section": "1.2 Influences",
    "text": "1.2 Influences\nThere are any number of books and papers out there that have articulated a similar perspective on the tools of quantitative analysis. The following have been particularly important for my own thinking and their influences will be felt throughout this work:\n\nStatistical Rethinking (1)\nThe Ecological Detective (2)\nARM/Regression and Other Stories (3)\n\nWhat makes these works so useful, strong, and enduring is the way that they articulate a coherent, opinionated perspective on the meaning and use of a set of methodological tools. On top of that, they are engaging and fun to read - the sort of thing you return to over time not just to get specific methodological tools, but to be exposed to their perspective."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Field Guide to Relational and Systems Epidemiology",
    "section": "References",
    "text": "References\n\n\n\n\n1. McElreath R. Statistical Rethinking: A Bayesian Course with Examples in R and STAN. 2nd edition. Boca Raton: Chapman and Hall/CRC; 2020.\n\n\n2. Hilborn R, Mangel M. The Ecological Detective: Confronting Models with Data. 1st edition. Princeton, NJ: Princeton University Press; 1997.\n\n\n3. Gelman A. Regression and Other Stories. 1st edition. Cambridge: Cambridge University Press; 2020."
  },
  {
    "objectID": "invitation.html#you-are-almost-certainly-a-relational-analyst-already",
    "href": "invitation.html#you-are-almost-certainly-a-relational-analyst-already",
    "title": "2  An invitation",
    "section": "2.1 You are (almost certainly) a relational analyst already",
    "text": "2.1 You are (almost certainly) a relational analyst already\nMy goal in this short essay is to chip away a bit at the idea of spatial/relational epidemiology as something separate and apart from mainstream epidemiology and public health. Instead, I argue that these are better understood as a loose wrapper around a core set of ideas and tools that are part of the working arsenal of most professionals, students, and researchers in public health.\nTo explain what I mean, and why I think it’s important, I’m going to subject you to a bit of autobiography about my own intellectual and professional trajectory."
  },
  {
    "objectID": "invitation.html#maps-gateway-or-destination",
    "href": "invitation.html#maps-gateway-or-destination",
    "title": "2  An invitation",
    "section": "2.2 Maps: Gateway or destination?",
    "text": "2.2 Maps: Gateway or destination?\nI began working as an epidemiologist or epidemology-adjacent type as a PhD student at the University of Michigan some time around 2006. As part of my dissertation research, I worked with Joe Eisenberg on an analysis ofthe role of social networks as sources of risk and protection against diarrheal disease and other infections in a group of villages in an area of rural Ecuador:\n\n\n\nFigure 2.2: A figure from my dissertation research representing hypothesized relationships between village context (represented by inaccessibility or ‘remoteness’) and variation in disease outcomes within and between villages. (From (1))\n\n\nLooking back, this was indisputably a spatial analysis: We were interested in how local social contexts impacted variation in health outcomes across a set of 20+ villages and also how within-village variation in social connectivity impacted risk within villages.\nWe employed multi-level data about common characteristics of individual villages, households, and the individuals within them. But at this time, I thought of myself as doing a few things, but none of them were spatial:\n\nInfectious disease epidemiology: Why and how do people become infected with various pathogens?\nSocial epidemiology: How do social relationships impact disease outcomes?\nNetwork analysis: How does the structure of relationships impact individual and community health?\n\nAs it happened, all of these things were correct. But what I didn’t really understand at the time was that the collection of these different approaches into a single analysis made it spatial or geographic in nature, even if I didn’t realize it"
  },
  {
    "objectID": "invitation.html#a-process-of-progressive-revelation",
    "href": "invitation.html#a-process-of-progressive-revelation",
    "title": "2  An invitation",
    "section": "2.3 A process of progressive revelation",
    "text": "2.3 A process of progressive revelation\nAs a postdoc, working with Ted Cohen, I began analyzing data from a large study of household-level tuberculosis transmission in Lima, Peru. Figure 2.3 illustrates the model we developed to characterize household-level differences in transmission rates as a function of exposure type:\n\n\n\nFigure 2.3: Characterizing household-level variation in risks of infection from community vs. household exposures (Figure from (2))\n\n\nAt the time, I knew that these households were distributed across neighborhoods of Lima, but I didn’t give it much thought. I was more interested in risks experienced by an average household. And to be honest, I didn’t know that spatial metadata were available on each of the households, since I wasn’t involved with the data collection!\nIn the interim, I got the chance to work on some collaborative projects with an explicitly spatial focus. In one, we reconstructed an outbreak of morbillivirus (think: measles) among a herd of migratory dolphins (Figure 2.4).\n\n\n\nFigure 2.4: Locations of dolphin strandings during a morbillivirus outbreak in the North Atlantic (dot size indicates a greater number of strandings; Figure from (3))\n\n\nIn another, we looked at the relationship between environmental risks, such as neighborhood-level flooding, on the rate of pediatric diarrheal disease in Ho Chi Minh City in Vietnam (Figure 2.5).\n\n\n\nFigure 2.5: Incidence of pediatric diarrhea across neighborhoods of Ho Chi Minh City, Vietnam (Figure from (4))\n\n\nThese were the first experiences I had explicitly looking at these outcomes as a function of geographic space. While I had previously thought that mapping and spatial analysis and health geography were big scary things I couldn’t do, I started to realize something important: These projects were not substantively very distinct from ones I had done before. The difference was that we were explicitly talking about spatial relationships and making maps (or simple one-dimensional diagrams as in Figure 2.4), instead of implicitly as in Figure 2.2 or Figure 2.3.\nAfter completing these other projects, I dove back in to the Lima TB data to look at the drivers of multi-drug resistant TB (MDR-TB) risk. This was when I finally found out (some 2 years after I had started working with these data!) that spatial information on each household was available. So, with great trepidation, for the first time I made a map to explore spatial variability in MDR-TB outcomes.\nAnd when I did this, we instantly saw that there were seemingly meaningful differences in the rate of TB overall, and MDR-TB in particular, across different health center catchment areas:\n\n\n\nFigure 2.6: The first map(s) I ever made, from (5), nearly 10 years after I started my research career.\n\n\nThis was the moment, some 10 years after I dipped my toes into the world of infectious disease epidemiology, where I realized I had been doing spatial work all along."
  },
  {
    "objectID": "invitation.html#so-what",
    "href": "invitation.html#so-what",
    "title": "2  An invitation",
    "section": "2.4 So what?",
    "text": "2.4 So what?\nWhy am I bothering you with this tedious and indulgent bit of personal history? It’s because it took me way too long to recognize that spatial epidemiology was a wrapper around a set of skills and ideas I had been working with for many years before I recognized what I was doing. I was intimidated by anything preceded by ‘spatial-’: it sounded like a bunch of skills I didn’t have and couldn’t acquire.\nMy belated realization about the emergent quality of spatial epidemiology, and its broader connection to relational thinking in public health and the social sciences, has been crucially important for me. It made me realize that when I push into new areas - in life as much as research - that I probably have more of the tools I need than I realized in advance.\nThis means that you don’t need to identify as a spatial or network or time series analyst to be one. And if you want to think of yourself as one, you should, because ultimately it is the intention to engage with the spatial, social and temporal relationships that drive health outcomes that makes the difference. This is likely true for many if not most scientific subfields1, but this one is mine and I’m glad I finally realized it!"
  },
  {
    "objectID": "invitation.html#references",
    "href": "invitation.html#references",
    "title": "2  An invitation",
    "section": "References",
    "text": "References\n\n\n\n\n1. Zelner JL, Trostle J, Goldstick JE, et al. Social Connectedness and Disease Transmission: Social Organization, Cohesion, Village Context, and Infection Risk in Rural Ecuador. American Journal of Public Health [electronic article]. 2012;102(12):2233–2239. (http://ajph.aphapublications.org/doi/10.2105/AJPH.2012.300795). (Accessed December 15, 2019)\n\n\n2. Zelner JL, Murray MB, Becerra MC, et al. Age-Specific Risks of Tuberculosis Infection From Household and Community Exposures and Opportunities for Interventions in a High-Burden Setting. American Journal of Epidemiology [electronic article]. 2014;180(8):853–861. (https://academic.oup.com/aje/article-lookup/doi/10.1093/aje/kwu192). (Accessed December 15, 2019)\n\n\n3. Morris SE, Zelner JL, Fauquier DA, et al. Partially observed epidemics in wildlife hosts: Modelling an outbreak of dolphin morbillivirus in the northwestern Atlantic, June 2013. Journal of The Royal Society Interface [electronic article]. 2015;12(112):20150676. (https://royalsocietypublishing.org/doi/10.1098/rsif.2015.0676). (Accessed December 15, 2019)\n\n\n4. Thompson CN, Zelner JL, Nhu TDH, et al. The impact of environmental and climatic variation on the spatiotemporal trends of hospitalized pediatric diarrhea in Ho Chi Minh City, Vietnam. Health & Place [electronic article]. 2015;35:147–154. (https://linkinghub.elsevier.com/retrieve/pii/S1353829215001094). (Accessed December 15, 2019)\n\n\n5. Zelner JL, Murray MB, Becerra MC, et al. Identifying Hotspots of Multidrug-Resistant Tuberculosis Transmission Using Spatial and Molecular Genetic Data. Journal of Infectious Diseases [electronic article]. 2016;213(2):287–294. (https://academic.oup.com/jid/article-lookup/doi/10.1093/infdis/jiv387). (Accessed December 15, 2019)"
  },
  {
    "objectID": "invitation.html#footnotes",
    "href": "invitation.html#footnotes",
    "title": "2  An invitation",
    "section": "",
    "text": "To be fair, you probably need to be working near-ish to a field for it to happen by chance: there is little chance of me taking on the characteristics a particle physicist or chemical engineer by chance, but I wouldn’t rule out lepidopterist or archaeologist entirely!↩︎"
  },
  {
    "objectID": "elements.html",
    "href": "elements.html",
    "title": "3  What are the elements of relational and systems epidemiology?",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "elements.html#systems-epidemiology",
    "href": "elements.html#systems-epidemiology",
    "title": "3  What are the elements of relational and systems epidemiology?",
    "section": "3.1 Systems Epidemiology",
    "text": "3.1 Systems Epidemiology\nIn the context of this book, a systems approach to epidemiology refers not only to systems methods but a systems orientation. But what does that mean?\nOften, in public health and epidemiology, our focus is on an outcome of interest such as the incidence of a particular disease, the rate of death following infection with a particular pathogen or a cancer diagnosis, or some other well-defined, clearly-specified endpoint. Obviously, we do this for a good reason: We care about preventing or treating these specific outcomes, and we couldn’t do a very good job of measuring risk and assessing intervention impact without thinking in these specific terms.\nSo, while we should care about enumerating and addressing these specific outcomes, the systems that generate them are not sui generis, i.e. they do not impact only one health or disease outcome at a time. Orienting towards systems means giving them the sort of in-depth attention we give to the outcomes. This approach pays enormous dividends: When we start from understanding a social or environmental system - for example, structural racism or anthropogenic climate change - that impacts multiple health outcomes, we are able to increase our overall health impact by intervening further up the causal chain.\nAnother thing that happens when we move away from outcome orientation is that we are forced to reckon with relationships and feedback loops in a more comprehensive way. This is because if we care about the behavior of the epidemiologic system, we care about how it changes over time, how it responds to different attempts to alter its behavior, and on and on. By contrast, an outcome is by definition an endpoint. When we take responsibility for characterizing and addressing the system, each outcome is a realization of a process of interest, and another piece of information that gives us a clue about how the system generating these outcomes behaves.\nWe have a tendency to conflate specific orientations with specific analytic methods: If we care about systems, then we must use dynamic models or network methods. While these tools may be more likely to show up in system-oriented analyses than in outcome-oriented ones, I would like to take this opportunity to reject this idea uncategorically. Methods are just tools of measurement and summarization that help us understand what is going on. They are important, but they are tools and they are often interchangeable. The important thing is, as it always has been, our very human ability to conceptualize and abstract away the key elements of the system of interest. The tools then let us express this understanding to others and to challenge and refine our assumptions about how it works."
  },
  {
    "objectID": "elements.html#problem-orientation-is-nonnegotiable",
    "href": "elements.html#problem-orientation-is-nonnegotiable",
    "title": "3  What are the elements of relational and systems epidemiology?",
    "section": "3.2 Problem-orientation is nonnegotiable",
    "text": "3.2 Problem-orientation is nonnegotiable\n\n“A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.” - Douglas Adams, Hitchhikers Guide to the Galaxy.\n\nSometimes, methods-y topics in epidemiology and public health are boiled down to a sequence of steps to be applied to each new problem. In the worst cases, they come to us as a series of copy-paste, plug-and-chug pieces of code to be reused each time the same type of problem is encountered.\nBeyond being a boring way to learn, this has the effect of putting the methods at the front of the train, with the question or problem implicitly assumed to be an opportunity or excuse to employ the model.\nThis is an approach that can get you some publications and maybe a little bit of clout within the musty world of academia, but it doesn’t do much to solve the types of problems working epidemiologists face. And in truth, it doesn’t even work so well on the academic side of the fence.\nInside the universe of this book, the problem is the first and most important thing. The question is the fixed point against which our analytic approaches are chosen. This means that there are no fixed methodological answers to applied questions: Our methodological approaches and tools must be as diverse and heterodox as the questions the world throws at us.\nMaking sense of the types of patterns we see in the real world requires us to first identify:\n\nA question we want or need to answer.\nThe most important types of relationships impacting our outcome of interest (time, space, individual-to-individual).\nA methodological approach that will allow us to characterize the impact of those relationships on the outcome we care about.\n\n\nA methodological caboose\nIt is important to note that the choice of method comes last here: A key motivation of this book is to sidestep the tendency to train ourselves into methodological hammers looking for data nails to whack away at.\nIn addition to this, there is no pre-supposition that the appropriate method is necessarily ‘fancy’ in the sense of being conceptually or mathematically complex, computationally intensive, or even primarily or at all quantitative in nature. Whether this is the case is entirely a function of what happens at the intersection between question, data, and theory.\n\nThis seems totally in line with some of the ideas on extensive and intensive research in (1) and (2)"
  },
  {
    "objectID": "philosophy.html",
    "href": "philosophy.html",
    "title": "4  Philosophical Guideposts",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "philosophy.html#the-map-is-not-the-territory",
    "href": "philosophy.html#the-map-is-not-the-territory",
    "title": "4  Philosophical Guideposts",
    "section": "4.1 “The Map is Not the Territory”",
    "text": "4.1 “The Map is Not the Territory”\nThe phrase “the map is not the territory” is attributed to the Polish-American polymath scientist, author and philosopher Alfred Korzybski (1). The phrase is pretty self-explanatory, but essentially means that we should not mistake our representations of reality for reality itself. In the social sciences, this insight is closely tied to the critical realist perspective.\nSayer (2) explains that the “real” in realism refers to the idea that realists accept reality as a fixed, external thing. The critical spect of it comes from a belief that our perceptions and interpretations of this outside reality are always approximations which are inherently subjective in nature.\nDunn (3) has outlined some of the ways a critical realist approach can be useful in epidemiology, particularly in the study of health inequities. Dunn states that the goal of realist science is to generate knowledge that is “practically adequate” but that is understood to be fallible and always subject to revision. This emphasis on fallibility also short circuits the quest for methodological perfection and obviously militates against any hard-and-fast rules for causal inference."
  },
  {
    "objectID": "philosophy.html#open-vs.-closed-systems",
    "href": "philosophy.html#open-vs.-closed-systems",
    "title": "4  Philosophical Guideposts",
    "section": "4.2 Open vs. Closed Systems",
    "text": "4.2 Open vs. Closed Systems\nBoth Dunn and Sayer emphasize the idea that it is more helpful to think in terms of “extensive” and “intensive” research rather than in terms of qualitative vs. quantitative research.\nIn Dunn’s version of events, extensive research is about obtaining surface-level information on large numbers of people. The idea here is to find regularities across individuals with a similar health condition or to look for key features of a population. Sayer (2) refers to this as a search for “formal relations of similarity” between “taxonomic groups”, e.g. people of a certain race/ethnicity, age, gender, etc.\nBy contrast, intensive research is about understanding processes, often using a smaller number of observations than might be considered acceptable or useful for extensive research. But the nature of the data employed for intensive research will also be different and more granular, with more of what we now call “metadata” on both individuals and their relations to each other in physical space, social networks, and temporal context. it is focused on understanding the particular relationships between people and institutions as well as how these operate in the context of local and more macro-structural contingencies."
  },
  {
    "objectID": "philosophy.html#a-personal-connection",
    "href": "philosophy.html#a-personal-connection",
    "title": "4  Philosophical Guideposts",
    "section": "4.3 A personal connection",
    "text": "4.3 A personal connection\nWhile the idea that “the map is not the territory” is quite intuitive, it is just a key that unlocks the door to a host of important intellctual consequences. For me, the dominoes may have begun to fall particularly early on: The phrase and the various books it has been written in, as well as the coffee mugs it has been printed on, have played an unusually central role in my own life. My mother, Marjorie Zelner, was the sole full-time employee of the Institute for General Semantics, the organization founded by Alfred Korzybski, from 1989 until her death in 2000. For a time, the sword carried by Alfred Korzybski when he was a member of the Polish cavalry for some reason resided in our downstairs coat closet.\nThe (very small) field of General Semantics (GS), founded by Korzybski and Marjorie Kendig, is focused on what happens when verbal language is mistaken for the reality of the real-world phenomena it seeks to describe. While the focus of GS is on the relationship between language, reality, and human psychology, it must have seeped into my understanding of public health and epidemiology somewhere along the line.\nWhen I was in middle school and high school in the 1990s, I would earn spending money by helping my mom to box up and ship mail-order copies of Science and Sanity, the book where the phrase was first published, to the small band of people around the world who were interested in it. I would fold hundreds of copies of their monthly newsletter, stuff them into envelopes, and run them through the postage meter. Eventually, the institute relocated its headquarters - including its entire library - from a basement in Lakeville, CT to the second floor of the old carriage barn behind the house I grew up in in New Jersey. When I was bored, I would often find myself paging through the various books in the library, entertained and somewhat mystified by their complex concepts and knack for introducing new buzzwords and using mystifying but captivating diagrams. I can’t say that much of what I read made a lot of sense to me at the time, but something must have made its way through. More than anything, I absorbed a lot from the various intellectual rogues and vagabonds associated with the organization who would come to our house for the Institute’s annual meetings, often sleeping on our living room floor or pull-out couch.\nAll this is a long way of getting to the point that my own perspective on relational epidemiology is in many ways totally inextricable from the social and physical context I grew up in, as well as the historical moment it happened in. It could have happened another way, but this was the series of contingent events in the still very much open system of my particular existence that got me here."
  },
  {
    "objectID": "relationships.html",
    "href": "relationships.html",
    "title": "6  Relationships that matter",
    "section": "",
    "text": "7 Space, time, and network: Multiple views of the the same systems\nWhen working on relational problems, there is sometimes a tendency to mistake the frame or perspective we are taking on a given problem for the real thing. This pitfall is exemplified by the phrase “the map is not the territory”: While some kind of abstraction or representation is always necessary, it is imperative to remember that it is always a proxy for the phenomenon under study.\nFor example, consider what we are doing when we say that an individual’s risk of infection declines with distance from the nearest case of that infection. Buried in the shape and intensity of the spatial autocorrelation function is the way interaction occurs between people over time. Those interactions may be purely a function of physical distance, i.e. I am less likely to bump into someone who lives far away from me than a neighbor, but may also reflect elements of ‘social space’. For example, certain forms of racial residential segregation may result in individuals of different racial identities living in very close proximity to each other but still having very limited or circumscribed contact with each other.\nFor example, in the excellent book Infectious Fear (1), the medical historian Samuel Kelton Roberts illustrates how the residential segregation of near-South cities like Washington D.C. and Baltimore was manifest in a configuration in which White families lived in the row houses facing the street, while their Black servants lived in smaller, more-cramped quarters facing the alleys behind the homes of the more-privileged and well-off Whites. (For an example of this living configuration, see Figure 7.1.) In many cases these homes would share an adjoining wall, but the risks of acquiring and dying from infections like Tuberculosis associated with living in them were vastly different.\nIn this case, a quantitative analysis that conflated spatial distance with social similarity and disease-transmitting contact leading to similar health outcomes would go badly astray. In the end, there are no universal meanings we can attribute to the geographic or temporal distances between events, and the formalisms we use to represent them are only as good as our ability to understand the social, environmental and historical context of the places we are looking at."
  },
  {
    "objectID": "relationships.html#spatial",
    "href": "relationships.html#spatial",
    "title": "6  Relationships that matter",
    "section": "7.1 Spatial",
    "text": "7.1 Spatial\nPlease read the following two pieces that discuss key ideas about geospatial relatedness:\nMiller HJ. Tobler’s First Law and Spatial Analysis. Annals of the Association of American Geographers. 2004;94(2):284-289. doi:10.1111/j.1467-8306.2004.09402005.x\nGoodchild MF. The Validity and Usefulness of Laws in Geographic Information Science and Geography. Annals of the Association of American Geographers. 2004;94(2):300-303. doi:10.1111/j.1467-8306.2004.09402008.x\n\nPhysical Space\n\n\nSocial Space"
  },
  {
    "objectID": "relationships.html#temporal",
    "href": "relationships.html#temporal",
    "title": "6  Relationships that matter",
    "section": "7.2 Temporal",
    "text": "7.2 Temporal\n\nShort-term correlation and fluctuation\n\n\nTime series\n\n\nHistory"
  },
  {
    "objectID": "relationships.html#networks",
    "href": "relationships.html#networks",
    "title": "6  Relationships that matter",
    "section": "7.3 Networks",
    "text": "7.3 Networks\n\nSocial Stratification & Inequality\n\n\nSocial Networks"
  },
  {
    "objectID": "relationships.html#references",
    "href": "relationships.html#references",
    "title": "6  Relationships that matter",
    "section": "References",
    "text": "References\n\n\n\n\n1. Roberts SK. Infectious Fear: Politics, Disease, and the Health Effects of Segregation. University of North Carolina Press; 2009.\n\n\n2. Logan J. Racial segregation in postbellum Southern cities: The case of Washington, D.C. Demographic Research. 2017;36:1759–1784."
  },
  {
    "objectID": "segregation/index.html#dimensions-of-segregation",
    "href": "segregation/index.html#dimensions-of-segregation",
    "title": "7  Introduction",
    "section": "7.1 Dimensions of Segregation",
    "text": "7.1 Dimensions of Segregation\nResidential segregaton generally refers to the physical separation of different groups as a function of their race, ethnicity, religion, income, or some combination of these factors. In addition to being a persistent and ever-present feature of unequal societies, it touches on every component of the relational toolbox.\n\nSpace\nSpace is the most common lens through which we tend to approach the concept of segregation: Classic notions of segregation focus on the intensity and scale of separation between groups. In the United States, these conversations often focus on the physical separation of Blacks from Whites in cities such as Chicago and New York. But this pattern has been documented around the world and often takes the form of separation by ethnicity or religion as well (1).\nThe spatial aspects of segregation may impact health outcomes through their effects on accessibility to healthcare, the quality of the neighborhood environment (e.g. access to parks and other forms of green space), the presence of crime and other factors that lead to a diminished sense of safety, and on and on. These effects of segregation are perhaps the most palpable and tangible, but as we will see, to truly understand and address residential segregation we need to grasp its other dimensions.\n\n\nTime\nThe impacts of segregation can be viewed through multiple temporal lenses. For example, much historical work has documented the ways in which patterns of redlining, which denied mortgages and home insurance to Blacks and other non-White home buyers in the U.S., entrenched the patterns of racial residential segregation we still see in many cities in the present day.\nOther work has looked specifically at segregation as a life-course exposure, with effects that may manifest in later life among people exposed even as children (2).\n\n\nSocial\nWhile it is most common to conceptualize segregation as a social process with an overt spatial manifestation, much research has complicated this picture. While a spaital perspective may reveal a lot about the underlying processes, a perspective focused at the level of individuals and groups reveals how segregation and its impacts play out through the sorting of individuals into occupations and social groups which then interact with each other in ways that reinforce hierarchies that drive deleterious health outcomes.\nThe social impacts of segregation may also manifest in what the sociologist Loïc Wacquant has referred to as “territorial stigmatization” (3) in which a place’s reputation is used to stigmatize its residence. In the context of the COVID-19 pandemic, Chowkwanyun and Reed (4) invoked this idea to sound a note of caution on spatial analyses of disease outcomes, suggesting that analyses that fuse place and disease together without appropriate attention to the structural drivers of these relationships may actually reinforce the social inequities they are intended to highlight and ameliorate."
  },
  {
    "objectID": "segregation/index.html#references",
    "href": "segregation/index.html#references",
    "title": "7  Introduction",
    "section": "References",
    "text": "References\n\n\n\n\n1. Susewind R. Muslims in Indian cities: Degrees of segregation and the elusive ghetto. Environment and Planning A: Economy and Space [electronic article]. 2017;49(6):1286–1307. (http://journals.sagepub.com/doi/10.1177/0308518X17696071). (Accessed September 30, 2022)\n\n\n2. Kershaw KN, Robinson WR, Gordon-Larsen P, et al. Association of Changes in Neighborhood-Level Racial Residential Segregation With Changes in Blood Pressure Among Black Adults: The CARDIA Study. JAMA Internal Medicine [electronic article]. 2017;177(7):996–1002. (https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2626858). (Accessed January 21, 2020)\n\n\n3. Wacquant L. Territorial Stigmatization in the Age of Advanced Marginality. Thesis Eleven [electronic article]. 2007;91(1):66–77. (https://doi.org/10.1177/0725513607082003). (Accessed September 21, 2023)\n\n\n4. Chowkwanyun M, Reed AL. Racial Health Disparities and Covid-19 Caution and Context. New England Journal of Medicine [electronic article]. 2020;383(3):201–203. (i). (Accessed November 16, 2020)"
  },
  {
    "objectID": "smoothing.html#notation-and-terminology",
    "href": "smoothing.html#notation-and-terminology",
    "title": "8  Codifying Tobler’s First Law using Locally Weighted Regression",
    "section": "8.1 Notation and Terminology",
    "text": "8.1 Notation and Terminology\nIn this example, we are interested in visualizing and predicting the values of a function \\(f(x_i)\\) which outputs values \\(y_i\\), the expected value of the output function.\n\\[\ny_i = f(x_i)\n\\]\nLets start by getting the values of \\(f(x)\\) for every input value \\(x\\). For simplicity, we will assume that \\(f(x)\\) is a sine function and that the values of \\(x\\) go from -1 to +5, allowing us to observe one half cycle of the sine function:\n\nx &lt;- seq(-1, 5, by = 0.1)\ny &lt;- sin(x)\nplot(x,y)\n\n\n\n\nYou can see right away that this simple curve pretty neatly expresses Tobler’s first law: \\(f(x)\\) values of each point are in general more similar to each other for nearby values of \\(x\\). If we want to press this idea into real-world practice, we need a model that can translate TFL into quantitative estimates and qualitative visualizations. There are lots of ways to do this, but we’ll focus in on locally-weighted regression, also known LOWESS.\nThe basic idea of a LOWESS regression is to define a window of size \\(k\\) points around each value one wishes to estimate, and calculate a weighted average of the value of those points, which can then be used as the estimated value \\(\\hat{y_j} \\approx f(x_j)\\). We then run the values of these nearest neighbors through a weight function \\(w(x)\\).\nThese weight functions can take a lot of different forms, but we’ll start simple with a uniform one, i.e. just taking the average of the \\(k\\) nearest neighbors, so that \\(\\hat{y} = sum(z(x_i, k))/k\\), where \\(KNNz\\) is a function returning the \\(y\\) values of the k nearest observations to \\(x_i\\). The value of \\(k\\) is sometimes referred to as the bandwidth of the smoothing function: Larger bandwidths use more data to estimate values at each point, smaller ones use less.\n\nMaking some locally weighted estimates\nUsing the fnn package for R, we can find the indices of the \\(k\\) nearest neighbors of each point we want to make an estimate at:\n\nlibrary(FNN)\nk &lt;- 10\nz &lt;- knn.index(x, k=k)\n\nYou can read the output of this function, below, as indicating the indices (\\(i\\)) of the 10 nearest points to each of the values of \\(x\\).\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    2    3    4    5    6    7    8    9   10    11\n [2,]    1    3    4    5    6    7    8    9   10    11\n [3,]    2    4    1    5    6    7    8    9   10    11\n [4,]    5    3    6    2    1    7    8    9   10    11\n [5,]    6    4    7    3    8    2    1    9   10    11\n [6,]    5    7    4    8    3    9    2   10    1    11\n [7,]    8    6    9    5   10    4   11    3   12     2\n [8,]    7    9   10    6   11    5    4   12    3    13\n [9,]   10    8   11    7   12    6    5   13   14     4\n[10,]    9   11    8   12    7   13   14    6    5    15\n\n\nWe can visualize this by picking a point in the middle of the series and its 10 nearest neighbors and the estimated value of \\(\\hat{y_i}\\) obtained by just taking the average of the k nearest points:\n\nlibrary(ggplot2)\n\n## Plot the original data\ng &lt;- ggplot() + geom_point(aes(x=x, y = y)) + \n  xlab(\"x\") + ylab(\"f(x)\")\n\n## Now, get the index for x = 2\nx_index &lt;- which(x==2)\n\n## Show the range of k nearest neighbors of this point\nknn_low &lt;- min(x[z[x_index, ]])\nknn_high &lt;- max(x[z[x_index, ]])\ny_hat &lt;- mean(y[z[x_index,]], )\n\n## Add errorbars to the figure to show the 10 nearest values with the height of the point indicating the estimated value at y_i, denoted by the red dot\ng &lt;- g + geom_errorbarh(aes(xmin = knn_low, xmax = knn_high, y = y_hat)) + geom_point(aes(x=2,y_hat), colour = \"red\")\n\nplot(g)\n\n\n\n\nNotice that if the knn function is applied at the low end of the series, i.e. to the first value, it will use points to the right of that one instead of to either side:\n\nlibrary(ggplot2)\n\n## Plot the original data\ng &lt;- ggplot() + geom_point(aes(x=x, y = y)) + \n  xlab(\"x\") + ylab(\"f(x)\")\n\n## Use the index for the lowest value\nx_index &lt;- 1\n\n## Show the range of k nearest neighbors of this point\nknn_low &lt;- min(x[z[x_index, ]])\nknn_high &lt;- max(x[z[x_index, ]])\ny_hat &lt;- mean(y[z[x_index,]], )\n\n## Add errorbars to the figure to show the 10 nearest values with the height of the point indicating the estimated value at y_i\ng &lt;- g + geom_errorbarh(aes(xmin = knn_low, xmax = knn_high, y = y_hat)) + geom_point(aes(x=x[x_index],y_hat), colour = \"red\")\n\nplot(g)\n\n\n\n\nNow, lets see what happens if we run our smoother over the whole series and take the average of the 10 nearest points for each and compare them to the observed data:\n\ny_hat &lt;- rep(0, length(x))\nfor (i in 1:length(x) ) {\n  y_hat[i] &lt;- mean(y[z[i,]], )\n}\n\nNow plot the predicted vs. the observed values:\n\ng &lt;- ggplot() + geom_point(aes(x=x, y = y)) + \n  xlab(\"x\") + ylab(\"f(x)\") + geom_line(aes(x=x, y=y_hat), colour = \"red\")\nplot(g)\n\n\n\n\nYou can see this does a pretty good job all the way through, except at the edges. Lets try it again with a smaller window - or bandwidth - of 5 and see what happens. First, we’ll write a function that will give us the predicted value of y at each point given a window of size k and an input value:\n\nknn_est &lt;- function(x, y, k) {\n  z &lt;- knn.index(x, k=k)\n  y_hat &lt;- rep(0, length(x))\n  \nfor (i in 1:length(x) ) {\n  y_hat[i] &lt;- mean(y[z[i,]], )\n}\n\n  df &lt;- data.frame(x=x, y = y, yhat = y_hat)\n  return(df)\n}\n\n\npred_df &lt;- knn_est(x, y, 5)\ng &lt;- ggplot(pred_df) + geom_point(aes(x=x, y=y)) + geom_line(aes(x=x,y=yhat),colour=\"red\")\nplot(g)\n\n\n\n\nThis gets rid of a lot of the weird effects at the edges but introduces some noise into the function. What if we make the window bigger, say 20, to get rid of some of the noise?\n\n\n\n\n\nThis seems to make the edge effects worse, as well as the estimates of the function overall worse.\nWhat happens if we go in the opposite direction and shrink the window down to 2?\n\n\n\n\n\n\n\nDiscussion Questions\n\nWhy does this appear to be more accurate for these data than \\(k=10\\) and \\(k=5\\)?\nWhat would happen if we added observation noise to the values of \\(y_i\\)? Which one of the smoothers do you think would work better then?\nIs the one best value of \\(k\\) for all datasets? How might you go about picking the best one?\nHow does our uniform weight function express Tobler’s first law? What kind of weight function \\(w(x)\\) might do a better job of capturing the notion of distance decay?"
  },
  {
    "objectID": "smoothing.html#references",
    "href": "smoothing.html#references",
    "title": "8  Codifying Tobler’s First Law using Locally Weighted Regression",
    "section": "8.2 References",
    "text": "8.2 References\n\n\n\n\n1. Tobler WR. A Computer Movie Simulating Urban Growth in the Detroit Region. Economic Geography [electronic article]. 1970;46:234–240. (http://www.jstor.org/stable/143141). (Accessed January 13, 2022)"
  },
  {
    "objectID": "density.html#a-motivating-example",
    "href": "density.html#a-motivating-example",
    "title": "9  Spatial Density",
    "section": "9.1 A motivating example",
    "text": "9.1 A motivating example\nA spatial transect is an area of space along a line crossing a landscape. These are often used in ecology and forestry to assess the health of an environment, species diversity and other factors. Using a transect can help simplify the problem of spatial analysis down to one dimension rather than the usual two, while still providing a tremendous amount of useful information.\n\n\n\nExample of an ecological transect from the US National Park Service (source)\n\n\nFor example, (1) were interested in characterizing the intensity of exposure to triatomine bugs and other insect vectors of the pathogen T. cruzi, which causes Chagas disease.\n\n\n\nTriatoma (left- and right-hand panels) and T. cruzi (center) (source)\n\n\n\n\n\nIntensity of Triatomine infestation along a 2km transect in Arequipa, Peru (Figure from (1))\n\n\nImagine we are estimating the density of some unknown insect vector along a 1 kilometer transect with the goal of characterizing the risk of infection with a vector-borne illness."
  },
  {
    "objectID": "density.html#kernel-density-estimation-in-one-dimension",
    "href": "density.html#kernel-density-estimation-in-one-dimension",
    "title": "9  Spatial Density",
    "section": "9.2 Kernel density estimation in one dimension",
    "text": "9.2 Kernel density estimation in one dimension\nMuch like in our discussion of kernel smoothing of continuous outcomes, kernel functions play a key role in this setting as well. In this case, imagine that the locations of vectors along our transect have been sampled at random from some unknown function \\(f(x)\\) which takes values from 0 (the beginning of the transect) to 1000m (the end).\nWe can use the Kernel function \\(K(d)\\) to approximate the intensity of the outcome of interest at each observed case location \\(x_i\\). Imagine that our observed data have locations \\(x_1, x_2, \\ldots, x_n\\) and that the distance between our point of interest, \\(x_j\\) and each observed point is \\(d_{ij} = | x_j - x_i |\\).\nFinally, lets include a bandwidth parameter, \\(h\\), which controls the width of the window we will use for smoothing. When we put this all together, we can get an estimate of the density of our outcome of interest at location \\(x_j\\) as follows:\n\\[\n\\hat{f_h}(x_j) = \\frac{1}{n} \\sum_{i=1}^{n} K(\\frac{x_j - x_i}{h})\n\\]\nAs you can see below, we can pick a range of kernel functions, but for the sake of simplicity, in this example, we will focus in on a Gaussian, or normal, kernel, which uses the probability density function of a normal distribution to weight points.\nLets start by sampling locations of observed points along a one dimensional line. To keep things interesting, we’ll use a Gaussian mixture distribution with two components:\n\n\n\nComparison of different kernel functions (source)"
  },
  {
    "objectID": "density.html#worked-example",
    "href": "density.html#worked-example",
    "title": "9  Spatial Density",
    "section": "9.3 Worked example",
    "text": "9.3 Worked example\nFirst, lets imagine a scenario in which the risk of observing an insect vector steadily decreases as we walk along our transect. However, along the way there is a hotspot of increased risk beyond what we would expect from the smooth decline before and after that spot. For the purpose of this example, we’ll assume that risk decays exponentially with distance from the origin, but that our hotspot is centered at a point 300 meters into the transect. The code below lets us sample the locations of the points along the transect where 🐜 are observed from two distributions:\n\nAn exponential distribution representing smooth decay from the beginning to the end of the transect, and\nA normal distribution representing a hotspot about 150m in width beginning 300m in\n\nThe figure below shows a histogram of locations sampled from \\(f(x)\\) (vertical bars) overlaid with the true value of \\(f(x)\\) in red:\n\n\nCode\nlibrary(ggplot2)\nd_a &lt;- dexp(1:1000, rate = 1/250) \nd_b &lt;- dnorm(1:1000, mean = 300, sd = 50)\ny &lt;- ((1-p_hot))*d_a + (p_hot*d_b)\n\ndens_df &lt;- data.frame(x = 1:1000, y = y)\nxdf &lt;- data.frame(x=x)\n\n\ng &lt;- ggplot(xdf) + geom_histogram(aes(x=x, y=..density..), bins=100) + \ngeom_line(data=dens_df, aes(x=x,y=y), colour=\"red\") +\nxlim(0, 1000) + ylab(\"Density\") + xlab(\"Distance from transect origin (m)\")\nplot(g)\n\n\n\n\n\nNow, imagine we have another set of finely spaced points along the line, and for each, we want to calculate the weight for each. The function below lets us do that:\nThe figure below shows the true value of our density function \\(f(x)\\) in red, the density of points in the simulated data along the x-axis of the ‘rug plot’, and our smoothed density in black, for a bandwidth of \\(h=10\\):\n\n\nCode\nlibrary(ggplot2)\npred_df &lt;- normal_smoother(x, h = 10)\n\ng &lt;- ggplot() + geom_rug(aes(x=x)) + \ngeom_line(data = pred_df, aes(x=x, y=y)) + \nylab(\"Density\") + geom_line(data = dens_df, aes(x=x,y=y), colour=\"red\") + \nxlim(0, 1000)\ndens_ojs &lt;- dens_df\ndens_ojs$y &lt;- dens_ojs$y*cc\nplot(g)\n\n\n\n\n\nNow, lets see what happens if we try this for different values of \\(h\\):\n\n\nCode\nall_df &lt;- data.frame()\nfor (hv in c(5, 10, 20, 50 ,100, 250)) {\n  pred_df &lt;- normal_smoother(x, h = hv)\n  pred_df$h &lt;- hv\n  all_df &lt;- rbind(all_df, pred_df) \n}\n\n  all_df$h &lt;- as.factor(all_df$h)\n\ng &lt;- ggplot(all_df) + geom_rug(aes(x=x)) + \ngeom_line(data = dens_df, aes(x=x,y=y), colour=\"red\") + \ngeom_line(aes(x=x, y=y)) + \nylab(\"Density\") + \nfacet_wrap(~ h) +\nxlim(0, 1000)\n\nplot(g)\n\n\n\n\n\n\n\nCode\nall_df &lt;- data.frame()\nhvals &lt;- seq(1, 100, by = 2)\ndistvals &lt;- seq(-100, 100, by = 1)\nall_kernvals &lt;- data.frame()\nfor (hv in hvals) {\n  pred_df &lt;- normal_smoother(x, h = hv)\n  pred_df$h &lt;- hv\n  pred_df$smoother &lt;- \"gaussian\"\n  all_df &lt;- rbind(all_df, pred_df) \n  all_kernvals &lt;- rbind(all_kernvals,data.frame(x=distvals, y=kdgaussian(distvals, bw = hv), smoother = \"gaussian\", h = hv))\n  \n  pred_df &lt;- normal_smoother(x, h = hv, kern = kduniform, kernp=kpuniform)\n  pred_df$h &lt;- hv\n  pred_df$smoother &lt;- \"uniform\"\n  all_df &lt;- rbind(all_df, pred_df) \n  all_kernvals &lt;- rbind(all_kernvals,data.frame(x=distvals, y=kduniform(distvals, bw = hv), smoother = \"uniform\", h = hv))\n\n  \n  pred_df &lt;- normal_smoother(x, h = hv, kern = kdtricube, kernp=kptricube)\n  pred_df$h &lt;- hv\n  pred_df$smoother &lt;- \"tricube\"\n  all_df &lt;- rbind(all_df, pred_df) \n    all_kernvals &lt;- rbind(all_kernvals, data.frame(x=distvals, y=kdtricube(distvals, bw = hv), smoother = \"tricube\", h = hv))\n    \n  pred_df &lt;- normal_smoother(x, h = hv, kern = kdtriangular, kernp=kptriangular)\n  pred_df$h &lt;- hv\n  pred_df$smoother &lt;- \"triangular\"\n  all_df &lt;- rbind(all_df, pred_df) \n    all_kernvals &lt;- rbind(all_kernvals, data.frame(x=distvals, y=kdtriangular(distvals, bw = hv), smoother = \"triangular\", h = hv))\n\n}\nall_df$y &lt;- all_df$y * cc"
  },
  {
    "objectID": "density.html#trying-different-bandwidths-and-kernels",
    "href": "density.html#trying-different-bandwidths-and-kernels",
    "title": "9  Spatial Density",
    "section": "9.4 Trying different bandwidths and kernels",
    "text": "9.4 Trying different bandwidths and kernels\nYou can adjust the range of the bandwidth here to get a better sense of the relationship between the smoothed curve (black) and true density (red). Adjust the bin width for the histogram of the underlying data to get a sense of the fit of the model to the underlying data.\n\nviewof h = Inputs.range([1, 100], {value: 10, step: 2, label: \"Bandwidth (m)\"})\nviewof bw = Inputs.range([5, 100], {value: 10, step: 5, label: \"Bin width (m)\"})\nviewof kern = Inputs.select([\"gaussian\", \"uniform\", \"tricube\", \"triangular\"], {value: \"gaussian\", label: \"Smoothing kernel\"})\n\nnumbins = Math.floor(1000/bw)\n\ndtrans = transpose(hvals)\nPlot.plot({\ny: {grid: true, \nlabel: \"Density\"},\nx: {\nlabel: \"Distance from transect start (m) →\"\n},\n    marks: [\n    Plot.rectY(transpose(sample),Plot.binX({y: \"count\"}, {x: \"loc\", fill: \"steelblue\", thresholds: numbins})),\n    Plot.lineY(dtrans, {filter: d =&gt; (d.h == h) && (d.smoother == kern), curve: \"linear\", x:\"x\",y: d =&gt; d.y * bw}),\n    Plot.lineY(transpose(dens), {x:\"x\", y: d =&gt; d.y * bw, curve:\"linear\", stroke: \"red\"})\n    ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure below shows the relative amount of weight placed on different points as a function of their distance from the point of interest (0, marked by the vertical red line):\n\nkv = transpose(kernvals).filter(d =&gt; d.h == h && d.smoother == kern)\nPlot.plot({\ny: {grid: true, label: \"Relative weight of point as compared to origin\"},\nx: {\nlabel: \"Distance from point of interest (m) ↔ \"\n},\nmarks: [\n//Plot.lineY(kv, {filter: d =&gt; (d.smoother == kern), x:\"x\", y: d =&gt; d.y*1000}),\nPlot.lineY(kv, Plot.normalizeY({x:\"x\", y: \"y\", basis: \"extent\"})),\nPlot.ruleX([0], {stroke: \"red\"})\n]})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions\n\nWhich of the bandwidth options seems to do the best job in capturing the value of \\(f(x)\\)? Why?\nHow does the choice of kernel impact the smoothing?\nHow do the different kernel functions encode different assumptions about distance decay?\nWhat is the relationship between the histogram of the data and the smoother? What do you see as you change the histogram bin width relative to the smoothing bandwidth?"
  },
  {
    "objectID": "density.html#additional-resources",
    "href": "density.html#additional-resources",
    "title": "9  Spatial Density",
    "section": "9.5 Additional Resources",
    "text": "9.5 Additional Resources\nPlease see Matthew Conlen’s excellent interactive KDE tutorial"
  },
  {
    "objectID": "density.html#references",
    "href": "density.html#references",
    "title": "9  Spatial Density",
    "section": "9.6 References",
    "text": "9.6 References\n\n\n\n\n1. Levy MZ, Barbu CM, Castillo-Neyra R, et al. Urbanization, land tenure security and vector-borne Chagas disease. Proceedings of the Royal Society B: Biological Sciences [electronic article]. 2014;281(1789):20141003. (https://royalsocietypublishing.org/doi/full/10.1098/rspb.2014.1003). (Accessed January 23, 2023)"
  },
  {
    "objectID": "radon-multilevel.html#fitting-the-models",
    "href": "radon-multilevel.html#fitting-the-models",
    "title": "10  Household Radon",
    "section": "10.1 Fitting the models",
    "text": "10.1 Fitting the models\n\nSetting up the workspace\nFirst, we will load the relevant packages:\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(bayesplot)\nlibrary(rstanarm)\nlibrary(purrr)\nlibrary(tidybayes)\n\n\n\nData Preparation\nFirst, lets take the raw radon dataset from the rstanarm package and recode the floor variable to be interpretable as the basement one from the original paper: some minor modifications and additonal datasets that we’ll use for the purposes of modeling and visualizing these data.\n\nradon$basement &lt;- 1 - radon$floor\n\nNow we can see that that the dataset has all of the variables we need:\n\n\n  floor county  log_radon log_uranium basement\n1     1 AITKIN 0.83290912  -0.6890476        0\n2     0 AITKIN 0.83290912  -0.6890476        1\n3     0 AITKIN 1.09861229  -0.6890476        1\n4     0 AITKIN 0.09531018  -0.6890476        1\n5     0  ANOKA 1.16315081  -0.8473129        1\n6     0  ANOKA 0.95551145  -0.8473129        1\n\n\n\n\n🚪 Door 1: Full pooling!\nThis corresponds to a model in which we are assuming exactly no variation across locations in terms of the baseline level of radon. So, we can run a simple regression model where we assume that:\n\\[\ny_{ij} = \\alpha + \\beta x_{ij} + \\epsilon_{i}\n\\]\nWhere \\(x_{ij} = 1\\) if a house has a basement and 0 otherwise.\nIn R, we can fit this model via least squares using a single line of code:\n\nm1 &lt;-lm(log_radon ~ basement, data = radon)\n\nWe can call the summary function to get a description of the key coefficients and the goodness-of-fit:\n\n\nlm(formula = log_radon ~ basement, data = radon)\n            coef.est coef.se\n(Intercept) 0.78     0.06   \nbasement    0.59     0.07   \n---\nn = 919, k = 2\nresidual sd = 0.79, R-Squared = 0.07\n\n\n\n\n🚪 Door 2: No pooling\nThe second approach is the “No Pooling” one in which we allow the baseline intensity of radon in each county (represented by the intercept term \\(\\alpha_j\\)) to vary, but we don’t do anything to constrain that variation. In other words, we treat each county as though it was independent.\nHowever, to estimate a consistent effect of having a basement across all counties, we estimate a single \\(\\beta\\) term. This leads to a model that looks like this:\n\\[\ny_{ij} = \\alpha_j + \\beta x_{ij} + \\epsilon_{i}\n\\]\nIn R this is easy to implement, because we are implicitly asking the regression model to treat county as a categorical variable if we pass it to it as a factor datatype:\n\nno_pool_m &lt;- lm(log_radon ~ basement + log_uranium + county, data = radon)\n\n\n\nlm(formula = log_radon ~ basement + log_uranium + county, data = radon)\n                     coef.est coef.se\n(Intercept)           0.42     0.37  \nbasement              0.69     0.07  \nlog_uranium           0.32     0.60  \ncountyANOKA           0.09     0.44  \ncountyBECKER          0.48     0.53  \ncountyBELTRAMI        0.67     0.43  \ncountyBENTON          0.39     0.48  \ncountyBIGSTONE        0.31     0.68  \ncountyBLUEEARTH       0.83     0.51  \ncountyBROWN           0.80     0.60  \ncountyCARLTON         0.05     0.38  \ncountyCARVER          0.43     0.50  \ncountyCASS            0.52     0.47  \ncountyCHIPPEWA        0.56     0.60  \ncountyCHISAGO         0.21     0.48  \ncountyCLAY            0.79     0.54  \ncountyCLEARWATER      0.28     0.50  \ncountyCOOK           -0.23     0.60  \ncountyCOTTONWOOD      0.06     0.63  \ncountyCROWWING        0.26     0.40  \ncountyDAKOTA          0.27     0.36  \ncountyDODGE           0.63     0.63  \ncountyDOUGLAS         0.60     0.49  \ncountyFARIBAULT      -0.42     0.57  \ncountyFILLMORE        0.18     0.75  \ncountyFREEBORN        0.94     0.51  \ncountyGOODHUE         0.80     0.48  \ncountyHENNEPIN        0.32     0.34  \ncountyHOUSTON         0.52     0.66  \ncountyHUBBARD         0.30     0.44  \ncountyISANTI          0.22     0.57  \ncountyITASCA          0.07     0.42  \ncountyJACKSON         0.83     0.59  \ncountyKANABEC         0.18     0.50  \ncountyKANDIYOHI       0.94     0.54  \ncountyKITTSON         0.51     0.55  \ncountyKOOCHICHING     0.04     0.52  \ncountyLACQUIPARLE     1.75     0.71  \ncountyLAKE           -0.40     0.44  \ncountyLAKEOFTHEWOODS  0.99     0.51  \ncountyLESUEUR         0.60     0.55  \ncountyLINCOLN         1.08     0.67  \ncountyLYON            0.75     0.59  \ncountyMAHNOMEN        0.23     0.84  \ncountyMARSHALL        0.53     0.44  \ncountyMARTIN         -0.05     0.51  \ncountyMCLEOD          0.17     0.46  \ncountyMEEKER          0.13     0.49  \ncountyMILLELACS      -0.07     0.60  \ncountyMORRISON        0.11     0.41  \ncountyMOWER           0.54     0.51  \ncountyMURRAY          1.27     0.90  \ncountyNICOLLET        0.99     0.59  \ncountyNOBLES          0.71     0.68  \ncountyNORMAN          0.10     0.63  \ncountyOLMSTED         0.16     0.48  \ncountyOTTERTAIL       0.60     0.40  \ncountyPENNINGTON      0.11     0.54  \ncountyPINE           -0.24     0.43  \ncountyPIPESTONE       0.62     0.68  \ncountyPOLK            0.55     0.60  \ncountyPOPE            0.11     0.70  \ncountyRAMSEY          0.22     0.33  \ncountyREDWOOD         0.78     0.61  \ncountyRENVILLE        0.46     0.67  \ncountyRICE            0.70     0.49  \ncountyROCK            0.06     0.79  \ncountyROSEAU          0.64     0.36  \ncountySCOTT           0.70     0.43  \ncountySHERBURNE       0.24     0.44  \ncountySIBLEY          0.10     0.58  \ncountySTLOUIS        -0.03     0.31  \ncountySTEARNS         0.38     0.43  \ncountySTEELE          0.41     0.53  \ncountySTEVENS         0.56     0.77  \ncountySWIFT          -0.18     0.61  \ncountyTODD            0.65     0.54  \ncountyTRAVERSE        0.76     0.69  \ncountyWABASHA         0.69     0.50  \ncountyWADENA          0.43     0.48  \ncountyWASECA         -0.47     0.58  \ncountyWASHINGTON      0.31     0.34  \ncountyWATONWAN        1.54     0.60  \ncountyWILKIN          1.06     0.86  \ncountyWINONA          0.41     0.60  \ncountyWRIGHT          0.59     0.39  \n---\nn = 919, k = 86\nresidual sd = 0.73, R-Squared = 0.29\n\n\n\n\n🚪 Door 3: Partial Pooling\nFinally, we get to the partial pooling, hierarchical model in which we introduce a hierarchical prior to the model to allow our model to shrink observations from places with few observations towards the population mean. This allows us to avoid the pitfalls of overfitting associated with the no-pooling approach while not making the homogeneity assumptions associated with the full-pooling approach.\nThis works out to a multi-level model that allows random variation in household-level radon measurements as well as variation at the county level in radon levels above or below the amount predicted by the county-level soil uranium measure. Much like the no-pooling model, we can write outcomes for individuals as:\n\\[\ny_{ij} = \\alpha_j + \\beta x_{ij} + \\epsilon_{i}\n\\]\nHowever, rather than stopping there, we introduce a second level of random variation to the county-level intercepts, \\(\\alpha_j\\).\n\\[\n\\alpha_j = \\gamma_0 + \\gamma \\zeta_{j} + \\epsilon_{j}\n\\]\nWhere \\(\\epsilon_i \\sim N(0, \\sigma_i)\\) and \\(\\epsilon_j \\sim N(0, \\sigma_j)\\).\nTo fit this model, we’ll use the rstanarm package, which uses the Stan Bayesian modeling language under the hook to fit the model. This model introduces another piece of syntax to our equation, which now reads log_radon ~ basement + log_uranium + (1 | county). The interesting part of this is the (1 | county) which is a syntax used by rstanarm and other hierarchical modeling packages (such as lme4) to specify random intercepts (typically represented by a 1 in the matrix of regressors) for each of a set of clusters, in this case counties. In this model, the county-level intercept terms are implicitly assumed to be normally distributed with unknown variance \\(\\sigma_j\\) which will be estimated when the model is fit.\nWe use the stan_lmer function to fit a hierarchical linear model with a normally-distributed response variable, as follows:\n\nm2 &lt;- stan_lmer(log_radon ~ basement + log_uranium + (1 | county), data = radon)\n\nBecause this model is fit by MCMC, we can use draws from the posterior distribution to understand uncertainty in the model. For example, this visualization of the median prediction and credible intervals for the basement and uranium effects can be visualized using the mcmc_areas function from the bayesplot package:\n\nposterior &lt;- as.matrix(m2)\ng2 &lt;- mcmc_areas(posterior, pars = c(\"basement\", \"log_uranium\"))\nplot(g2)"
  },
  {
    "objectID": "radon-multilevel.html#making-the-figures",
    "href": "radon-multilevel.html#making-the-figures",
    "title": "10  Household Radon",
    "section": "10.2 Making the Figures",
    "text": "10.2 Making the Figures\n\nFigure 1\n\nData Preparation\nSince each row of radon dataset includes an observation of a single house, we need to work backwards to obtain the county-level soil uranium measure for each individual county. This is pretty straightforward to do using the dplyr package:\n\ncounty_uranium &lt;- radon %&gt;%\n  group_by(county) %&gt;%\n  summarize(log_uranium = first(log_uranium)) \n\nWe will also make a second dataset that we will use for storing the predicted radon levels for households with and without basements each for county. This contains 2 entries for each county, representing observations taken in the basement or on the first floor.\n\ncounty_uranium_tmp_1 &lt;- county_uranium\ncounty_uranium_tmp_1$basement &lt;- 1\ncounty_uranium_tmp_2 &lt;- county_uranium\ncounty_uranium_tmp_2$basement &lt;- 0\n\ncounty_dummy_df &lt;- rbind(county_uranium_tmp_1, county_uranium_tmp_2)\n\nNow, we will take each of our fitted models (fully pooled, unpooled and partially pooled) and put their predicted values into our plotting dataset\n\ncounty_dummy_df$pooled_pred &lt;- predict(m1, county_dummy_df)\ncounty_dummy_df$no_pool_pred &lt;- predict(no_pool_m, county_dummy_df)\n\nWarning in predict.lm(no_pool_m, county_dummy_df): prediction from a rank-\ndeficient fit may be misleading\n\n\nBecause the partial pooling model was fit using MCMC, we will take a slightly different approach and use the median of the posterior predictive distribution for each observation, which is analogous to (but not exactly the same as) the OLS predictions from the other models:\n\n## Gives posterior median for each prediction.\ncounty_dummy_df$partial_pred &lt;- posterior_predict(m2, county_dummy_df) %&gt;%\n  apply(2,median) \n\n\n\n\nPlotting\nTo re-create Figure 1, we will subset out the observed data and predictions for the 8 counties included in the original figure:\n\n## Place the county names in a vector we will use to keep track of them\nfig_1_counties &lt;-\n  c(\n    \"LACQUIPARLE\",\n    \"AITKIN\",\n    \"KOOCHICHING\",\n    \"DOUGLAS\",\n    \"CLAY\",\n    \"STEARNS\",\n    \"RAMSEY\",\n    \"STLOUIS\"\n  )\n\n\n# First, using the `county_dummy_df` with the basement/non-basement predictions in it,\n# subset out the relevant counties and make a new county factor variable which\n# will be used to ensure that the counties in Fig. 1 plot in the right order\n\ncounty_df_fig_1 &lt;- county_dummy_df %&gt;%\n  filter(county %in% fig_1_counties) %&gt;%\n  mutate(county2 = factor(county, levels = fig_1_counties)) %&gt;%\n  arrange(county)\n\n## Now select out the households in the original data that\n## are in each county and create another county-level factor\n## variable in the same order\n\npred_counties &lt;- radon %&gt;% filter(county %in% fig_1_counties) %&gt;%\n  mutate(county2 = factor(county, levels = fig_1_counties))\n\nOnce we have the datasets together for the figure, we can begin constructing it using ggplot2:\n\ng &lt;- ggplot() +\n  ## The geom_jitter geom plots the log_radon values for each household and \n  ## jitters the points slightly to avoid overplotting. \n  geom_jitter(\n    data = pred_counties,\n    aes(x = basement, y = log_radon, group = county2),\n    height = 0,\n    width = 0.1\n  ) +\n  \n  ## This superimposes the partial-pooling (α + β x_i + ϵ_i +ϵ_j) predictions\n  ## over the raw data\n  geom_line(\n    data = county_df_fig_1,\n    aes(x = basement, y = partial_pred, group = county2),\n    linetype = \"solid\",\n    colour = \"gray\"\n  ) +\n  \n  ## No-pooling predictions (α_{ij} + β x_i + ϵ_i)\n  geom_line(\n    data = county_df_fig_1, \n    aes(x = basement, y = no_pool_pred, group = county2)\n  ) +\n  \n  ## Full pooling predicitons (α + β x_i + ϵ_i)\n  geom_line(\n    data = county_df_fig_1,\n    aes(x = basement, y = pooled_pred, group = county2),\n    linetype = \"dashed\"\n  ) +\n  \n  ## Finally, use facet_wrap to arrange the panels in two \n  ## rows of four\n  facet_wrap(vars(county2), nrow = 2) +\n  xlab(\"basement\") +\n  ylab(\"log radon level\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())\n\nplot(g)\n\n\n\n\n\n\nFigure 2\nFigure 2 reproduces the relationship between the county-level random intercepts, \\(\\alpha_j\\) and the expected level of radon at a county level as a function of county-level soil uranium.\n\nData Preparation\nThe following code allows us to extract predictions at the county level using our prediction dataset. To do this, we use the predicted_draws function from the tidybayes package, which lets us sample from the posterior distribution of the fitted model. The median_qi function, also from tidybayes, lets us calculate the width of a 1 standard error interval (equivalent to the range containing ~17% of the posterior probability mass around the posterior median) used in the original Figure 1 from (1):\n\ndd &lt;- predicted_draws(m2, county_dummy_df) %&gt;%\n  median_qi(.width = 0.17) %&gt;%\n  filter(basement == 0)\n\nIn order to calculate the predicted mean radon at a county level, we need to access the coefficients corresponding to the level two model, including the intercept \\(\\gamma_0\\) and the effect of a 1-log change in log-uranium on predicted log-radon, \\(\\gamma_1\\). In order to get these values out of the model, we can use the gather_draws function from tidybayes, which allows us to access the posterior distributions for each of these parameters:\n\nuranium_coefs &lt;-\n  gather_draws(m2, c(`(Intercept)`, log_uranium)) %&gt;% median_qi()\n\nNow it is as simple as calculating the linear predictor \\(\\gamma_0 + \\gamma_1 z_j\\), where \\(z_j\\) is the log-uranium measure for the j-th county, and storing this information in a data frame we will use for plotting:\n\nlog_uranium_range &lt;-\n  seq(min(county_uranium$log_uranium) - .1,\n      max(county_uranium$log_uranium) + .1,\n      by = 0.1)\n\npred_log_radon &lt;-\n  uranium_coefs$.value[1] + uranium_coefs$.value[2] * log_uranium_range\n\nmedian_radon_pred &lt;-\n  data.frame(log_uranium = log_uranium_range, .prediction = pred_log_radon)\n\n\n\nPlotting\nNow, we can build this figure up one step at a time, starting with our mean predictions:\n\ng &lt;- ggplot(dd) +\n     geom_line(data = median_radon_pred, aes(x = log_uranium, y = .prediction)) \n\nplot(g)\n\n\n\n\nThe next step is to then add the median predictions (points) and 1 SE errorbars to the plot, and then fix the theme to match the original figure, et voilà!\n\ng &lt;- g +  geom_point(aes(x = log_uranium, y = .prediction, group = county)) +\n  geom_errorbar(aes(\n    x = log_uranium,\n    y = .prediction,\n    ymin = .lower,\n    ymax = .upper\n  )) +\n  theme_bw() + theme(panel.grid.major = element_blank(),\n                     panel.grid.minor = element_blank()) +\n  xlab(\"county-level uranium measure\") +\n  ylab(\"regression intercept\")\n\nplot(g)"
  },
  {
    "objectID": "radon-multilevel.html#references",
    "href": "radon-multilevel.html#references",
    "title": "10  Household Radon",
    "section": "References",
    "text": "References\n\n\n\n\n1. Gelman A. Multilevel (Hierarchical) Modeling: What It Can and Cannot Do. Technometrics [electronic article]. 2006;48(3):432–435. (http://www.tandfonline.com/doi/abs/10.1198/004017005000000661). (Accessed December 15, 2019)\n\n\n2. Price PN, Nero AV, Gelman A. Bayesian prediction of mean indoor radon concentrations for Minnesota counties. Health Physics. 1996;71(6):922–936."
  },
  {
    "objectID": "spatial-radon.html#learning-goals",
    "href": "spatial-radon.html#learning-goals",
    "title": "11  Taking a spatial perspective on the radon data",
    "section": "11.1 Learning Goals",
    "text": "11.1 Learning Goals\nThe primary goals of this tutorial are to introduce you to:\n\nMerging of non-spatial health exposure or outcome data with spatial metadata.\nCalculation of important spatial summary statistics, e.g. Moran’s I, from such data.\nSpatial analysis of residuals from aspatial regression models of spatially-referenced data.\n\n\n\n\n\n\n\nLook out 👀 for stretch exercises!\n\n\n\nIf you see a box with a 💡 like this, it’s in an invitation to go a bit further. This could be a conceptual question or a chance to write a bit of code to explore the data or outputs of the analysis a bit more."
  },
  {
    "objectID": "spatial-radon.html#setting-up-the-environment",
    "href": "spatial-radon.html#setting-up-the-environment",
    "title": "11  Taking a spatial perspective on the radon data",
    "section": "11.2 Setting up the environment",
    "text": "11.2 Setting up the environment\n\n\nCode\nlibrary(ggplot2)\nlibrary(arm)\nlibrary(tidycensus)\nlibrary(dplyr)\nlibrary(rstanarm)\nlibrary(stringr)\nlibrary(spdep)\nknitr::opts_chunk$set(message = FALSE, warning=FALSE, tidy=TRUE)"
  },
  {
    "objectID": "spatial-radon.html#data-preparation",
    "href": "spatial-radon.html#data-preparation",
    "title": "11  Taking a spatial perspective on the radon data",
    "section": "11.3 Data Preparation",
    "text": "11.3 Data Preparation\nBefore diving into the analysis steps, there are several key things we need to do to be able to easily work with these data.\n\nDownload a shapefile for Minnesota\nFirst, we need to download a shapefile for the state of Minnesota in which each polygon represents an individual county. Thankfully, in R, this is made easy using the excellent tidycensus package:\n\noptions(tigris_use_cache = TRUE)\n\nminnesota &lt;- get_acs(\n  state = \"MN\",\n  geography = \"county\",\n  variables = \"B19013_001\",\n  geometry = TRUE,\n  year = 2020\n)\n\nTidycensus gives us the data as an sf dataframe containing a number of fields including population estimates, which we can plot straightforwardly using the plot function supplied by the sf package:\n\nplot(minnesota[\"estimate\"])\n\n\n\n\n\n\nMerge the spatial data with the radon data\nIn its raw form, this spatial dataset isn’t quite ready to merge with the radon data. If we take a peek at the county names in the shapefile, we can see that they don’t quite match the formatting of the ones in the original data:\n\nhead(sort(minnesota$NAME))\n\n[1] \"Aitkin County, Minnesota\"    \"Anoka County, Minnesota\"    \n[3] \"Becker County, Minnesota\"    \"Beltrami County, Minnesota\" \n[5] \"Benton County, Minnesota\"    \"Big Stone County, Minnesota\"\n\n\nWhereas in the radon data we see:\n\nhead(unique(as.character(radon$county)))\n\n[1] \"AITKIN\"   \"ANOKA\"    \"BECKER\"   \"BELTRAMI\" \"BENTON\"   \"BIGSTONE\"\n\n\nThe big differences here are that the shapefile uses: 1) mixed-case county names and 2) includes the name of the state in each label. To make these match the radon dataset, we can use some tools from the stringr package as well as some base R functions:\n\nminnesota &lt;-\n  minnesota %&gt;% mutate(\n    ## Since all of the original county names have the same substring \" County, Minnesota\"\n    ## we can use the str_remove function to pull them out of all of them\n    county = str_remove(NAME, \" County, Minnesota\") %&gt;% \n      ## Since some of the counties  officially have two-word names (e.g. Big Stone)\n      ## which are collapsed in the radon dataset, we will use this function to remove all spaces:\n      str_replace_all(\" \", \"\") %&gt;% \n      ## A few county names include abbreviations indicated by the presence of a '.' (e.g. St. Louis)\n      ## so we will get rid of that bit of punctuation since it is not in the original data\n      str_replace_all(\"\\\\.\", \"\") %&gt;% \n      ## Finally, convert all the county names to uppercase\n      toupper()\n  )\n\nNow, the county labels should match:\n\nhead(sort(minnesota$county))\n\n[1] \"AITKIN\"   \"ANOKA\"    \"BECKER\"   \"BELTRAMI\" \"BENTON\"   \"BIGSTONE\"\n\n\n\n\nPreparing the radon dataset\nWe will repeat the steps from the earlier tutorial in order to prepare our data for analysis:\n\nradon &lt;- radon %&gt;% mutate(basement = 1 - floor)\n\ncounty_uranium &lt;- radon %&gt;%\n  group_by(county) %&gt;%\n  summarize(log_uranium = first(log_uranium), \n            mean_radon = mean(log_radon))\n\nBecause the sf dataset returned by tidycensus is a dataframe, we can then easily merge the county-level soil uranium concentrations we derived above into the shapefile. We use the left_join function from dplyr to ensure that all of the counties in the original shapefile are represented in the final dataset, even if a soil uranium measure is unavailable for them in the original data:\n\nminnesota_radon &lt;- left_join(minnesota, county_uranium)\n\nJoining with `by = join_by(county)`\n\n\nWe can then plot the log-uranium measures on the map and see that, in fact, they are quite spatially correlated. We can also see that there appear to be two counties which are missing soil uranium data in the radon dataset. To have a bit more control over our plots, we’ll switch here to using the geom_sf function of ggplot2, which makes plotting geographies from sf objects easy:\n\ng &lt;- ggplot(minnesota_radon) + \n  geom_sf(aes(fill = log_uranium )) + \n  scale_fill_viridis_c() +\n  ggtitle(\"Soil uranium by MN county\")\nplot(g)"
  },
  {
    "objectID": "spatial-radon.html#measuring-spatial-correlation",
    "href": "spatial-radon.html#measuring-spatial-correlation",
    "title": "11  Taking a spatial perspective on the radon data",
    "section": "11.4 Measuring Spatial Correlation",
    "text": "11.4 Measuring Spatial Correlation\nTo validate our hunch that soil uranium is spatially concentrated in Minnesota, we can calculate the value of Moran’s I for these data using some functions from the spdep package. First, we use the poly2nb function to obtain the neighbors for each polygon, which will be used to calculate Moran’s I.\n\nnb &lt;- poly2nb(minnesota_radon)\n\nThis function yields an R list in which each entry is a vector with the indices for the neighbors of the i-th county. For example, this prints the neighbors of the first three counties in the dataset:\n\n\nCode\nprint(nb[1:3])\n\n\n[[1]]\n[1] 12 47\n\n[[2]]\n[1] 27\n\n[[3]]\n[1]  8 24 46 57 67 76 83 87\n\n\nWe then pass this function to the nb2listw function to obtain weights for the relationships between neighbors. Here, we use the simplest option available, “B”, for binary weights equal to 1 if the areas are neighbors and 0 otherwise:\n\nlw &lt;- nb2listw(nb, style=\"B\", zero.policy=TRUE)\nprint(lw$weights[1:3])\n\n[[1]]\n[1] 1 1\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 1 1 1 1 1 1 1 1\n\n\nFinally, we can pass these weights, along with some additional information including the outcome of interest at each location, the total number of locations, and the sum of all the weights to the moran function. The NAOK=TRUE option used here also allows the function to drop locations where data are missing:\n\nradon_i &lt;- moran(minnesota_radon$log_uranium, lw, length(nb), Szero(lw),NAOK=TRUE)$I\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\n\nWhen we do this, we find that the value of Moran’s I = 0.71, which is close to the maximum value of 1. Since we’ll be returning to the calculation of Moran’s I using our spatial data, lets pack it up into a function:\n\nmoranFromSF &lt;- function(x, sfdf, style=\"B\") {\n  nb &lt;- poly2nb(sfdf)\n  lw &lt;- nb2listw(nb, style=style, zero.policy=TRUE)\n  mi &lt;- moran(x, lw, length(nb), Szero(lw), NAOK=TRUE)$I\n  return(mi)\n}\n\nprint(moranFromSF(minnesota_radon$log_uranium, minnesota_radon))\n\n[1] 0.712615\n\n\nOf course, our key quantity of interest isn’t soil uranium but the concentration of radon at the household level. When we constructed the county_uranium dataset above, we also calculated the median radon concentration in the data for each county. When we plot it, we see something similar to the soil uranium, but perhaps a bit less clear:\n\ng &lt;- ggplot(minnesota_radon) + \n  geom_sf(aes(fill = mean_radon )) + \n  scale_fill_viridis_c() +\n  ggtitle(paste0(\"Median household radon by MN county (I=\", round(moranFromSF(minnesota_radon$mean_radon, minnesota_radon),2), \")\"))\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\nplot(g)\n\n\n\n\nAs you can see in the figure, the value of Moran’s I is smaller than we got for log-uranium but still substantial.\n\n\n\n\n\n\nWhat’s going on?\n\n\n\nPause here and take a moment to try to figure out what might account for the difference in this intensity of clustering in radon vs. soil uranium measurements.\n\n\n\nTesting, testing\nOne way to determine whether the spatial aggregation of the radon measurements is meaningful is to compare it to a counterfactual scenario in which the distribution of radon concentrations is uncorrelated with space. This assumption, known as complete spatial randomness (or CSR), allows us to provide a benchmark against which we determine whether the value of Moran’s I we determined is highly likely to occur by chance alone. Thankfully, it is easy to generate a dataset in which the median radon values are distributed randomly across the map:\n\n## Make a new dataset representing 'random minnesota':\n## Use the sample function to resample household radon values without replacement,\n## we then recalculate county values based on these suffled values\ncounty_uranium_random &lt;- radon %&gt;%\n  mutate(log_radon = sample(log_radon, nrow(.), replace=FALSE)) %&gt;%\n  group_by(county) %&gt;%\n  summarize(log_uranium = first(log_uranium), \n            mean_radon = mean(log_radon))\n\nrandom_minnesota &lt;- left_join(minnesota, county_uranium_random)\n\nJoining with `by = join_by(county)`\n\n## Plot the new randomized data\ng &lt;- ggplot(random_minnesota) + \n  geom_sf(aes(fill = mean_radon )) + \n  scale_fill_viridis_c() +\n  ggtitle(paste0(\"Spatially randomized median radon by MN county (I=\", round(moranFromSF(random_minnesota$mean_radon, random_minnesota),2), \")\"))\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\nplot(g)\n\n\n\n\nThis yields something that looks pretty randomly distributed, which is reflected in a Moran’s I estimate closer to the null value of 0. This doesn’t necessarily tell us whether this result is meaningful rather than an artifact of random chance.\n\n\n\n\n\n\nWhat is the same? What is different?\n\n\n\nTake a minute to explore the distribution of different quantities between some random minnesotas and the observed one. For example, look at distributions of the number of observations per county, the proportion of households in each county that have basements, etc. Which are similar and which are different?\n\n\n\n\nComplete Spatial Randomness\nWhat we can do, though, is to generate a bunch of random Minnesotas in which there is no relationship between geographic location and median radon, calculate Moran’s I for each of those, and see how our observed data stack up.\n\ncsrMorans &lt;- function(radon, minnesota, trials = 1000, style=\"B\") {\n   county_uranium &lt;- radon %&gt;%\n    group_by(county) %&gt;%\n    summarize(log_uranium = first(log_uranium), \n            mean_radon = mean(log_radon)) %&gt;%\n     left_join(minnesota, .)\n   \n  nb &lt;- poly2nb(minnesota)\n  lw &lt;- nb2listw(nb, style=style, zero.policy=TRUE)\n  mv &lt;- moran(county_uranium$mean_radon, lw, length(nb), Szero(lw),NAOK=TRUE)$I\n  \n\n  moran_vals &lt;- rep(0, trials)\n  for (i in 1:trials) {\n\n    county_uranium_random &lt;- radon %&gt;%\n    mutate(log_radon = sample(log_radon, nrow(.), replace=FALSE)) %&gt;%\n    group_by(county) %&gt;%\n    summarize(log_uranium = first(log_uranium), \n            mean_radon = mean(log_radon))\n\n    random_minnesota &lt;- left_join(minnesota, county_uranium_random)\n    moran_vals[i] &lt;- moran(random_minnesota$mean_radon, lw, length(nb), Szero(lw),NAOK=TRUE)$I\n  }\n  \n  return(list(midist = moran_vals, \n              mi = mv))\n}  \n\ncsr_dist &lt;- csrMorans(radon, minnesota)\n\nWe can use the distribution of Moran’s I values taken from the randomized datasets to benchmark how likely our observed value is to occur by purely random chance. The figure below shows that this is quite unlikely:\n\ng &lt;- ggplot() + \n  geom_histogram(aes(x=csr_dist$midist),bins=50) + \n  xlab(\"Moran's I value\") + \n  geom_vline(xintercept=csr_dist$mi, colour = \"red\") + \n  geom_vline(xintercept = median(csr_dist$midist), colour = \"green\") + \n  ggtitle(\"Randomized values of Moran's I vs. observed for median household radon\")\n\nplot(g)\n\n\n\n\nAnd we can directly estimate this probability as follows:\n\nreal_moran &lt;- moranFromSF(minnesota_radon$mean_radon, minnesota_radon)\np_moran &lt;- sum(csr_dist$midist &gt;= csr_dist$mi)/length(csr_dist$midist)\nprint(p_moran)\n\n[1] 0\n\n\nFrom 1000 samples, it appears that none of our random datasets yielded a value of Moran’s I \\(\\ge\\) to the observed value, suggesting that it is unlikely that we would observe this value as a simple function of sampling variability.\n\n\n\n\n\n\nWhat could go wrong?\n\n\n\nBefore you move on, take a minute to think about what some of the potential flaws in our CSR-based approach to assessing the meaningfulness or signficance of this result might be."
  },
  {
    "objectID": "spatial-radon.html#models",
    "href": "spatial-radon.html#models",
    "title": "11  Taking a spatial perspective on the radon data",
    "section": "11.5 Models!",
    "text": "11.5 Models!\nUp to this point, we have relied on county-level summaries of the household-level radon data. For the final section of this tutorial, we are going to go back to using the full dataset and implement regression models that are able to characterize variation at the household and community level. Specifcially, we are going to first fit the full-pooling, no-pooling and partial pooling models from the original (1) paper. We won’t go into detail on these as they have been discussed in depth in the original paper and the previous post.\n\n\n\n\n\n\nConfused?\n\n\n\nFor more detail on the implementation on interpretation of these models, please check out Chapter 10.\n\n\n\nFull-pooling model\nThe full-pooling model has the following form, in which the variable \\(x_{ij}\\) indicates whether house \\(i\\) in county \\(j\\) has a basement (1) or not (0).\n\\[\ny_{ij} = \\alpha + \\beta x_{ij} + \\epsilon_{i}\n\\]\n\nfull_pooling_model &lt;- lm(log_radon ~ basement, data = radon) \nradon$full_pooling_resid &lt;- resid(full_pooling_model)\nradon$full_pooling_pred &lt;- predict(full_pooling_model)\n\n\n\n\n\n\n\nStoring Model Predictions\n\n\n\nNote that we are storing the residuals and predictions for this model (and the ones below) as a column inside the radon dataframe.\n\n\n\n\nNo Pooling\nThe no-pooling model assumes essentially that each county is indepenedent, and includes a categorical variable for the county that the observed household is in:\n\\[\ny_{ij} = \\alpha_j + \\beta x_{ij} + \\epsilon_{i}\n\\]\n\nno_pooling_model &lt;- lm(log_radon ~ basement + county, data = radon) \nradon$no_pooling_resid &lt;- resid(no_pooling_model)\nradon$no_pooling_pred &lt;- predict(no_pooling_model)\n\n\n\nPartial pooling model\nThe partial-pooling model is the multi-level analogue to the no-pooling model. For more detail, please see the partial pooling section of the original tutorial.\n\npartial_pool_model &lt;- stan_lmer(log_radon ~ basement + log_uranium + (1 | county), data = radon)\nradon$partial_pooling_resid &lt;- resid(partial_pool_model)\nradon$partial_pooling_pred &lt;- posterior_predict(partial_pool_model) %&gt;% apply(2,mean)"
  },
  {
    "objectID": "spatial-radon.html#residual-analysis",
    "href": "spatial-radon.html#residual-analysis",
    "title": "11  Taking a spatial perspective on the radon data",
    "section": "11.6 Residual Analysis",
    "text": "11.6 Residual Analysis\nOne thing that is important to note is that none of the regression models we are looking at directly account for spatial clustering. In other words, the spatial arrangement of the counties is not an input to the model. This doesn’t mean that they cannot adequately account for spatial correlation through the inclusion of key covariates, however.\nOne way to assess how well a model is accounting for observed and unobserved spatial hererogeneity is to examine the model residuals for evidence of spatial clustering, which is what we will do in this section.\nSince the residuals for each model are generated at the level of individual households, we will go back to working with county-level summaries of both the prediction error (residuals) and predicted household radon values:\n\nresults_by_county &lt;- radon %&gt;% \n  group_by(county) %&gt;%\n  summarize(p_basement = sum(basement)/n(),\n            full_pooling_resid = mean(full_pooling_resid),\n            no_pooling_resid = mean(no_pooling_resid),\n            full_pooling_pred = mean(full_pooling_pred),\n            no_pooling_pred = mean(no_pooling_pred),     \n            partial_pooling_pred = mean(partial_pooling_pred),\n            partial_pooling_resid = mean(partial_pooling_resid))\nresults_by_county &lt;- left_join(minnesota, results_by_county)\n\nJoining with `by = join_by(county)`\n\n\n\nFull Pooling Residuals\nWhen we look at the results of the full-pooling model, the residuals that still look pretty spatially clustered, and this is reflected in the value of Moran’s I &gt; 0:\n\nmi &lt;- round(moranFromSF(results_by_county$full_pooling_resid, results_by_county), 2)\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\ng &lt;- ggplot(results_by_county) + \n  geom_sf(aes(fill =full_pooling_resid )) + \n  scale_fill_viridis_c() + \n  ggtitle(paste0(\"Full pooling residuals with I=\", mi))\nplot(g)\n\n\n\n\nThis is probably intuitive: the full pooling model didn’t include any county-level information, so it might not account for all of the sptial variation. On the flipside, if we look at the predictions of the model - reflecting the expected household levels of radon in each county - should we should expect to find that they are spatially un-clustered or also clustered?\n\nmi &lt;- round(moranFromSF(results_by_county$full_pooling_pred, results_by_county), 2)\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\ng &lt;- ggplot(results_by_county) + \n  geom_sf(aes(fill = full_pooling_pred )) + \n  scale_fill_viridis_c() + \n  ggtitle(paste0(\"Full pooling predictions with I=\", mi))\nplot(g)\n\n\n\n\nWait - what? The predictions are also quite clustered, although the pattern looks a bit like a photographic negative of the residual map. It looks like our model is predicting lower values in the northwest corner of the state relative to the rest of the state. How is this possible, if our model doesn’t include contextual information?\nThis might be explained by differences in composition at the county level: maybe houses in some counties are more likely to have basements than in others? If this is the case, then those high-basement counties may have higher avg. levels of radon. So, lets just check and see if our one predictor - the presence or absence of a basement - exhibits any spatial variability?\n\nmi &lt;- round(moranFromSF(results_by_county$p_basement, results_by_county), 2)\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\ng &lt;- ggplot(results_by_county) + \n  geom_sf(aes(fill = p_basement )) + \n  scale_fill_viridis_c() + \n  ggtitle(paste0(\"Proportion of surveyed households with a basement, I=\", mi))\nplot(g)\n\n\n\n\nWhoops…that looks familiar! It seems like the pattern of spatial variation in the presence/absence of basements may be driving the clustering in our predictions and - by consequence - our residuals!\n\n\n\n\n\n\nSpatially correlated predictors → Spatially correlated predictions\n\n\n\nSometimes, it is easy to forget that the input data may be as or more correlated than the outcome data. In this example, the presence or absence of a basement in a house seems to have a spatial pattern and this impacts the spatial patterning of our predictions and model residuals!\n\n\nSo it looks like we are over-predicting risk in some areas where more surveyed households have basements and under-predicting it in other places where fewer households have basements.\n\n\nNo Pooling\nOk, so lets try this again with our no-pooling model which at least includes the counties as categorical covariates. Unless something weird is going on, this model should do a good job of explaining spatial variation:\n\n\nCode\nmi &lt;- round(moranFromSF(results_by_county$no_pooling_resid, results_by_county), 2)\n\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\n\nCode\ng &lt;- ggplot(results_by_county) + \n  geom_sf(aes(fill =no_pooling_resid )) +\n  scale_fill_viridis_c() + \n  ggtitle(paste0(\"No pooling residuals with I=\", mi))\nplot(g)\n\n\n\n\n\nWell, that’s a bit better, although it does such a good job at explaining away the overall variability in our measurments, we might be concerned that it is overfitting the model through the inclusion of the county level random effects. This is evidenced in the tiny size of the residuals and their minimal variation:\n\n\nCode\ng &lt;- ggplot() + \n  geom_histogram(aes(x=results_by_county$no_pooling_resid))\nplot(g)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nUnsurprisingly, this model does an excellent job of predicting the spatial patterns in the original data:\n\n\nCode\nmi &lt;- round(moranFromSF(results_by_county$no_pooling_pred, results_by_county), 2)\n\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\n\nCode\ng &lt;- ggplot(results_by_county) + \n  geom_sf(aes(fill =no_pooling_pred )) +\n  scale_fill_viridis_c() + \n  ggtitle(paste0(\"No pooling predictions with I=\", mi))\nplot(g)\n\n\n\n\n\nI love this model! It’s perfect! It captures almost the exact same clustering and spatial patterning of risk as the original data.\n\n\n\n\n\n\nDanger!\n\n\n\nWhat is problematic about this model? What limits its usefulness for both interpretation and prediction?\n\n\n\n\nPartial Pooling\nWhen we look at the predictions of the partial pooling model, they are notably smoother and more clustered than those of the full- and no-pooling models:\n\n\nCode\nmi &lt;- round(moranFromSF(results_by_county$partial_pooling_pred, results_by_county), 2)\n\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\n\nCode\ng &lt;- ggplot(results_by_county) + \n  geom_sf(aes(fill = partial_pooling_pred )) + \n  scale_fill_viridis_c() + \n  ggtitle(paste0(\"Partial pooling predictions with I=\", mi))\nplot(g)\n\n\n\n\n\nIf we compare this pattern and intensity of clustering to the log-uranium data, it is clear that the smoothness in the model predictions reflects the relative smoothness and clustering of the soil uranium data:\n\n\nCode\nmi &lt;- round(moranFromSF(minnesota_radon$log_uranium, minnesota_radon), 2)\n\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\n\nCode\ng &lt;- ggplot(minnesota_radon) + \n  geom_sf(aes(fill = log_uranium )) + \n  scale_fill_viridis_c() +\n  ggtitle(paste0(\"Soil uranium by MN county (I = \", mi, \")\") )\nplot(g)\n\n\n\n\n\nWhen we look at the residuals, they are still quite un-clustered - similar to the no pooling model, but their magnitude is larger, suggesting that the multi-level model is less suceptible to overfitting:\n\n\nCode\nmi &lt;- round(moranFromSF(results_by_county$partial_pooling_resid, results_by_county), 2)\n\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\n\nCode\ng &lt;- ggplot(results_by_county) + \n  geom_sf(aes(fill = partial_pooling_resid )) + \n  scale_fill_viridis_c() + \n  ggtitle(paste0(\"Partial pooling residuals with I=\", mi))\nplot(g)\n\n\n\n\n\nBy contrast, the aggregated residuals at the county level are less indicative of overfitting than the no-pooling model, but are still a bit fat-tailed, suggesting that some counties may still be over- or under-fit.\n\n\nCode\ng &lt;- ggplot() + \n  geom_histogram(aes(x=results_by_county$partial_pooling_resid))\nplot(g)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`)."
  },
  {
    "objectID": "spatial-radon.html#whats-next",
    "href": "spatial-radon.html#whats-next",
    "title": "11  Taking a spatial perspective on the radon data",
    "section": "11.7 What’s next?",
    "text": "11.7 What’s next?\nIn this tutorial, we have thoroughly reviewed the spatial implications of the three types of models reviewed in the (1) analysis of household-level radon in Minnesota. While our results suggest that the partial-pooling model provides the most compelling explanation of spatial variability in our data, we are not done yet! In the next tutorial, we will look specifically at the predictive capabilities of each of these models and use the ability to predict risk for counties in which household-level measures are unavaialble or missing as the final guide in our odyssey of model comparison."
  },
  {
    "objectID": "spatial-radon.html#references",
    "href": "spatial-radon.html#references",
    "title": "11  Taking a spatial perspective on the radon data",
    "section": "References",
    "text": "References\n\n\n\n\n1. Gelman A. Multilevel (Hierarchical) Modeling: What It Can and Cannot Do. Technometrics [electronic article]. 2006;48(3):432–435. (http://www.tandfonline.com/doi/abs/10.1198/004017005000000661). (Accessed December 15, 2019)"
  }
]