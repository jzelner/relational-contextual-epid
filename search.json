[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Field Guide of Relational and Contextual Epidemiology",
    "section": "",
    "text": "1 Inspirations and Influences\nWhat - if any - kind of book needs to be written about relational and contextual epidemiology?"
  },
  {
    "objectID": "index.html#why-relational-and-contextual-epidemiology",
    "href": "index.html#why-relational-and-contextual-epidemiology",
    "title": "Field Guide of Relational and Contextual Epidemiology",
    "section": "Why relational and contextual epidemiology?",
    "text": "Why relational and contextual epidemiology?\nThere is a hole in the epidemiological literature that limits our ability to come to grips with the relational aspects of health and illness. For the purposes of this book, relationships are sources of non-independence. These could be social influences, e.g. from our friends and family. But they could also be spatial in nature, e.g. a result of some physically proximate environmental exposure. Often, the relationships we care about are temporal in nature, as is evident in the lifecourse perspective in which early-life exposures are understood to impact later-life outcomes.\nThere are many good books and papers out there about these topics as well as some of their conjunctions with each other (e.g. spatiotemporal analysis). But there is less written about their overlaps and commonalities, why knowing one teaches you so much about the others, and what having access to this set of tools might mean for a working epidemiologist.\nIn many ways, my goal in writing this is selfish, to satisfy a personal curiosity I suspect is of some greater import to at least a few people. Specifically: What does it mean to do contextual or relational epidemiology? Are these meaningless terms that just appeal to the social and natural science centers of my brain, or does this tap into something more meaningful?\n\nMore than methods\nPart of my inspiration is a reaction to the technocratic impulse and imperative in modern epidemiology. In particular, I find myself a bit paralyzed by the worry about what happens when we operate under the assumption that escalating methodological complexity is an imperative and that the road out of socio-epidemiological problems is paved with technological solutions.\n\n\nBut also, methods…\nOn the other side, simply put, I love the methodological tools of spatial epidemiology, Bayesian hierarchical analysis, and systems modeling. I have learned more than I ever could have hoped through learning, tinkering with, and applying these tools to problems in the real world and in my own head. But for me, the large majority of lessons learned have been from their conceptual isomorphisms (or conflicts) with the world as it appears to us through qualitative and quantitative data, rather than in the exact values of their parameter values and quantitative predictions.\nFor me, this book is about resolving this cognitive dissonance while providing useful ‘how-to’ pointers along the way. I hope to articulate the affirmative case for a systems-based, contextually-sensitive, justice-oriented, morally and ethically opinionated, and theoretically driven approach to epidemiology. Along the way, I hope to show why the tools of such an approach are necessarily heterogeneous in nature and require us to accept uncertaintities quantitative and epistemic."
  },
  {
    "objectID": "index.html#influences",
    "href": "index.html#influences",
    "title": "Field Guide of Relational and Contextual Epidemiology",
    "section": "Influences",
    "text": "Influences\nThere are any number of books and papers out there that have articulated a similar perspective on the tools of quantitative analysis. The following have been particularly important for my own thinking and their influences will be felt throughout this work:\n\nStatistical Rethinking (1)\nThe Ecological Detective (2)\nARM/Regression and Other Stories (3)\n\nWhat makes these works so useful, strong, and enduring is the way that they articulate a coherent, opinionated perspective on the meaning and use of a set of methodological tools. On top of that, they are engaging and fun to read - the sort of thing you return to over time not just to get specific methodological tools, but to be exposed to their perspective."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Field Guide of Relational and Contextual Epidemiology",
    "section": "References",
    "text": "References\n\n\n\n\n1. McElreath R. Statistical Rethinking: A Bayesian Course with Examples in R and STAN. 2nd edition. Boca Raton: Chapman and Hall/CRC; 2020.\n\n\n2. Hilborn R, Mangel M. The Ecological Detective: Confronting Models with Data. 1st edition. Princeton, NJ: Princeton University Press; 1997.\n\n\n3. Gelman A. Regression and Other Stories. 1st edition. Cambridge: Cambridge University Press; 2020."
  },
  {
    "objectID": "invitation.html#you-are-almost-certainly-a-relational-analyst-already",
    "href": "invitation.html#you-are-almost-certainly-a-relational-analyst-already",
    "title": "2  An invitation",
    "section": "You are (almost certainly) a relational analyst already",
    "text": "You are (almost certainly) a relational analyst already\nMy goal in this short essay is to chip away a bit at the idea of spatial/relational epidemiology as something separate and apart from mainstream epidemiology and public health. Instead, I argue that these are better understood as a loose wrapper around a core set of ideas and tools that are part of the working arsenal of most professionals, students, and researchers in public health.\nTo explain what I mean, and why I think it’s important, I’m going to subject you to a bit of autobiography about my own intellectual and professional trajectory."
  },
  {
    "objectID": "invitation.html#maps-gateway-or-destination",
    "href": "invitation.html#maps-gateway-or-destination",
    "title": "2  An invitation",
    "section": "Maps: Gateway or destination?",
    "text": "Maps: Gateway or destination?\nI began working as an epidemiologist or epidemology-adjacent type as a PhD student at the University of Michigan some time around 2006. As part of my dissertation research, I worked with Joe Eisenberg on an analysis ofthe role of social networks as sources of risk and protection against diarrheal disease and other infections in a group of villages in an area of rural Ecuador:\n\n\n\nFigure 2.2: A figure from my dissertation research representing hypothesized relationships between village context (represented by inaccessibility or ‘remoteness’) and variation in disease outcomes within and between villages. (From (1))\n\n\nLooking back, this was indisputably a spatial analysis: We were interested in how local social contexts impacted variation in health outcomes across a set of 20+ villages and also how within-village variation in social connectivity impacted risk within villages.\nWe employed multi-level data about common characteristics of individual villages, households, and the individuals within them. But at this time, I thought of myself as doing a few things, but none of them were spatial:\n\nInfectious disease epidemiology: Why and how do people become infected with various pathogens?\nSocial epidemiology: How do social relationships impact disease outcomes?\nNetwork analysis: How does the structure of relationships impact individual and community health?\n\nAs it happened, all of these things were correct. But what I didn’t really understand at the time was that the collection of these different approaches into a single analysis made it spatial or geographic in nature, even if I didn’t realize it"
  },
  {
    "objectID": "invitation.html#a-process-of-progressive-revelation",
    "href": "invitation.html#a-process-of-progressive-revelation",
    "title": "2  An invitation",
    "section": "A process of progressive revelation",
    "text": "A process of progressive revelation\nAs a postdoc, working with Ted Cohen, I began analyzing data from a large study of household-level tuberculosis transmission in Lima, Peru. Figure 2.3 illustrates the model we developed to characterize household-level differences in transmission rates as a function of exposure type:\n\n\n\nFigure 2.3: Characterizing household-level variation in risks of infection from community vs. household exposures (Figure from (2))\n\n\nAt the time, I knew that these households were distributed across neighborhoods of Lima, but I didn’t give it much thought. I was more interested in risks experienced by an average household. And to be honest, I didn’t know that spatial metadata were available on each of the households, since I wasn’t involved with the data collection!\nIn the interim, I got the chance to work on some collaborative projects with an explicitly spatial focus. In one, we reconstructed an outbreak of morbillivirus (think: measles) among a herd of migratory dolphins (Figure 2.4).\n\n\n\nFigure 2.4: Locations of dolphin strandings during a morbillivirus outbreak in the North Atlantic (dot size indicates a greater number of strandings; Figure from (3))\n\n\nIn another, we looked at the relationship between environmental risks, such as neighborhood-level flooding, on the rate of pediatric diarrheal disease in Ho Chi Minh City in Vietnam (Figure 2.5).\n\n\n\nFigure 2.5: Incidence of pediatric diarrhea across neighborhoods of Ho Chi Minh City, Vietnam (Figure from (4))\n\n\nThese were the first experiences I had explicitly looking at these outcomes as a function of geographic space. While I had previously thought that mapping and spatial analysis and health geography were big scary things I couldn’t do, I started to realize something important: These projects were not substantively very distinct from ones I had done before. The difference was that we were explicitly talking about spatial relationships and making maps (or simple one-dimensional diagrams as in Figure 2.4), instead of implicitly as in Figure 2.2 or Figure 2.3.\nAfter completing these other projects, I dove back in to the Lima TB data to look at the drivers of multi-drug resistant TB (MDR-TB) risk. This was when I finally found out (some 2 years after I had started working with these data!) that spatial information on each household was available. So, with great trepidation, for the first time I made a map to explore spatial variability in MDR-TB outcomes.\nAnd when I did this, we instantly saw that there were seemingly meaningful differences in the rate of TB overall, and MDR-TB in particular, across different health center catchment areas:\n\n\n\nFigure 2.6: The first map(s) I ever made, from (5), nearly 10 years after I started my research career.\n\n\nThis was the moment, some 10 years after I dipped my toes into the world of infectious disease epidemiology, where I realized I had been doing spatial work all along."
  },
  {
    "objectID": "invitation.html#so-what",
    "href": "invitation.html#so-what",
    "title": "2  An invitation",
    "section": "So what?",
    "text": "So what?\nWhy am I bothering you with this tedious and indulgent bit of personal history? It’s because it took me way too long to recognize that spatial epidemiology was a wrapper around a set of skills and ideas I had been working with for many years before I recognized what I was doing. I was intimidated by anything preceded by ‘spatial-’: it sounded like a bunch of skills I didn’t have and couldn’t acquire.\nMy belated realization about the emergent quality of spatial epidemiology, and its broader connection to relational thinking in public health and the social sciences, has been crucially important for me. It made me realize that when I push into new areas - in life as much as research - that I probably have more of the tools I need than I realized in advance.\nThis means that you don’t need to identify as a spatial or network or time series analyst to be one. And if you want to think of yourself as one, you should, because ultimately it is the intention to engage with the spatial, social and temporal relationships that drive health outcomes that makes the difference. This is likely true for many if not most scientific subfields1, but this one is mine and I’m glad I finally realized it!"
  },
  {
    "objectID": "invitation.html#references",
    "href": "invitation.html#references",
    "title": "2  An invitation",
    "section": "References",
    "text": "References\n\n\n\n\n1. Zelner JL, Trostle J, Goldstick JE, et al. Social Connectedness and Disease Transmission: Social Organization, Cohesion, Village Context, and Infection Risk in Rural Ecuador. American Journal of Public Health [electronic article]. 2012;102(12):2233–2239. (http://ajph.aphapublications.org/doi/10.2105/AJPH.2012.300795). (Accessed December 15, 2019)\n\n\n2. Zelner JL, Murray MB, Becerra MC, et al. Age-Specific Risks of Tuberculosis Infection From Household and Community Exposures and Opportunities for Interventions in a High-Burden Setting. American Journal of Epidemiology [electronic article]. 2014;180(8):853–861. (https://academic.oup.com/aje/article-lookup/doi/10.1093/aje/kwu192). (Accessed December 15, 2019)\n\n\n3. Morris SE, Zelner JL, Fauquier DA, et al. Partially observed epidemics in wildlife hosts: Modelling an outbreak of dolphin morbillivirus in the northwestern Atlantic, June 2013. Journal of The Royal Society Interface [electronic article]. 2015;12(112):20150676. (https://royalsocietypublishing.org/doi/10.1098/rsif.2015.0676). (Accessed December 15, 2019)\n\n\n4. Thompson CN, Zelner JL, Nhu TDH, et al. The impact of environmental and climatic variation on the spatiotemporal trends of hospitalized pediatric diarrhea in Ho Chi Minh City, Vietnam. Health & Place [electronic article]. 2015;35:147–154. (https://linkinghub.elsevier.com/retrieve/pii/S1353829215001094). (Accessed December 15, 2019)\n\n\n5. Zelner JL, Murray MB, Becerra MC, et al. Identifying Hotspots of Multidrug-Resistant Tuberculosis Transmission Using Spatial and Molecular Genetic Data. Journal of Infectious Diseases [electronic article]. 2016;213(2):287–294. (https://academic.oup.com/jid/article-lookup/doi/10.1093/infdis/jiv387). (Accessed December 15, 2019)"
  },
  {
    "objectID": "invitation.html#footnotes",
    "href": "invitation.html#footnotes",
    "title": "2  An invitation",
    "section": "",
    "text": "To be fair, you probably need to be working near-ish to a field for it to happen by chance: there is little chance of me taking on the characteristics a particle physicist or chemical engineer by chance, but I wouldn’t rule out lepidopterist or archaeologist entirely!↩︎"
  },
  {
    "objectID": "elements.html#problem-orientation-is-nonnegotiable",
    "href": "elements.html#problem-orientation-is-nonnegotiable",
    "title": "3  What are the elements of relational epidemiology?",
    "section": "Problem-orientation is nonnegotiable",
    "text": "Problem-orientation is nonnegotiable\n\n“A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.” - Douglas Adams, Hitchhikers Guide to the Galaxy.\n\nSometimes, methods-y topics in epidemiology and public health are boiled down to a sequence of steps to be applied to each new problem. In the worst cases, they come to us as a series of copy-paste, plug-and-chug pieces of code to be reused each time the same type of problem is encountered.\nBeyond being a boring way to learn, this has the effect of putting the methods at the front of the train, with the question or problem implicitly assumed to be an opportunity or excuse to employ the model.\nThis is an approach that can get you some publications and maybe a little bit of clout within the musty world of academia, but it doesn’t do much to solve the types of problems working epidemiologists face. And in truth, it doesn’t even work so well on the academic side of the fence.\nInside the universe of this book, the problem is the first and most important thing. The question is the fixed point against which our analytic approaches are chosen. This means that there are no fixed methodological answers to applied questions: Our methodological approaches and tools must be as diverse and heterodox as the questions the world throws at us.\nMaking sense of the types of patterns we see in the real world requires us to first identify:\n\nA question we want or need to answer.\nThe most important types of relationships impacting our outcome of interest (time, space, individual-to-individual).\nA methodological approach that will allow us to characterize the impact of those relationships on the outcome we care about.\n\n\nA methodological caboose\nIt is important to note that the choice of method comes last here: A key motivation of this book is to sidestep the tendency to train ourselves into methodological hammers looking for data nails to whack away at.\nIn addition to this, there is no pre-supposition that the appropriate method is necessarily ‘fancy’ in the sense of being conceptually or mathematically complex, computationally intensive, or even primarily or at all quantitative in nature. Whether this is the case is entirely a function of what happens at the intersection between question, data, and theory."
  },
  {
    "objectID": "relationships.html",
    "href": "relationships.html",
    "title": "Relationships",
    "section": "",
    "text": "Space, time, and network"
  },
  {
    "objectID": "social.html#social-stratification-inequality",
    "href": "social.html#social-stratification-inequality",
    "title": "5  Social",
    "section": "Social Stratification & Inequality",
    "text": "Social Stratification & Inequality"
  },
  {
    "objectID": "social.html#social-networks",
    "href": "social.html#social-networks",
    "title": "5  Social",
    "section": "Social Networks",
    "text": "Social Networks"
  },
  {
    "objectID": "spatial.html#physical-space",
    "href": "spatial.html#physical-space",
    "title": "6  Spatial",
    "section": "Physical Space",
    "text": "Physical Space"
  },
  {
    "objectID": "spatial.html#social-space",
    "href": "spatial.html#social-space",
    "title": "6  Spatial",
    "section": "Social Space",
    "text": "Social Space"
  },
  {
    "objectID": "temporal.html#short-term-correlation-and-fluctuation",
    "href": "temporal.html#short-term-correlation-and-fluctuation",
    "title": "7  Temporal",
    "section": "Short-term correlation and fluctuation",
    "text": "Short-term correlation and fluctuation"
  },
  {
    "objectID": "temporal.html#time-series",
    "href": "temporal.html#time-series",
    "title": "7  Temporal",
    "section": "Time series",
    "text": "Time series"
  },
  {
    "objectID": "temporal.html#history",
    "href": "temporal.html#history",
    "title": "7  Temporal",
    "section": "History",
    "text": "History"
  },
  {
    "objectID": "smoothing.html#notation-and-terminology",
    "href": "smoothing.html#notation-and-terminology",
    "title": "8  Codifying Tobler’s First Law using Locally Weighted Regression",
    "section": "Notation and Terminology",
    "text": "Notation and Terminology\nIn this example, we are interested in visualizing and predicting the values of a function \\(f(x_i)\\) which outputs values \\(y_i\\), the expected value of the output function.\n\\[\ny_i = f(x_i)\n\\]\nLets start by getting the values of \\(f(x)\\) for every input value \\(x\\). For simplicity, we will assume that \\(f(x)\\) is a sine function and that the values of \\(x\\) go from -1 to +5, allowing us to observe one half cycle of the sine function:\n\nx &lt;- seq(-1, 5, by = 0.1)\ny &lt;- sin(x)\nplot(x,y)\n\n\n\n\nYou can see right away that this simple curve pretty neatly expresses Tobler’s first law: \\(f(x)\\) values of each point are in general more similar to each other for nearby values of \\(x\\). If we want to press this idea into real-world practice, we need a model that can translate TFL into quantitative estimates and qualitative visualizations. There are lots of ways to do this, but we’ll focus in on locally-weighted regression, also known LOWESS.\nThe basic idea of a LOWESS regression is to define a window of size \\(k\\) points around each value one wishes to estimate, and calculate a weighted average of the value of those points, which can then be used as the estimated value \\(\\hat{y_j} \\approx f(x_j)\\). We then run the values of these nearest neighbors through a weight function \\(w(x)\\).\nThese weight functions can take a lot of different forms, but we’ll start simple with a uniform one, i.e. just taking the average of the \\(k\\) nearest neighbors, so that \\(\\hat{y} = sum(z(x_i, k))/k\\), where \\(KNNz\\) is a function returning the \\(y\\) values of the k nearest observations to \\(x_i\\). The value of \\(k\\) is sometimes referred to as the bandwidth of the smoothing function: Larger bandwidths use more data to estimate values at each point, smaller ones use less.\n\nMaking some locally weighted estimates\nUsing the fnn package for R, we can find the indices of the \\(k\\) nearest neighbors of each point we want to make an estimate at:\n\nlibrary(FNN)\nk &lt;- 10\nz &lt;- knn.index(x, k=k)\n\nYou can read the output of this function, below, as indicating the indices (\\(i\\)) of the 10 nearest points to each of the values of \\(x\\).\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    2    3    4    5    6    7    8    9   10    11\n [2,]    1    3    4    5    6    7    8    9   10    11\n [3,]    2    4    1    5    6    7    8    9   10    11\n [4,]    5    3    6    2    1    7    8    9   10    11\n [5,]    6    4    7    3    8    2    1    9   10    11\n [6,]    5    7    4    8    3    9    2   10    1    11\n [7,]    8    6    9    5   10    4   11    3   12     2\n [8,]    7    9   10    6   11    5    4   12    3    13\n [9,]   10    8   11    7   12    6    5   13   14     4\n[10,]    9   11    8   12    7   13   14    6    5    15\n\n\nWe can visualize this by picking a point in the middle of the series and its 10 nearest neighbors and the estimated value of \\(\\hat{y_i}\\) obtained by just taking the average of the k nearest points:\n\nlibrary(ggplot2)\n\n## Plot the original data\ng &lt;- ggplot() + geom_point(aes(x=x, y = y)) + \n  xlab(\"x\") + ylab(\"f(x)\")\n\n## Now, get the index for x = 2\nx_index &lt;- which(x==2)\n\n## Show the range of k nearest neighbors of this point\nknn_low &lt;- min(x[z[x_index, ]])\nknn_high &lt;- max(x[z[x_index, ]])\ny_hat &lt;- mean(y[z[x_index,]], )\n\n## Add errorbars to the figure to show the 10 nearest values with the height of the point indicating the estimated value at y_i, denoted by the red dot\ng &lt;- g + geom_errorbarh(aes(xmin = knn_low, xmax = knn_high, y = y_hat)) + geom_point(aes(x=2,y_hat), colour = \"red\")\n\nplot(g)\n\n\n\n\nNotice that if the knn function is applied at the low end of the series, i.e. to the first value, it will use points to the right of that one instead of to either side:\n\nlibrary(ggplot2)\n\n## Plot the original data\ng &lt;- ggplot() + geom_point(aes(x=x, y = y)) + \n  xlab(\"x\") + ylab(\"f(x)\")\n\n## Use the index for the lowest value\nx_index &lt;- 1\n\n## Show the range of k nearest neighbors of this point\nknn_low &lt;- min(x[z[x_index, ]])\nknn_high &lt;- max(x[z[x_index, ]])\ny_hat &lt;- mean(y[z[x_index,]], )\n\n## Add errorbars to the figure to show the 10 nearest values with the height of the point indicating the estimated value at y_i\ng &lt;- g + geom_errorbarh(aes(xmin = knn_low, xmax = knn_high, y = y_hat)) + geom_point(aes(x=x[x_index],y_hat), colour = \"red\")\n\nplot(g)\n\n\n\n\nNow, lets see what happens if we run our smoother over the whole series and take the average of the 10 nearest points for each and compare them to the observed data:\n\ny_hat &lt;- rep(0, length(x))\nfor (i in 1:length(x) ) {\n  y_hat[i] &lt;- mean(y[z[i,]], )\n}\n\nNow plot the predicted vs. the observed values:\n\ng &lt;- ggplot() + geom_point(aes(x=x, y = y)) + \n  xlab(\"x\") + ylab(\"f(x)\") + geom_line(aes(x=x, y=y_hat), colour = \"red\")\nplot(g)\n\n\n\n\nYou can see this does a pretty good job all the way through, except at the edges. Lets try it again with a smaller window - or bandwidth - of 5 and see what happens. First, we’ll write a function that will give us the predicted value of y at each point given a window of size k and an input value:\n\nknn_est &lt;- function(x, y, k) {\n  z &lt;- knn.index(x, k=k)\n  y_hat &lt;- rep(0, length(x))\n  \nfor (i in 1:length(x) ) {\n  y_hat[i] &lt;- mean(y[z[i,]], )\n}\n\n  df &lt;- data.frame(x=x, y = y, yhat = y_hat)\n  return(df)\n}\n\n\npred_df &lt;- knn_est(x, y, 5)\ng &lt;- ggplot(pred_df) + geom_point(aes(x=x, y=y)) + geom_line(aes(x=x,y=yhat),colour=\"red\")\nplot(g)\n\n\n\n\nThis gets rid of a lot of the weird effects at the edges but introduces some noise into the function. What if we make the window bigger, say 20, to get rid of some of the noise?\n\n\n\n\n\nThis seems to make the edge effects worse, as well as the estimates of the function overall worse.\nWhat happens if we go in the opposite direction and shrink the window down to 2?\n\n\n\n\n\n\n\nDiscussion Questions\n\nWhy does this appear to be more accurate for these data than \\(k=10\\) and \\(k=5\\)?\nWhat would happen if we added observation noise to the values of \\(y_i\\)? Which one of the smoothers do you think would work better then?\nIs the one best value of \\(k\\) for all datasets? How might you go about picking the best one?\nHow does our uniform weight function express Tobler’s first law? What kind of weight function \\(w(x)\\) might do a better job of capturing the notion of distance decay?"
  },
  {
    "objectID": "smoothing.html#references",
    "href": "smoothing.html#references",
    "title": "8  Codifying Tobler’s First Law using Locally Weighted Regression",
    "section": "References",
    "text": "References\n\n\n\n\n1. Tobler WR. A Computer Movie Simulating Urban Growth in the Detroit Region. Economic Geography [electronic article]. 1970;46:234–240. (http://www.jstor.org/stable/143141). (Accessed January 13, 2022)"
  },
  {
    "objectID": "density.html#a-motivating-example",
    "href": "density.html#a-motivating-example",
    "title": "9  Spatial Density",
    "section": "A motivating example",
    "text": "A motivating example\nA spatial transect is an area of space along a line crossing a landscape. These are often used in ecology and forestry to assess the health of an environment, species diversity and other factors. Using a transect can help simplify the problem of spatial analysis down to one dimension rather than the usual two, while still providing a tremendous amount of useful information.\n\n\n\nExample of an ecological transect from the US National Park Service (source)\n\n\nFor example, (1) were interested in characterizing the intensity of exposure to triatomine bugs and other insect vectors of the pathogen T. cruzi, which causes Chagas disease.\n\n\n\nTriatoma (left- and right-hand panels) and T. cruzi (center) (source)\n\n\n\n\n\nIntensity of Triatomine infestation along a 2km transect in Arequipa, Peru (Figure from (1))\n\n\nImagine we are estimating the density of some unknown insect vector along a 1 kilometer transect with the goal of characterizing the risk of infection with a vector-borne illness."
  },
  {
    "objectID": "density.html#kernel-density-estimation-in-one-dimension",
    "href": "density.html#kernel-density-estimation-in-one-dimension",
    "title": "9  Spatial Density",
    "section": "Kernel density estimation in one dimension",
    "text": "Kernel density estimation in one dimension\nMuch like in our discussion of kernel smoothing of continuous outcomes, kernel functions play a key role in this setting as well. In this case, imagine that the locations of vectors along our transect have been sampled at random from some unknown function \\(f(x)\\) which takes values from 0 (the beginning of the transect) to 1000m (the end).\nWe can use the Kernel function \\(K(d)\\) to approximate the intensity of the outcome of interest at each observed case location \\(x_i\\). Imagine that our observed data have locations \\(x_1, x_2, \\ldots, x_n\\) and that the distance between our point of interest, \\(x_j\\) and each observed point is \\(d_{ij} = | x_j - x_i |\\).\nFinally, lets include a bandwidth parameter, \\(h\\), which controls the width of the window we will use for smoothing. When we put this all together, we can get an estimate of the density of our outcome of interest at location \\(x_j\\) as follows:\n\\[\n\\hat{f_h}(x_j) = \\frac{1}{n} \\sum_{i=1}^{n} K(\\frac{x_j - x_i}{h})\n\\]\nAs you can see below, we can pick a range of kernel functions, but for the sake of simplicity, in this example, we will focus in on a Gaussian, or normal, kernel, which uses the probability density function of a normal distribution to weight points.\nLets start by sampling locations of observed points along a one dimensional line. To keep things interesting, we’ll use a Gaussian mixture distribution with two components:\n\n\n\nComparison of different kernel functions (source)"
  },
  {
    "objectID": "density.html#worked-example",
    "href": "density.html#worked-example",
    "title": "9  Spatial Density",
    "section": "Worked example",
    "text": "Worked example\nFirst, lets imagine a scenario in which the risk of observing an insect vector steadily decreases as we walk along our transect. However, along the way there is a hotspot of increased risk beyond what we would expect from the smooth decline before and after that spot. For the purpose of this example, we’ll assume that risk decays exponentially with distance from the origin, but that our hotspot is centered at a point 300 meters into the transect. The code below lets us sample the locations of the points along the transect where 🐜 are observed from two distributions:\n\nAn exponential distribution representing smooth decay from the beginning to the end of the transect, and\nA normal distribution representing a hotspot about 150m in width beginning 300m in\n\nThe figure below shows a histogram of locations sampled from \\(f(x)\\) (vertical bars) overlaid with the true value of \\(f(x)\\) in red:\n\n\nCode\nlibrary(ggplot2)\nd_a &lt;- dexp(1:1000, rate = 1/250) \nd_b &lt;- dnorm(1:1000, mean = 300, sd = 50)\ny &lt;- ((1-p_hot))*d_a + (p_hot*d_b)\n\ndens_df &lt;- data.frame(x = 1:1000, y = y)\nxdf &lt;- data.frame(x=x)\n\n\ng &lt;- ggplot(xdf) + geom_histogram(aes(x=x, y=..density..), bins=100) + \ngeom_line(data=dens_df, aes(x=x,y=y), colour=\"red\") +\nxlim(0, 1000) + ylab(\"Density\") + xlab(\"Distance from transect origin (m)\")\nplot(g)\n\n\n\n\n\nNow, imagine we have another set of finely spaced points along the line, and for each, we want to calculate the weight for each. The function below lets us do that:\nThe figure below shows the true value of our density function \\(f(x)\\) in red, the density of points in the simulated data along the x-axis of the ‘rug plot’, and our smoothed density in black, for a bandwidth of \\(h=10\\):\n\n\nCode\nlibrary(ggplot2)\npred_df &lt;- normal_smoother(x, h = 10)\n\ng &lt;- ggplot() + geom_rug(aes(x=x)) + \ngeom_line(data = pred_df, aes(x=x, y=y)) + \nylab(\"Density\") + geom_line(data = dens_df, aes(x=x,y=y), colour=\"red\") + \nxlim(0, 1000)\ndens_ojs &lt;- dens_df\ndens_ojs$y &lt;- dens_ojs$y*cc\nplot(g)\n\n\n\n\n\nNow, lets see what happens if we try this for different values of \\(h\\):\n\n\nCode\nall_df &lt;- data.frame()\nfor (hv in c(5, 10, 20, 50 ,100, 250)) {\n  pred_df &lt;- normal_smoother(x, h = hv)\n  pred_df$h &lt;- hv\n  all_df &lt;- rbind(all_df, pred_df) \n}\n\n  all_df$h &lt;- as.factor(all_df$h)\n\ng &lt;- ggplot(all_df) + geom_rug(aes(x=x)) + \ngeom_line(data = dens_df, aes(x=x,y=y), colour=\"red\") + \ngeom_line(aes(x=x, y=y)) + \nylab(\"Density\") + \nfacet_wrap(~ h) +\nxlim(0, 1000)\n\nplot(g)\n\n\n\n\n\n\n\nCode\nall_df &lt;- data.frame()\nhvals &lt;- seq(1, 100, by = 2)\ndistvals &lt;- seq(-100, 100, by = 1)\nall_kernvals &lt;- data.frame()\nfor (hv in hvals) {\n  pred_df &lt;- normal_smoother(x, h = hv)\n  pred_df$h &lt;- hv\n  pred_df$smoother &lt;- \"gaussian\"\n  all_df &lt;- rbind(all_df, pred_df) \n  all_kernvals &lt;- rbind(all_kernvals,data.frame(x=distvals, y=kdgaussian(distvals, bw = hv), smoother = \"gaussian\", h = hv))\n  \n  pred_df &lt;- normal_smoother(x, h = hv, kern = kduniform, kernp=kpuniform)\n  pred_df$h &lt;- hv\n  pred_df$smoother &lt;- \"uniform\"\n  all_df &lt;- rbind(all_df, pred_df) \n  all_kernvals &lt;- rbind(all_kernvals,data.frame(x=distvals, y=kduniform(distvals, bw = hv), smoother = \"uniform\", h = hv))\n\n  \n  pred_df &lt;- normal_smoother(x, h = hv, kern = kdtricube, kernp=kptricube)\n  pred_df$h &lt;- hv\n  pred_df$smoother &lt;- \"tricube\"\n  all_df &lt;- rbind(all_df, pred_df) \n    all_kernvals &lt;- rbind(all_kernvals, data.frame(x=distvals, y=kdtricube(distvals, bw = hv), smoother = \"tricube\", h = hv))\n    \n  pred_df &lt;- normal_smoother(x, h = hv, kern = kdtriangular, kernp=kptriangular)\n  pred_df$h &lt;- hv\n  pred_df$smoother &lt;- \"triangular\"\n  all_df &lt;- rbind(all_df, pred_df) \n    all_kernvals &lt;- rbind(all_kernvals, data.frame(x=distvals, y=kdtriangular(distvals, bw = hv), smoother = \"triangular\", h = hv))\n\n}\nall_df$y &lt;- all_df$y * cc"
  },
  {
    "objectID": "density.html#trying-different-bandwidths-and-kernels",
    "href": "density.html#trying-different-bandwidths-and-kernels",
    "title": "9  Spatial Density",
    "section": "Trying different bandwidths and kernels",
    "text": "Trying different bandwidths and kernels\nYou can adjust the range of the bandwidth here to get a better sense of the relationship between the smoothed curve (black) and true density (red). Adjust the bin width for the histogram of the underlying data to get a sense of the fit of the model to the underlying data.\n\nviewof h = Inputs.range([1, 100], {value: 10, step: 2, label: \"Bandwidth (m)\"})\nviewof bw = Inputs.range([5, 100], {value: 10, step: 5, label: \"Bin width (m)\"})\nviewof kern = Inputs.select([\"gaussian\", \"uniform\", \"tricube\", \"triangular\"], {value: \"gaussian\", label: \"Smoothing kernel\"})\n\nnumbins = Math.floor(1000/bw)\n\ndtrans = transpose(hvals)\nPlot.plot({\ny: {grid: true, \nlabel: \"Density\"},\nx: {\nlabel: \"Distance from transect start (m) →\"\n},\n    marks: [\n    Plot.rectY(transpose(sample),Plot.binX({y: \"count\"}, {x: \"loc\", fill: \"steelblue\", thresholds: numbins})),\n    Plot.lineY(dtrans, {filter: d =&gt; (d.h == h) && (d.smoother == kern), curve: \"linear\", x:\"x\",y: d =&gt; d.y * bw}),\n    Plot.lineY(transpose(dens), {x:\"x\", y: d =&gt; d.y * bw, curve:\"linear\", stroke: \"red\"})\n    ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure below shows the relative amount of weight placed on different points as a function of their distance from the point of interest (0, marked by the vertical red line):\n\nkv = transpose(kernvals).filter(d =&gt; d.h == h && d.smoother == kern)\nPlot.plot({\ny: {grid: true, label: \"Relative weight of point as compared to origin\"},\nx: {\nlabel: \"Distance from point of interest (m) ↔ \"\n},\nmarks: [\n//Plot.lineY(kv, {filter: d =&gt; (d.smoother == kern), x:\"x\", y: d =&gt; d.y*1000}),\nPlot.lineY(kv, Plot.normalizeY({x:\"x\", y: \"y\", basis: \"extent\"})),\nPlot.ruleX([0], {stroke: \"red\"})\n]})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions\n\nWhich of the bandwidth options seems to do the best job in capturing the value of \\(f(x)\\)? Why?\nHow does the choice of kernel impact the smoothing?\nHow do the different kernel functions encode different assumptions about distance decay?\nWhat is the relationship between the histogram of the data and the smoother? What do you see as you change the histogram bin width relative to the smoothing bandwidth?"
  },
  {
    "objectID": "density.html#additional-resources",
    "href": "density.html#additional-resources",
    "title": "9  Spatial Density",
    "section": "Additional Resources",
    "text": "Additional Resources\nPlease see Matthew Conlen’s excellent interactive KDE tutorial"
  },
  {
    "objectID": "density.html#references",
    "href": "density.html#references",
    "title": "9  Spatial Density",
    "section": "References",
    "text": "References\n\n\n\n\n1. Levy MZ, Barbu CM, Castillo-Neyra R, et al. Urbanization, land tenure security and vector-borne Chagas disease. Proceedings of the Royal Society B: Biological Sciences [electronic article]. 2014;281(1789):20141003. (https://royalsocietypublishing.org/doi/full/10.1098/rspb.2014.1003). (Accessed January 23, 2023)"
  }
]