[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Field Guide to Relational and Systems Epidemiology",
    "section": "",
    "text": "Fixing a Hole\nThere is a hole in the epidemiological literature that limits our ability to come to grips with the relational aspects of health and illness. For the purposes of this book, relationships are sources of non-independence. These could be social influences, e.g. from our friends and family. But they could also be spatial in nature, e.g. a result of some physically proximate environmental exposure. Often, the relationships we care about are temporal in nature, as is evident in the lifecourse perspective in which early-life exposures are understood to impact later-life outcomes. This book is an attempt to bring these threads together into a more coherent whole that recognizes the important overlaps - and differences - in these approaches",
    "crumbs": [
      "Fixing a Hole"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "An invitation",
    "section": "",
    "text": "You are (almost certainly) a relational analyst already\nMy goal in this short essay is to chip away a bit at the idea of spatial/relational epidemiology as something separate and apart from mainstream epidemiology and public health. Instead, I argue that these are better understood as a loose wrapper around a core set of ideas and tools that are part of the working arsenal of most professionals, students, and researchers in public health.\nTo explain what I mean, and why I think it’s important, I’m going to subject you to a bit of autobiography about my own intellectual and professional trajectory.",
    "crumbs": [
      "An invitation"
    ]
  },
  {
    "objectID": "preface.html#influences",
    "href": "preface.html#influences",
    "title": "An invitation",
    "section": "Influences",
    "text": "Influences\nThere are any number of books and papers out there that have articulated important perspectives on the tools of quantitative analysis of relational and contextual data. The following have been particularly important for my own thinking and their influences will be felt throughout this work:\n\nStatistical Rethinking (mcelreath2020?)\nThe Ecological Detective (hilborn1997?)\nARM/Regression and Other Stories (gelman2020a?)\n\nWhat makes these works so useful, strong, and enduring is the way that they articulate a coherent, opinionated perspective on the meaning and use of a set of methodological tools. On top of that, they are engaging and fun to read - the sort of thing you return to over time not just to get specific methodological tools, but to be exposed to their perspective.",
    "crumbs": [
      "An invitation"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Why relational and contextual epidemiology?",
    "section": "",
    "text": "1.1 More than methods\nPart of my inspiration is a reaction to the technocratic impulse and imperative in modern epidemiology. In particular, I find myself a bit paralyzed by the worry about what happens when we operate under the assumption that escalating methodological complexity is an imperative and that the road out of socio-epidemiological problems is paved with technological solutions. This note of caution has gone up many times in public health and the social sciences, but it is important enough that it really can’t be over-emphasized. For example, in weighing in against atheoretical ‘garbage-can’ regression analysis in which all possible explanatory variables are plunked on the right hand side of the model equation, the political scientist Chris Achen argued against a mechanical approach to quantitative analysis and in favor of one that draws on classic skills like making exploratory figures, examining simple correlations and crosstabs, and “just plain looking at data.” (Achen 2005)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why relational and contextual epidemiology?</span>"
    ]
  },
  {
    "objectID": "introduction.html#more-than-methods",
    "href": "introduction.html#more-than-methods",
    "title": "1  Why relational and contextual epidemiology?",
    "section": "",
    "text": "Achen, Christopher H. 2005. “Let’s Put Garbage-Can Regressions and Garbage-Can Probits Where They Belong.” Conflict Management and Peace Science 22 (4): 327–39. https://doi.org/10.1080/07388940500339167.\n\nBut also, methods…\nOn the other side, simply put, I love the methodological tools of spatial epidemiology, Bayesian hierarchical analysis, and systems modeling. I have learned more than I ever could have hoped through learning, tinkering with, and applying these tools to problems in the real world and in my own head. But for me, the large majority of lessons learned have been from their conceptual isomorphisms (or conflicts) with the world as it appears to us through qualitative and quantitative data, rather than in the exact values of their parameter values and quantitative predictions.\nFor me, this book is about resolving this cognitive dissonance while providing useful ‘how-to’ pointers along the way. I hope to articulate the affirmative case for a systems-based, contextually-sensitive, justice-oriented, morally and ethically opinionated, and theoretically driven approach to epidemiology. Along the way, I hope to show why the tools of such an approach are necessarily heterogeneous in nature and require us to accept uncertaintities quantitative and epistemic.\n\n\nProblem-orientation is nonnegotiable\n\n“A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.” - Douglas Adams, Hitchhikers Guide to the Galaxy.\n\nSometimes, methods-y topics in epidemiology and public health are boiled down to a sequence of steps to be applied to each new problem. In the worst cases, they come to us as a series of copy-paste, plug-and-chug pieces of code to be reused each time the same type of problem is encountered. Beyond being a boring way to learn, this has the effect of putting the methods at the front of the train, with the question or problem implicitly assumed to be an opportunity or excuse to employ the model.\nThis is an approach that can get you some publications and maybe a little bit of clout within the musty world of academia, but it doesn’t do much to solve the types of problems working epidemiologists face. And in truth, it doesn’t even work so well on the academic side of the fence.\nInside the universe of this book, the problem is the first and most important thing. The question is the fixed point against which our analytic approaches are chosen. This means that there are no fixed methodological answers to applied questions: Our methodological approaches and tools must be as diverse and heterodox as the questions the world throws at us.\nMaking sense of the types of patterns we see in the real world requires us to identify:\n\nA question we want or need to answer.\nThe most important types of relationships impacting our outcome of interest (time, space, individual-to-individual).\nA methodological approach that will allow us to characterize the impact of those relationships on the outcome we care about.\n\n\nA methodological caboose\nThe choice of method comes last: A key motivation of this book is to sidestep the tendency to train ourselves into methodological hammers looking for data nails to whack away at.\nIn addition to this, there is no pre-supposition that the appropriate method is necessarily ‘fancy’ in the sense of being conceptually or mathematically complex, computationally intensive, or even heavilty or at all quantitative in nature. Whether this is the case is entirely a function of what happens at the intersection between question, data, and theory.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why relational and contextual epidemiology?</span>"
    ]
  },
  {
    "objectID": "introduction.html#component-approaches",
    "href": "introduction.html#component-approaches",
    "title": "1  Why relational and contextual epidemiology?",
    "section": "1.2 Component Approaches",
    "text": "1.2 Component Approaches\n\nSpatial Epidemiology\nI arrived at the idea of relational epidemiology as an umbrella category that covers, but goes beyond, spatial and hierarchical analysis. The stereotypical challenge in this setting is to adjust away the impact of context to get at some more generally meaningful parameter estimate, e.g. a treatment effect. But another way of thinking about this is that the hard work is in characterizing not only the ‘main’ or fixed effects, but in capturing the drivers of variability in outcomes across locations.\nBut not every contextual question is strictly spatial: If we care about how the structure of social networks impacts individual and collective risks, we are talking about context once again. In the network example, the context is one’s network ‘neighborhood’, the collection of individuals one interacts with. In reasonably homogeneous networks, groups of individuals may share very similar or almost identitical network contexts.\nUltimately, the key to understanding context is relatedness: Individuals sharing a context are likely to have similar relationships to their physical environment, the societies they are a part of, and potentially to each other, than those not sharing the same context. These relationships may be micro-level social relationships or macro-scale spatial ones, but they may also be temporal in nature. Temporal relationships may occur at a micro scale, e.g. the rise or fall in incidence of an infectious disease during a given week is likely to be a function of the prevalence of that same disease in the previous week. But these temporal relationships are also often more macro-scale and historical in nature: The long history of racial residential discrimination in the United States is undoubtedly a driver of many racial health dispartities we see today.\n\n\nSocial Epidemiology\n\n\nSystems Epidemiology\nIn the context of this book, a systems approach to epidemiology refers not only to systems methods but a systems orientation. But what does that mean?\nOften, in public health and epidemiology, our focus is on an outcome of interest such as the incidence of a particular disease, the rate of death following infection with a particular pathogen or a cancer diagnosis, or some other well-defined, clearly-specified endpoint. Obviously, we do this for a good reason: We care about preventing or treating these specific outcomes, and we couldn’t do a very good job of measuring risk and assessing intervention impact without thinking in these specific terms.\nSo, while we should care about enumerating and addressing these specific outcomes, the systems that generate them are not sui generis, i.e. they do not impact only one health or disease outcome at a time. Orienting towards systems means giving them the sort of in-depth attention we give to the outcomes. This approach pays enormous dividends: When we start from understanding a social or environmental system - for example, structural racism or anthropogenic climate change - that impacts multiple health outcomes, we are able to increase our overall health impact by intervening further up the causal chain.\nIn discussing approaches to analyzing infectious disease outbreaks, epidemics, and pandmeics, the medical historian Charles Rosenberg discussed the a distinction between configuration and contamination based modes of explanation of infection disease transmission. In Rosenberg’s words:\n\nThe configurational style of explanation is interactive, contextual, and often environmental; the emphasis on contamination reductionist and monocausal. Much of epidemiological thought between classical antiquity and the present can be usefully understood as a series of shifting rearrangements of these thematic building blocks. (Rosenberg (1992), emphasis added).\n\nRosenberg, Charles E., ed. 1992. “Explaining Epidemics.” In Explaining Epidemics, 293–304. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511666865.015.\n\nIn other words, the configuration perspective is a systems-oriented one. But it is more of a friendly competitor to the more proximal ‘contamination’ approach rather than its antagtonist. For a systems approach to succeed, we need to have a clear and accurate understanding of its component parts. The insights that come from the configurationist worlvdiew are ones that show us when we can and cannot intervene on individual components one at a time and what the consequentces of our attempts to act on a given health- or illness-generating system might be.\n\nSystems put interventions in context\nWhen we move away from outcome orientation, we are forced to reckon with relationships and feedback loops as first-class features of the world, on an equal footing with the outcomes that they influence: If we care about the behavior of the epidemiologic system, we care about how it changes over time, how it responds to different attempts to alter its behavior, and on and on. By contrast, an outcome is by definition an endpoint. When we take responsibility for characterizing and addressing the system, each outcome is a realization of a process of interest, and another piece of information that gives us a clue about how the system generating these outcomes behaves.\nWe have a tendency to conflate specific orientations with specific analytic methods: If we care about systems, then we must use dynamic models or network methods. While these tools may be more likely to show up in system-oriented analyses than in outcome-oriented ones, I would like to take this opportunity to reject this idea uncategorically. Methods are just tools of measurement and summarization that help us understand what is going on. They are important, but they are tools and they are often interchangeable. The important thing is, as it always has been, our very human ability to conceptualize and abstract away the key elements of the system of interest. The tools then let us express this understanding to others and to challenge and refine our assumptions about how it works.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why relational and contextual epidemiology?</span>"
    ]
  },
  {
    "objectID": "introduction.html#philosophical-guideposts",
    "href": "introduction.html#philosophical-guideposts",
    "title": "1  Why relational and contextual epidemiology?",
    "section": "1.3 Philosophical Guideposts",
    "text": "1.3 Philosophical Guideposts\nThis book is motivated by a several philosophical perspectives on epidemiology, social science, and medicine. Since everything else in here flows from this foundation, it’s important that we start with a little bit of it.\nPublic Health and epidemiology are sometimes accused of being atheoretical in nature. That is, reflecting a kind of “just the facts ma’am” approach that obscures the ideological roots and operating assumptions of the field. As many have pointed out, this ostensibly neutral or neutral-ish approach skirts the line of what is called “scientism”, i.e. a kind of fetishization of a supposedly objective ‘view from nowhere’. This has fed into a methodological orthodoxy, particularly around the fraught and messy notion of causal inference.\n\nWhy theory is everything\nWhen working on relational problems, there is sometimes a tendency to mistake the frame or perspective we are taking on a given problem for the real thing. This pitfall is exemplified by the phrase “the map is not the territory”: While some kind of abstraction or representation is always necessary, it is imperative to remember that it is always a proxy for the phenomenon under study.\nFor example, consider what we are doing when we say that an individual’s risk of infection declines with distance from the nearest case of that infection. Buried in the shape and intensity of the spatial autocorrelation function is the way interaction occurs between people over time. Those interactions may be purely a function of physical distance, i.e. I am less likely to bump into someone who lives far away from me than a neighbor, but may also reflect elements of ‘social space’. For example, certain forms of racial residential segregation may result in individuals of different racial identities living in very close proximity to each other but still having very limited or circumscribed contact with each other.\nIn the book Infectious Fear (Roberts 2009), the medical historian Samuel Kelton Roberts illustrates in detail how the residential segregation of near-South cities like Washington D.C. and Baltimore was manifest in a configuration in which White families lived in the row houses facing the street, while their Black servants lived in smaller, more-cramped quarters facing the alleys behind the homes of the more-privileged and well-off Whites. (For an example of this living configuration, see Figure 1.1.) In many cases these homes would share an adjoining wall, but the risks of acquiring and dying from infections like Tuberculosis associated with living in them were vastly different.\n\nRoberts, Samuel K. 2009. Infectious Fear: Politics, Disease, and the Health Effects of Segregation. University of North Carolina Press.\n\n\n\n\n\n\nFigure 1.1: Example of row-house/alley-house segregation in Washington D.C. from (logan2017a?)\n\n\n\nIn this case, a quantitative analysis that conflates spatial distance with social similarity and disease-transmitting contact leading to similar health outcomes would go badly astray. In the end, there are no universal meanings we can attribute to the geographic or temporal distances between events, and the formalisms we use to represent them are only as good as our ability to understand the social, environmental and historical context of the places we are looking at.\n\n\n“The Map is Not the Territory”\nThe phrase “the map is not the territory” is attributed to the Polish-American polymath scientist, author and philosopher Alfred Korzybski (korzybski2005?). The phrase is pretty self-explanatory, but essentially means that we should not mistake our representations of reality for reality itself. In the social sciences, this insight is closely tied to the critical realist perspective.\nSayer (Sayer 2010) explains that the “real” in realism refers to the idea that realists accept reality as a fixed, external thing. The critical spect of it comes from a belief that our perceptions and interpretations of this outside reality are always approximations which are inherently subjective in nature.\nDunn (dunn2012?) has outlined some of the ways a critical realist approach can be useful in epidemiology, particularly in the study of health inequities. Dunn states that the goal of realist science is to generate knowledge that is “practically adequate” but that is understood to be fallible and always subject to revision. This emphasis on fallibility also short circuits the quest for methodological perfection and obviously militates against any hard-and-fast rules for causal inference.\n\n. . . In that Empire, the Art of Cartography attained such Perfection that the map of a single Province occupied the entirety of a City, and the map of the Empire, the entirety of a Province. In time, those Unconscionable Maps no longer satisfied, and the Cartographers Guilds struck a Map of the Empire whose size was that of the Empire, and which coincided point for point with it. The following Generations, who were not so fond of the Study of Cartography as their Forebears had been, saw that that vast map was Useless, and not without some Pitilessness was it, that they delivered it up to the Inclemencies of Sun and Winters. In the Deserts of the West, still today, there are Tattered Ruins of that Map, inhabited by Animals and Beggars; in all the Land there is no other Relic of the Disciplines of Geography.\n\n\n—Suarez Miranda, Viajes de varones prudentes, Libro IV, Cap. XLV, Lerida, 1658\n\n\n\nOpen vs. Closed Systems\nBoth Dunn and Sayer emphasize the idea that it is more helpful to think in terms of “extensive” and “intensive” research rather than in terms of qualitative vs. quantitative research. In Dunn’s version of events, extensive research is about obtaining surface-level information on large numbers of people. The idea here is to find regularities across individuals with a similar health condition or to look for key features of a population. Sayer (Sayer 2010) refers to this as a search for “formal relations of similarity” between “taxonomic groups”, e.g. people of a certain race/ethnicity, age, gender, etc.\n\nSayer, Andrew. 2010. Method in Social Science: Revised 2nd Edition. 2nd ed. London: Routledge. https://doi.org/10.4324/9780203850374.\nBy contrast, intensive research is about understanding processes, often with a smaller number of observations than might be considered acceptable or useful for extensive research. But the nature of the data employed for intensive research will also be different and more granular, with more of what we now call “metadata” on both individuals and their relations to each other in physical space, social networks, and temporal context. it is focused on understanding the particular relationships between people and institutions as well as how these operate in the context of local and more macro-structural contingencies.\n\n\nA personal connection\nWhile the idea that “the map is not the territory” is intuitive from our lived experiences, it is just a key that unlocks the door to a host of important intellctual consequences. For me, the dominoes may have begun to fall particularly early on: The phrase and the various books it has been written in, as well as the coffee mugs it has been printed on, have played an unusually central role in my own life. My mother, Marjorie Zelner, was the sole full-time employee of the Institute for General Semantics, the organization founded by Alfred Korzybski, from 1989 until her death in 2000. For a time, the sword carried by Alfred Korzybski when he was a member of the Polish cavalry for some reason resided in our downstairs coat closet.\nThe (very small) field of General Semantics (GS), founded by Korzybski and Marjorie Kendig, is focused on what happens when verbal language is mistaken for the reality of the real-world phenomena it seeks to describe. While the focus of GS is on the relationship between language, reality, and human psychology, it must have seeped into my understanding of public health and epidemiology somewhere along the line.\nWhen I was in middle school and high school in the 1990s, I would earn spending money by helping my mom to box up and ship mail-order copies of Science and Sanity, the book where the phrase was first published, to the small band of people around the world who were interested in it. I would fold hundreds of copies of their monthly newsletter, stuff them into envelopes, and run them through the postage meter. Eventually, the institute relocated its headquarters - including its entire library - from a basement in Lakeville, CT to the second floor of the old carriage barn behind the house I grew up in in New Jersey. When I was bored, I would often find myself paging through the various books in the library, entertained and somewhat mystified by their complex concepts and knack for introducing new buzzwords and using mystifying but captivating diagrams (See Figure 1.2 for a particularly striking and important example).\nI can’t say that much of what I read made a lot of sense to me at the time, but something must have made its way through. More than anything, I absorbed a lot from the various intellectual rogues and vagabonds associated with the organization who would come to our house for the Institute’s annual meetings, often sleeping on our living room floor or pull-out couch.\n\n\n\n\n\n\nFigure 1.2: Korzybski’s structural differential. A representation of the process of abstraction from reality (the half-oval at the top of the image) through various levels of individual and collective consciousness. The direct connections between the levels represent the bits that make it through directly, while the connections between the tags represent progressively higher levels of abstraction away from reality.\n\n\n\nAll this is a long way of getting to the point that my own perspective on relational epidemiology is inextricable from the social and even physical context I grew up in, as well as the historical moment it happened in. It could have happened another way, but this was the series of contingent events in the still very much open system of my particular existence that got me here.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why relational and contextual epidemiology?</span>"
    ]
  },
  {
    "objectID": "relationships.html",
    "href": "relationships.html",
    "title": "2  Autocorrelation Everywhere",
    "section": "",
    "text": "2.1 Tobler’s First Law\nIn 1970, the geographer Waldo Tobler coined what is often referred to as Tobler’s first law (TFL), which states that “[e]verything is related to everything else, but near things are more related than distant things” (Tobler 1970). What Tobler is talking about here is the idea that spatial autocorrelation is ubiquitous in geography. While TFL may sound obvious, Goodchild shows its importance and far-reaching implications through a simple thought ecperiment:\nThe notion of the role of ‘nearness’ and ‘farness’ in shaping the similarity of outcomes is really the heart of a contextual approach to epidemiology. Tobler was speaking specifically about geographic distance when he made this statement. Specifically, he was referring to the way human populations were distributed in space, and observing that their composition and density were spatially related, with nearby neighborhoods more similar to each other than distant ones. Since Tobler wrote, others have generalized TFL to a more generic notion of nearness or farness across different types of space.\nApplying this idea to a broad array relationships is what it means, at least to me, to do relational or contextual epidemiology. Simply put, this is just the idea that events separated by less time are more likely to be similar - on average - than those separated by longer periods of time.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocorrelation Everywhere</span>"
    ]
  },
  {
    "objectID": "relationships.html#toblers-first-law",
    "href": "relationships.html#toblers-first-law",
    "title": "2  Autocorrelation Everywhere",
    "section": "",
    "text": "Tobler, W. R. 1970. “A Computer Movie Simulating Urban Growth in the Detroit Region.” Economic Geography 46: 234–40. https://doi.org/10.2307/143141.\n\n“Perhaps the easiest way to see the validity and value of TFL is through a thought experiment, by trying to imagine a world in which it is not true. In such a world, the full range of conditions could be encountered in every minute portion of the world. Every room, for example, might contain the full observed range of the Earth’s topographic variation, from the bottom of the Marianas Trench to the summit of Mount Everest; and the full range of climatic conditions as well. This is the world of white noise, a world that would be impossible to describe, live in, sense, navigate through, or farm.” (Goodchild 2004)\n\n\n\n\nWhat does closeness do?\nMiller (Miller 2004) specifically draws on systems theory to argue that the very simple concept of nearness is enough to explain complex geographic phenomena including natural ecosystems and economies, because it is not the nearness that does the ‘doing’ but that interaction between dynamic components is more likely over short spatial, temporal, or social distances than over longer ones.\n\nMiller, Harvey J. 2004. “Tobler’s First Law and Spatial Analysis.” Annals of the Association of American Geographers 94 (2): 284–89. https://doi.org/10.1111/j.1467-8306.2004.09402005.x.\n\n\nSimilarity is meaningless without variation\n\nNo similarity without variability (i.e. the real first law according to Goodchild (Goodchild 2004))\n\n\nGoodchild, Michael F. 2004. “The Validity and Usefulness of Laws in Geographic Information Science and Geography.” Annals of the Association of American Geographers 94 (2): 300–303. https://doi.org/10.1111/j.1467-8306.2004.09402008.x.\n\n\nWhy negative autocorrelation matters",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocorrelation Everywhere</span>"
    ]
  },
  {
    "objectID": "relationships.html#how-does-autocorrelation-manifest-in-the-real-world",
    "href": "relationships.html#how-does-autocorrelation-manifest-in-the-real-world",
    "title": "2  Autocorrelation Everywhere",
    "section": "2.2 How does autocorrelation manifest in the real world?",
    "text": "2.2 How does autocorrelation manifest in the real world?\n\nSpatial\n\nGradients: relatively smooth variability over space, e.g. as a function of elevation in ecological systems (Legendre 1993). According to Legendre, in a true linear gradient the errors at each location are not autocorrelated with each other.\nPatchiness: Discreate areas separated by ecological discontinuities or interfaces (Legendre 1993).\n\n\nLegendre, Pierre. 1993. “Spatial Autocorrelation: Trouble or New Paradigm?” Ecology 74 (6): 1659–73. https://doi.org/10.2307/1939924.\n\nGetis (Getis and Ord 2010) discusses how it can be difficult to disentangle local spatial autocorrelation from global, i.e. when there is global autocorrelation we are likely to also find local, so have to be careful about interpreting.\nGriffith (Griffith 2023) makes the point that positive and negative spatial autocorrelation commonly co-occur, so that it is important to not over-focus on one vs. the other.\nAlso points out that the nature of autocorrelation will be a function of scale. Provides the Snow Cholera data as an example: At small scales, there will be a combination of positive (PSA) and negative spatial autocorrelation (NSA), while at higher levels (e.g. all of london) the prevailing pattern will be PSA due to the relatively small size of the area of the outbreak.\nTemporal\n\n\nGetis, Arthur, and J. K. Ord. 2010. “The Analysis of Spatial Association by Use of Distance Statistics.” Geographical Analysis 24 (3): 189–206. https://doi.org/10.1111/j.1538-4632.1992.tb00261.x.\n\nGriffith, Daniel A. 2023. “Spatial Autocorrelation Mixtures in Geospatial Disease Data: An Important Global Epidemiologic/Public Health Assessment Ingredient?” Transactions in GIS 27 (3): 730–51. https://doi.org/10.1111/tgis.13042.\n\n\nSocial\n\nStratification & Inequality\n\n\nNetworks",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocorrelation Everywhere</span>"
    ]
  },
  {
    "objectID": "relationships.html#nearness-farness-and-the-problem-of-scale",
    "href": "relationships.html#nearness-farness-and-the-problem-of-scale",
    "title": "2  Autocorrelation Everywhere",
    "section": "2.3 Nearness, farness, and the problem of scale",
    "text": "2.3 Nearness, farness, and the problem of scale",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocorrelation Everywhere</span>"
    ]
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "3  Why model?",
    "section": "",
    "text": "Quantitative models are often taken to be self-justifying in epidemiology and public health.\nBut there is no a priori reason to assume that these approaches are worth the effort and energy put into them.\nCOVID-19 and other emergencies have demonstrated the potential for failure in these approaches.\nWhat are the philosophical and practical justifivations for “interrogating models with data” as Hilborn and Mangels put it?\nHow can we know we are doing the right thing?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Why model?</span>"
    ]
  },
  {
    "objectID": "lags.html",
    "href": "lags.html",
    "title": "4  Matrices",
    "section": "",
    "text": "4.1 Matrix Notation",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "lags.html#matrix-notation",
    "href": "lags.html#matrix-notation",
    "title": "4  Matrices",
    "section": "",
    "text": "When we talk about a matrix, we label it using a bold, capital letter, e.g. \\(\\mathbf{A}\\) is an adjacency matrix.\nWhen we talk about a vector, we will utilize the convention of referring to it using a bold lowercase letter, i.e. \\(\\mathbf(a)\\) could be a vector representing one of the columns of \\(bold(A).\\)\nFinally, when we refer to the elements of a matrix or vector, we will use a lowercase letter with the appropriate index in subscript, i.e. \\(a_i\\) is the i-th element of \\(\\mathbf{A}\\)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "lags.html#indices",
    "href": "lags.html#indices",
    "title": "4  Matrices",
    "section": "4.2 Indices",
    "text": "4.2 Indices\nWhen indexing locations or individuals, we will start with \\(i\\). The simple grid in #ref() shows a scenario in which locations are indexed by \\(i\\). The set of locations in can be represented by \\(\\mathbf{Z}\\), with each location \\(i \\in \\mathbf{Z}\\).\nWhen looking at relationships between locations, we may be interested in the (x,y) coordinates of a location. For this, we will adopt the ‘row-major’ order common in linear algebra, in which rows are indexed first by \\(i\\) and columns by \\(j\\).\n\nSometimes, we will think about nested data, i.e. individuals within locations. In this case, our indices will work upwards from the left, i.e. individual \\(i\\) residing within location \\(j\\).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "lags.html#adjacency-matrices",
    "href": "lags.html#adjacency-matrices",
    "title": "4  Matrices",
    "section": "4.3 Adjacency Matrices",
    "text": "4.3 Adjacency Matrices\nOften, we want to know who is next to whom, by whatever definition. The matrix in #ref() is an example of a binary adjacency matrix#note([In a binary adjacency matrix, we have only 1 and 0 entries, i.e., you’re a neighbor or you aren’t.]) representing a Rooks’ contiguity approach to neighborhoods in the matrix in #ref().\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0\n\\end{bmatrix}\n\\]\nIn this setup, each row of \\(\\mathbf{A}\\) represents a location \\(i\\) and the columns indicate whether \\(i\\) and \\(j\\) are neighbors, i.e. if \\(a_{i,j} = 1\\). In this example, we assume that the matrix is symmetric, i.e. if \\(i\\) is next to \\(j\\), \\(j\\) is next to \\(i\\). But this may not always be the case.#note([We will sometimes refer to this as a weights matrix, \\(\\mathbf{W}\\) with elements \\(w_{ij}\\)]).\n== Spatial Lags\nOne very useful property of adjacency matrices is that they can be used to calculate spatial lags, which are importantant for measuring spatial autocorrelation.\nLet’s take a step back and think about the notion of a lag term in a series of observations ordered in time, which we will denote by the vector \\(bold(y)\\). If the elements of \\(bold(x)\\) are ordered in time, we can define the value of \\(x_(t-1)\\) as the lagged value of \\(x_t\\). We could then use this set of relationships to estimate the first-order autocorrelation function, which is just the strength of relationships between observations at a given time and the ones immediately before them.\nBack in our two-dimensional spatial setting, we may want to compare the strength of the relationship between the value of an outcome at location \\(i\\), denoted \\(x_i\\), and the values of its neighbors as define dby some adjacency matrix, \\(bold(A)\\).\nThis is where the matrix-y-ness of the adjacency matrix comes in handy. We can multiply \\(bold(z) = bold(A dot x)\\) to obtain the total value of \\(x\\) for each location’s neighbors.#note([So, \\(z_i\\) would be the sum of the values in \\(bold(x)\\) for the neighbors of \\(i\\). ]) Just like in the temporal example, we can then use the output of this calculation to inform the strength of the relationship between the observed value \\(x_i\\) at a particular location and its neighbors. #note([If this is confusing, it will begin to make more sense when we dive into some of the more commonly-used spatial statistics like Geti-Ord Gi* and Moran’s I in the coming weeks. ])",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "lags.html#adjacency-lists",
    "href": "lags.html#adjacency-lists",
    "title": "4  Matrices",
    "section": "4.4 Adjacency Lists",
    "text": "4.4 Adjacency Lists\nThe size of the adjacency matrix scales with the square of the number of locations. So in our 3x3 example, we have 9 total locations and 81 potential pairs of locations. In a 10x10 example, we have 100 locations, and 10,000 adjacency matrix entries! Most of the time, we have relatively sparse adjancency matrices in which most people have a few relationships and the large majority of entries are 0.\nAn alternative approach is to use what is known as an adjacency list representation, in which the indices of locations (\\(j\\)) that are neighbors of location \\(i\\) are represented like we see in\n#figure( $ i = 1:& [2, 4]\n2:& [1, 3, 5]\ndots.v\n5:& [2,4,6,8]\ndots.v\n9:& [6,8] $, caption: [Adjacency list representation of Rook’s contiguity relationships in #ref().], ) \nThe adjacency list representation has the advantage of being compact and easier to inspect but without the ability to do straightforward matrix operations like we can with an adjacency matrix.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "neighbors.html",
    "href": "neighbors.html",
    "title": "5  Finding Your Neighbors",
    "section": "",
    "text": "5.1 Loading Packages\nlibrary(tidyverse)\nlibrary(spdep)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finding Your Neighbors</span>"
    ]
  },
  {
    "objectID": "neighbors.html#setting-up-a-simulated-city-with-spatially-varying-risk",
    "href": "neighbors.html#setting-up-a-simulated-city-with-spatially-varying-risk",
    "title": "5  Finding Your Neighbors",
    "section": "5.2 Setting up a simulated city with spatially varying risk",
    "text": "5.2 Setting up a simulated city with spatially varying risk",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finding Your Neighbors</span>"
    ]
  },
  {
    "objectID": "neighbors.html#constructing-a-grid",
    "href": "neighbors.html#constructing-a-grid",
    "title": "5  Finding Your Neighbors",
    "section": "5.3 Constructing a grid",
    "text": "5.3 Constructing a grid\n\nmakeSquareGrid &lt;- function(D) {\n  \n\n  x_coords &lt;- 1:D\n  y_coords &lt;- 1:D\n\n## The expand.grid function creates a data frame\n## with all possible combinations of the \n## input elements\n  grid_df &lt;- expand.grid(x_coords, y_coords)\n\n## Set the variable names\ncolnames(grid_df) &lt;- c(\"X\",\"Y\")\n\n## Each cell also gets a unique ID from 1:D^2\ngrid_df$cellID &lt;- 1:nrow(grid_df)\n\n## Each cell has the same population\ngrid_df$N &lt;- 100\n\nreturn(grid_df)\n}\n\nD &lt;- 10\ngrid_df &lt;- makeSquareGrid(D)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finding Your Neighbors</span>"
    ]
  },
  {
    "objectID": "neighbors.html#simulating-risks-by-neighborhood-zones",
    "href": "neighbors.html#simulating-risks-by-neighborhood-zones",
    "title": "5  Finding Your Neighbors",
    "section": "5.4 Simulating risks by neighborhood zones",
    "text": "5.4 Simulating risks by neighborhood zones\n\n## This is the background, per-capita rate in each \n## cell of our city\nbaseline_per_100 &lt;- 1\n\n## This is the relative risk of the outcome for individuals\n## in the top-left qudrant\ntop_left_rr &lt;- 5\n\n\n## This is the relative risk of the outcome for \n## individuals living towards the center of the\n## simulated city\ncenter_rr &lt;- 0.25\n\n## Here, we initialize a variable for the per-capita rate\n## within each cell\ngrid_df$rate &lt;- 0\n\n## This is just filling in the per-capita rates for each cell\n## as a function of where they are on the grid (assuming a 10 x 10 grid)\ngrid_df &lt;- grid_df %&gt;%\n    mutate(rate = case_when( (X &lt;= 5) & (Y &gt;= 5) ~ baseline_per_100*top_left_rr,\n    .default = baseline_per_100)) %&gt;%\n        mutate(rate = case_when(between(X,3,7)&between(Y,3,7) ~ rate*center_rr,\n        .default = rate))\n\n## Now we can also draw the number of cases observed in \n## each grid cell\ngrid_df &lt;- grid_df %&gt;%\n  mutate(p_disease = 1-exp(-rate/N))\n\n## Simulate the observed number of cases in each cell\n## as a function of the population size (N) and probability of diseaw\ngrid_df$numCases &lt;- rbinom(nrow(grid_df), grid_df$N, grid_df$p_disease)\n\n\ng &lt;- ggplot(grid_df) + \n    geom_tile(aes(x = X, y = Y, fill = numCases)) + \n  coord_equal()\nplot(g)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finding Your Neighbors</span>"
    ]
  },
  {
    "objectID": "neighbors.html#exercises",
    "href": "neighbors.html#exercises",
    "title": "5  Finding Your Neighbors",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\nPlot the sampled number of cases in addition to the per-capita rate in each cell.\nChange the baseline and spatial relative risks to higher or lower values. What happens to the agreement between the spatial patterning of the cases vs. the rates at very high rates vs. very low ones?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finding Your Neighbors</span>"
    ]
  },
  {
    "objectID": "neighbors.html#exercises-1",
    "href": "neighbors.html#exercises-1",
    "title": "5  Finding Your Neighbors",
    "section": "6.1 Exercises",
    "text": "6.1 Exercises\n\nTry changing the neighbor type to queen from rook. What happens to the number of neighbors for each cell?\nLook at the help file for the nb2mat function. Try one of the adjacency matrix styles other than B (binary).\nWhat happens if you change the option torus = TRUE? Why?\n\n\nxy$totalRate &lt;- gridNeighborMatrix %*% grid_df$rate / xy$numNeighbors\n\n\ng &lt;- ggplot(xy) + \n    geom_tile(aes(x = x, y = y, fill = totalRate)) + \n  coord_equal()\nplot(g)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finding Your Neighbors</span>"
    ]
  },
  {
    "objectID": "neighbors.html#k-nearest-neighbors",
    "href": "neighbors.html#k-nearest-neighbors",
    "title": "5  Finding Your Neighbors",
    "section": "6.2 K Nearest Neighbors",
    "text": "6.2 K Nearest Neighbors\n\nkNearestNeighborsList &lt;- function(df, k) {\n  xy &lt;- df %&gt;% \n  select(X,Y)\n\nkNeighbors &lt;- knearneigh(xy, k = k)$nn %&gt;%\n  data.frame()\n\nkNeighbors$cellID &lt;- 1:nrow(kNeighbors)\n\nneighborsList &lt;- kNeighbors %&gt;% \n  pivot_longer(cols = !matches(\"cellID\"), names_to = NULL, values_to = \"neighborID\") %&gt;%\n  arrange(cellID, neighborID)\n\nreturn(neighborsList)\n}\n\nk &lt;- 10\nneighborsList &lt;- kNearestNeighborsList(grid_df, k)\n\nprint(neighborsList)\n\n# A tibble: 1,000 × 2\n   cellID neighborID\n    &lt;int&gt;      &lt;int&gt;\n 1      1          2\n 2      1          3\n 3      1          4\n 4      1         11\n 5      1         12\n 6      1         13\n 7      1         21\n 8      1         22\n 9      1         23\n10      1         31\n# ℹ 990 more rows\n\n\n\n## This takes a data frame with pairs of cells and neighbors and brings in \n## contextual information (number of cases, the per-capita rate) by cell.\n## It then \nkNeighborsRates &lt;- inner_join(neighborsList, grid_df,\n                              by = join_by(neighborID == cellID)\n                              ) %&gt;% \n  select(cellID, numCases, rate) %&gt;%\n  group_by(cellID) %&gt;%\n  summarize(avgRate = sum(numCases)/k) %&gt;%\n  inner_join(select(grid_df, cellID, X, Y))\n\nprint(kNeighborsRates)\n\n# A tibble: 100 × 4\n   cellID avgRate     X     Y\n    &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1      1     0.9     1     1\n 2      2     1       2     1\n 3      3     1.2     3     1\n 4      4     1.4     4     1\n 5      5     0.8     5     1\n 6      6     0.9     6     1\n 7      7     0.9     7     1\n 8      8     0.7     8     1\n 9      9     0.8     9     1\n10     10     1      10     1\n# ℹ 90 more rows\n\n\n\ng &lt;- ggplot(kNeighborsRates) + \n    geom_tile(aes(x = X, y = Y, fill = avgRate)) + \n  coord_equal()\nplot(g)\n\n\n\n\n\n\n\n\n\nAdditional Exercises\n\nTry different values of k and observe the impact that smaller or larger numbers of neighbors involved in the smoothing have on the image.\nTry this with the number of cases rather than the rates. What changes when the number of cases is relatively low due to sampling error?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finding Your Neighbors</span>"
    ]
  },
  {
    "objectID": "getis-ord.html",
    "href": "getis-ord.html",
    "title": "6  Getis-Ord \\(G_i^*\\)",
    "section": "",
    "text": "6.1 Loading Packages\nlibrary(tidyverse)\nlibrary(spdep)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Getis-Ord $G_i^*$</span>"
    ]
  },
  {
    "objectID": "getis-ord.html#some-helpful-functions",
    "href": "getis-ord.html#some-helpful-functions",
    "title": "6  Getis-Ord \\(G_i^*\\)",
    "section": "6.2 Some helpful functions",
    "text": "6.2 Some helpful functions\n\nmakeSquareGrid &lt;- function(D) {\n  x_coords &lt;- 1:D\n  y_coords &lt;- 1:D\n\n  ## The expand.grid function creates a data frame\n  ## with all possible combinations of the\n  ## input elements\n  grid_df &lt;- expand.grid(x_coords, y_coords)\n\n  ## Set the variable names\n  colnames(grid_df) &lt;- c(\"X\", \"Y\")\n\n  ## Each cell also gets a unique ID from 1:D^2\n  grid_df$cellID &lt;- 1:nrow(grid_df)\n\n  return(grid_df)\n}\n\n\noverlappingRiskGrid &lt;- function(D, N, alpha = 1, tl = 2.0, center = 2.0) {\n  grid_df &lt;- makeSquareGrid(D)\n\n  grid_df$N &lt;- N\n\n  ## This is the background, per-capita rate in each\n  ## cell of our city\n  baseline_per_capita &lt;- alpha\n\n  ## This is the relative risk of the outcome for individuals\n  ## in the top-left qudrant\n  top_left_rr &lt;- tl\n\n  ## This is the relative risk of the outcome for\n  ## individuals living towards the center of the\n  ## simulated city\n  center_rr &lt;- center\n\n  ## Here, we initialize a variable for the per-capita rate\n  ## within each cell\n  grid_df$rate &lt;- 0\n\n  ## Make the middle risk area 4 x 4 and the top-left area also 4x4\n\n  center_low_xy &lt;- round(D / 2) - 2\n  center_high_xy &lt;- round(D / 2) + 2\n\n  ## This is just filling in the per-capita rates for each cell\n  ## as a function of where they are on the grid (assuming a 10 x 10 grid)\n  grid_df &lt;- grid_df %&gt;%\n    mutate(rate = case_when(between(X, 1, 4) & between(Y, 1, 4) ~ baseline_per_capita * top_left_rr,\n      .default = baseline_per_capita\n    )) %&gt;%\n    mutate(rate = case_when(between(X, center_low_xy, center_high_xy) & between(Y, center_low_xy, center_high_xy) ~ rate * center_rr,\n      .default = rate\n    ))\n\n  ## Now we can also draw the number of cases observed in\n  ## each grid cell\n  grid_df &lt;- grid_df %&gt;%\n    mutate(p_disease = 1 - exp(-rate / N))\n\n  ## Simulate the observed number of cases in each cell\n  ## as a function of the population size (N) and probability of diseaw\n  grid_df$numCases &lt;- rbinom(nrow(grid_df), grid_df$N, grid_df$p_disease)\n\n  return(grid_df)\n}",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Getis-Ord $G_i^*$</span>"
    ]
  },
  {
    "objectID": "getis-ord.html#simulate-input-data",
    "href": "getis-ord.html#simulate-input-data",
    "title": "6  Getis-Ord \\(G_i^*\\)",
    "section": "6.3 Simulate input data",
    "text": "6.3 Simulate input data\n\n1D &lt;- 10\n2cityData &lt;- overlappingRiskGrid(D, 100, 5, 10, 0.5)\n3cityDataLong &lt;- cityData %&gt;%\n  pivot_longer(c(rate, numCases))\n\n4g &lt;- ggplot(cityDataLong) +\n  geom_tile(aes(x = X, y = Y, fill = value), colour = \"black\") +\n  coord_equal() +\n  facet_wrap(~name) +\n  scale_fill_viridis_c()\n\nplot(g)\n\n\n1\n\nSet grid dimension.\n\n2\n\nSimulate outcomes based on input values.\n\n3\n\nConvert to long format to enable plotting with facet_wrap.\n\n4\n\nPlot rates and simulated cases in a 2-panel figure.\n\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\nTry expanding the dimension of the grid to get a sense of the overall patterns.\nVary the size of the population in each cell to see how that impacts the difference between the simulated counts and underlying rates.\nVisualize the counts and rates as a histogram. Does this provide any insight you don’t get from the spatial representation?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Getis-Ord $G_i^*$</span>"
    ]
  },
  {
    "objectID": "getis-ord.html#getis-ord-g-and-gi",
    "href": "getis-ord.html#getis-ord-g-and-gi",
    "title": "6  Getis-Ord \\(G_i^*\\)",
    "section": "6.4 Getis-Ord G* and Gi*",
    "text": "6.4 Getis-Ord G* and Gi*\nWe can calculate the local Gi* statistic using the equation below:\n\\[\nG_i^* = \\frac{\\sum_j w_{ij} x_j}{\\sum_j x_j}\n\\tag{6.1}\\]\nPractically speaking, the numerator in Equation 6.1 contains the weighed sum of the outcome of interest across all of the neighbors of location \\(i\\) as well as the value of \\(x_i\\). The denominator contains the sum across all locations in the dataset. So, the value of \\(G_i^*\\) at a given location is the ratio of the weighted sum of neigboring values and the total.\nThe code below shows how to calculate values of the local Getis-Ord Gi* statistic using a neighbor list constructed using spdep functions:\n\n1gridNeighbors &lt;- include.self(\n  cell2nb(D, D, type = \"queen\", torus = FALSE)\n);\n\n2nbw &lt;- nb2listw(gridNeighbors, style = \"B\")\n\n3num_loc &lt;- length(nbw$neighbours)\ngi_vals &lt;- rep(0, num_loc)\n\n4totalCases &lt;- sum(cityData$numCases)\n\n5for (i in 1:num_loc) {\n6  neighborIndices &lt;- nbw$neighbours[[i]]\n7  weights &lt;- nbw$weights[[i]]\n  gi_vals[[i]] &lt;- sum(neighborIndices * cityData$numCases[neighborIndices] *\n8    weights) / totalCases\n}\n\n\n1\n\nThis function adds the self-neighbor relation required by \\(G_i^*\\) to the neighbors object created by spdep.\n\n2\n\nTurn the neighbors object into a neighbor list.\n\n3\n\nCreate an empty vector to store \\(G_i^*\\) values.\n\n4\n\nCalculate the total number of cases to be used when calculating the denominator from Equation 6.1.\n\n5\n\nLoop over all locations to calculate each \\(G_i^*\"\\) value.\n\n6\n\nPull out the indices of the neighbors of location \\(i\\).\n\n7\n\nExtract the weights calculated for each neighborhood.\n\n8\n\nBring it all together to calculate the value of \\(G_i^*\\) at location \\(i\\).\n\n\n\n\n\ncityData$giStar &lt;- gi_vals\ng &lt;- ggplot(cityData) +\n  geom_tile(aes(x = X, y = Y, fill = giStar), colour = \"black\") +\n  coord_equal() +\n  scale_fill_viridis_c()\n\nplot(g)\n\n\n\n\n\n\n\n\n\nExercises\n\nChange the underlying rates in the model and relative risks. How does this appear to change the values of the Gi* stat?\nWhich values of Gi* in the maps you generate reflect a ‘real’ cluster (based on the input parameters) and which might be artifacts of random sampling?\nChange the way neighbors are calculated from queen to rook’s adjacency. What happens? Does this effect seem more pronounced under different input datasets? Why?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Getis-Ord $G_i^*$</span>"
    ]
  },
  {
    "objectID": "getis-ord.html#calculate-getis-ord-gi-for-simulated-data-using-spdep",
    "href": "getis-ord.html#calculate-getis-ord-gi-for-simulated-data-using-spdep",
    "title": "6  Getis-Ord \\(G_i^*\\)",
    "section": "6.5 Calculate Getis-Ord Gi* for simulated data using spdep",
    "text": "6.5 Calculate Getis-Ord Gi* for simulated data using spdep\n\ngridNeighbors &lt;- include.self(cell2nb(D, D, type = \"queen\", torus = FALSE))\nxyc &lt;- attr(gridNeighbors, \"region.id\")\n\n## Extracts the x,y coords of each cell\nxy &lt;- matrix(as.integer(unlist(strsplit(xyc, \":\"))), ncol = 2, byrow = TRUE)\n\nxy &lt;- data.frame(x = xy[, 1], y = xy[, 2])\n\ngridNeighborList &lt;- nb2listw(gridNeighbors, zero.policy = TRUE, style = \"B\")\n\nlocalGEst &lt;- localG(cityData$numCases, gridNeighborList)\n\ncityData$cluster &lt;- attr(localGEst, \"cluster\")\ncityData$gScore &lt;- as.vector(localGEst)\ncityData$sig &lt;- as.numeric(abs(as.vector(localGEst)) &gt; 3.2)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Getis-Ord $G_i^*$</span>"
    ]
  },
  {
    "objectID": "getis-ord.html#plot-labeled-outcomes",
    "href": "getis-ord.html#plot-labeled-outcomes",
    "title": "6  Getis-Ord \\(G_i^*\\)",
    "section": "6.6 Plot labeled outcomes",
    "text": "6.6 Plot labeled outcomes\n\ng &lt;- ggplot(cityData, aes(x = X, y = Y)) +\n  geom_tile(aes(fill = gScore)) +\n  geom_text(aes(label = sig), color = \"white\") +\n  coord_equal() +\n  scale_fill_viridis_c()\n\nplot(g)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Getis-Ord $G_i^*$</span>"
    ]
  },
  {
    "objectID": "segregation/index.html",
    "href": "segregation/index.html",
    "title": "7  Residential Segregation",
    "section": "",
    "text": "7.1 Dimensions of Segregation\nResidential segregaton generally refers to the physical separation of different groups as a function of their race, ethnicity, religion, income, or some combination of these factors. In addition to being a persistent and ever-present feature of unequal societies, it touches on every component of the relational toolbox.",
    "crumbs": [
      "Relational Problems",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Residential Segregation</span>"
    ]
  },
  {
    "objectID": "segregation/index.html#dimensions-of-segregation",
    "href": "segregation/index.html#dimensions-of-segregation",
    "title": "7  Residential Segregation",
    "section": "",
    "text": "Space\nSpace is the most common lens through which we tend to approach the concept of segregation: Classic notions of segregation focus on the intensity and scale of separation between groups. In the United States, these conversations often focus on the physical separation of Blacks from Whites in cities such as Chicago and New York. But this pattern has been documented around the world and often takes the form of separation by ethnicity or religion as well (Susewind 2017).\n\nSusewind, Raphael. 2017. “Muslims in Indian Cities: Degrees of Segregation and the Elusive Ghetto.” Environment and Planning A: Economy and Space 49 (6): 1286–1307. https://doi.org/10.1177/0308518X17696071.\nThe spatial aspects of segregation may impact health outcomes through their effects on accessibility to healthcare, the quality of the neighborhood environment (e.g. access to parks and other forms of green space), the presence of crime and other factors that lead to a diminished sense of safety, and on and on. These effects of segregation are perhaps the most palpable and tangible, but as we will see, to truly understand and address residential segregation we need to grasp its other dimensions.\n\n\nTime\nThe impacts of segregation can be viewed through multiple temporal lenses. For example, much historical work has documented the ways in which patterns of redlining, which denied mortgages and home insurance to Blacks and other non-White home buyers in the U.S., entrenched the patterns of racial residential segregation we still see in many cities in the present day.\nOther work has looked specifically at segregation as a life-course exposure, with effects that may manifest in later life among people exposed even as children (Kershaw et al. 2017).\n\nKershaw, Kiarri N., Whitney R. Robinson, Penny Gordon-Larsen, Margaret T. Hicken, David C. Goff, Mercedes R. Carnethon, Catarina I. Kiefe, Stephen Sidney, and Ana V. Diez Roux. 2017. “Association of Changes in Neighborhood-Level Racial Residential Segregation With Changes in Blood Pressure Among Black Adults: The CARDIA Study.” JAMA Internal Medicine 177 (7): 996–1002. https://doi.org/10.1001/jamainternmed.2017.1226.\n\n\nSocial\nWhile it is most common to conceptualize segregation as a social process with an overt spatial manifestation, much research has complicated this picture. While a spaital perspective may reveal a lot about the underlying processes, a perspective focused at the level of individuals and groups reveals how segregation and its impacts play out through the sorting of individuals into occupations and social groups which then interact with each other in ways that reinforce hierarchies that drive deleterious health outcomes.\nThe social impacts of segregation may also manifest in what the sociologist Loïc Wacquant has referred to as “territorial stigmatization” (Wacquant 2007) in which a place’s reputation is used to stigmatize its residence. In the context of the COVID-19 pandemic, Chowkwanyun and Reed (Chowkwanyun and Reed 2020) invoked this idea to sound a note of caution on spatial analyses of disease outcomes, suggesting that analyses that fuse place and disease together without appropriate attention to the structural drivers of these relationships may actually reinforce the social inequities they are intended to highlight and ameliorate.\n\nWacquant, Loïc. 2007. “Territorial Stigmatization in the Age of Advanced Marginality.” Thesis Eleven 91 (1): 66–77. https://doi.org/10.1177/0725513607082003.\n\nChowkwanyun, Merlin, and Adolph L. Reed. 2020. “Racial Health Disparities and Covid-19 — Caution and Context.” New England Journal of Medicine 383 (3): 201–3. https://doi.org/10.1056/NEJMp2012910.",
    "crumbs": [
      "Relational Problems",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Residential Segregation</span>"
    ]
  },
  {
    "objectID": "segregation/index.html#references",
    "href": "segregation/index.html#references",
    "title": "7  Residential Segregation",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Relational Problems",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Residential Segregation</span>"
    ]
  },
  {
    "objectID": "smoothing.html",
    "href": "smoothing.html",
    "title": "8  Smoothing",
    "section": "",
    "text": "8.1 Codifying Tobler’s First Law using Locally Weighted Regression\nIn this brief tutorial, we will review some basic ideas about smoothing and start thinking through how we can express these ideas mathematically and in R.\nTobler’s profound - but deceptively simple - first law states that:\nHe applied this idea to his development of a dynamic model of urban growth in the Detroit region which assumed that rates of population growth were spatially similar:\nIn this post, we’re going to start with a simpler problem - change in the value of a function in one dimension - to see how we can translate the concept of distance decay implied by Tobler’s first law (TFL) into a useful model. To begin, we’re going to keep it simple with no noise or observation error and just interpolate some values.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "smoothing.html#codifying-toblers-first-law-using-locally-weighted-regression",
    "href": "smoothing.html#codifying-toblers-first-law-using-locally-weighted-regression",
    "title": "8  Smoothing",
    "section": "",
    "text": "“Everything is related to everything else. But near things are more related than distant things.” (tobler1970a?)",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "smoothing.html#notation-and-terminology",
    "href": "smoothing.html#notation-and-terminology",
    "title": "8  Smoothing",
    "section": "8.2 Notation and Terminology",
    "text": "8.2 Notation and Terminology\nIn this example, we are interested in visualizing and predicting the values of a function \\(f(x_i)\\) which outputs values \\(y_i\\), the expected value of the output function.\n\\[\ny_i = f(x_i)\n\\]\nLets start by getting the values of \\(f(x)\\) for every input value \\(x\\). For simplicity, we will assume that \\(f(x)\\) is a sine function and that the values of \\(x\\) go from -1 to +5, allowing us to observe one half cycle of the sine function:\n\nx &lt;- seq(-1, 5, by = 0.1)\ny &lt;- sin(x)\nplot(x, y)\n\n\n\n\n\n\n\n\nYou can see right away that this simple curve pretty neatly expresses Tobler’s first law: \\(f(x)\\) values of each point are in general more similar to each other for nearby values of \\(x\\). If we want to press this idea into real-world practice, we need a model that can translate TFL into quantitative estimates and qualitative visualizations. There are lots of ways to do this, but we’ll focus in on locally-weighted regression, also known LOWESS.\nThe basic idea of a LOWESS regression is to define a window of size \\(k\\) points around each value one wishes to estimate, and calculate a weighted average of the value of those points, which can then be used as the estimated value \\(\\hat{y_j} \\approx f(x_j)\\). We then run the values of these nearest neighbors through a weight function \\(w(x)\\).\nThese weight functions can take a lot of different forms, but we’ll start simple with a uniform one, i.e. just taking the average of the \\(k\\) nearest neighbors, so that \\(\\hat{y} = sum(z(x_i, k))/k\\), where \\(KNNz\\) is a function returning the \\(y\\) values of the k nearest observations to \\(x_i\\). The value of \\(k\\) is sometimes referred to as the bandwidth of the smoothing function: Larger bandwidths use more data to estimate values at each point, smaller ones use less.\n\nMaking some locally weighted estimates\nUsing the fnn package for R, we can find the indices of the \\(k\\) nearest neighbors of each point we want to make an estimate at:\n\nlibrary(FNN)\nk &lt;- 10\nz &lt;- knn.index(x, k = k)\n\nYou can read the output of this function, below, as indicating the indices (\\(i\\)) of the 10 nearest points to each of the values of \\(x\\).\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    2    3    4    5    6    7    8    9   10    11\n [2,]    1    3    4    5    6    7    8    9   10    11\n [3,]    2    4    1    5    6    7    8    9   10    11\n [4,]    5    3    6    2    1    7    8    9   10    11\n [5,]    6    4    7    3    8    2    1    9   10    11\n [6,]    5    7    4    8    3    9    2   10    1    11\n [7,]    8    6    9    5   10    4   11    3   12     2\n [8,]    7    9   10    6   11    5    4   12    3    13\n [9,]   10    8   11    7   12    6    5   13   14     4\n[10,]    9   11    8   12    7   13   14    6    5    15\n\n\nWe can visualize this by picking a point in the middle of the series and its 10 nearest neighbors and the estimated value of \\(\\hat{y_i}\\) obtained by just taking the average of the k nearest points:\n\nlibrary(ggplot2)\n\n## Plot the original data\ng &lt;- ggplot() +\n  geom_point(aes(x = x, y = y)) +\n  xlab(\"x\") +\n  ylab(\"f(x)\")\n\n## Now, get the index for x = 2\nx_index &lt;- which(x == 2)\n\n## Show the range of k nearest neighbors of this point\nknn_low &lt;- min(x[z[x_index, ]])\nknn_high &lt;- max(x[z[x_index, ]])\ny_hat &lt;- mean(y[z[x_index, ]], )\n\n## Add errorbars to the figure to show the 10 nearest values with the height of the point indicating the estimated value at y_i, denoted by the red dot\ng &lt;- g + geom_errorbarh(aes(xmin = knn_low, xmax = knn_high, y = y_hat)) + geom_point(aes(x = 2, y_hat), colour = \"red\")\n\nplot(g)\n\n\n\n\n\n\n\n\nNotice that if the knn function is applied at the low end of the series, i.e. to the first value, it will use points to the right of that one instead of to either side:\n\nlibrary(ggplot2)\n\n## Plot the original data\ng &lt;- ggplot() +\n  geom_point(aes(x = x, y = y)) +\n  xlab(\"x\") +\n  ylab(\"f(x)\")\n\n## Use the index for the lowest value\nx_index &lt;- 1\n\n## Show the range of k nearest neighbors of this point\nknn_low &lt;- min(x[z[x_index, ]])\nknn_high &lt;- max(x[z[x_index, ]])\ny_hat &lt;- mean(y[z[x_index, ]], )\n\n## Add errorbars to the figure to show the 10 nearest values with the height of the point indicating the estimated value at y_i\ng &lt;- g + geom_errorbarh(aes(xmin = knn_low, xmax = knn_high, y = y_hat)) +\n  geom_point(aes(x = x[x_index], y_hat), colour = \"red\")\n\nplot(g)\n\n\n\n\n\n\n\n\nNow, lets see what happens if we run our smoother over the whole series and take the average of the 10 nearest points for each and compare them to the observed data:\n\ny_hat &lt;- rep(0, length(x))\nfor (i in 1:length(x)) {\n  y_hat[i] &lt;- mean(y[z[i, ]], )\n}\n\nNow plot the predicted vs. the observed values:\n\ng &lt;- ggplot() +\n  geom_point(aes(x = x, y = y)) +\n  xlab(\"x\") +\n  ylab(\"f(x)\") +\n  geom_line(aes(x = x, y = y_hat), colour = \"red\")\nplot(g)\n\n\n\n\n\n\n\n\nYou can see this does a pretty good job all the way through, except at the edges. Lets try it again with a smaller window - or bandwidth - of 5 and see what happens. First, we’ll write a function that will give us the predicted value of y at each point given a window of size k and an input value:\n\nknn_est &lt;- function(x, y, k) {\n  z &lt;- knn.index(x, k = k)\n  y_hat &lt;- rep(0, length(x))\n\n  for (i in 1:length(x)) {\n    y_hat[i] &lt;- mean(y[z[i, ]], )\n  }\n\n  df &lt;- data.frame(x = x, y = y, yhat = y_hat)\n  return(df)\n}\n\n\npred_df &lt;- knn_est(x, y, 5)\ng &lt;- ggplot(pred_df) +\n  geom_point(aes(x = x, y = y)) +\n  geom_line(aes(x = x, y = yhat), colour = \"red\")\nplot(g)\n\n\n\n\n\n\n\n\nThis gets rid of a lot of the weird effects at the edges but introduces some noise into the function. What if we make the window bigger, say 20, to get rid of some of the noise?\n\n\n\n\n\n\n\n\n\nThis seems to make the edge effects worse, as well as the estimates of the function overall worse.\nWhat happens if we go in the opposite direction and shrink the window down to 2?\n\n\n\n\n\n\n\n\n\n\n\nDiscussion Questions\n\nWhy does this appear to be more accurate for these data than \\(k=10\\) and \\(k=5\\)?\nWhat would happen if we added observation noise to the values of \\(y_i\\)? Which one of the smoothers do you think would work better then?\nIs the one best value of \\(k\\) for all datasets? How might you go about picking the best one?\nHow does our uniform weight function express Tobler’s first law? What kind of weight function \\(w(x)\\) might do a better job of capturing the notion of distance decay?",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "smoothing.html#a-motivating-example",
    "href": "smoothing.html#a-motivating-example",
    "title": "8  Smoothing",
    "section": "9.1 A motivating example",
    "text": "9.1 A motivating example\nA spatial transect is an area of space along a line crossing a landscape. These are often used in ecology and forestry to assess the health of an environment, species diversity and other factors. Using a transect can help simplify the problem of spatial analysis down to one dimension rather than the usual two, while still providing a tremendous amount of useful information.\n\n\n\nExample of an ecological transect from the US National Park Service (source)\n\n\nFor example, (levy2014?) were interested in characterizing the intensity of exposure to triatomine bugs and other insect vectors of the pathogen T. cruzi, which causes Chagas disease.\n\n\n\nTriatoma (left- and right-hand panels) and T. cruzi (center) (source)\n\n\n\n\n\nIntensity of Triatomine infestation along a 2km transect in Arequipa, Peru (Figure from (levy2014?))\n\n\nImagine we are estimating the density of some unknown insect vector along a 1 kilometer transect with the goal of characterizing the risk of infection with a vector-borne illness.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "smoothing.html#kernel-density-estimation-in-one-dimension",
    "href": "smoothing.html#kernel-density-estimation-in-one-dimension",
    "title": "8  Smoothing",
    "section": "9.2 Kernel density estimation in one dimension",
    "text": "9.2 Kernel density estimation in one dimension\nMuch like in our discussion of kernel smoothing of continuous outcomes, kernel functions play a key role in this setting as well. In this case, imagine that the locations of vectors along our transect have been sampled at random from some unknown function \\(f(x)\\) which takes values from 0 (the beginning of the transect) to 1000m (the end).\nWe can use the Kernel function \\(K(d)\\) to approximate the intensity of the outcome of interest at each observed case location \\(x_i\\). Imagine that our observed data have locations \\(x_1, x_2, \\ldots, x_n\\) and that the distance between our point of interest, \\(x_j\\) and each observed point is \\(d_{ij} = | x_j - x_i |\\).\nFinally, lets include a bandwidth parameter, \\(h\\), which controls the width of the window we will use for smoothing. When we put this all together, we can get an estimate of the density of our outcome of interest at location \\(x_j\\) as follows:\n\\[\n\\hat{f_h}(x_j) = \\frac{1}{n} \\sum_{i=1}^{n} K(\\frac{x_j - x_i}{h})\n\\]\nAs you can see below, we can pick a range of kernel functions, but for the sake of simplicity, in this example, we will focus in on a Gaussian, or normal, kernel, which uses the probability density function of a normal distribution to weight points.\nLets start by sampling locations of observed points along a one dimensional line. To keep things interesting, we’ll use a Gaussian mixture distribution with two components:\n\n\n\nComparison of different kernel functions (source)",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "smoothing.html#worked-example",
    "href": "smoothing.html#worked-example",
    "title": "8  Smoothing",
    "section": "9.3 Worked example",
    "text": "9.3 Worked example\nFirst, lets imagine a scenario in which the risk of observing an insect vector steadily decreases as we walk along our transect. However, along the way there is a hotspot of increased risk beyond what we would expect from the smooth decline before and after that spot. For the purpose of this example, we’ll assume that risk decays exponentially with distance from the origin, but that our hotspot is centered at a point 300 meters into the transect. The code below lets us sample the locations of the points along the transect where 🐜 are observed from two distributions:\n\nAn exponential distribution representing smooth decay from the beginning to the end of the transect, and\nA normal distribution representing a hotspot about 150m in width beginning 300m in\n\nThe figure below shows a histogram of locations sampled from \\(f(x)\\) (vertical bars) overlaid with the true value of \\(f(x)\\) in red:\n\n\nCode\nlibrary(ggplot2)\nd_a &lt;- dexp(1:1000, rate = 1 / 250)\nd_b &lt;- dnorm(1:1000, mean = 300, sd = 50)\ny &lt;- ((1 - p_hot)) * d_a + (p_hot * d_b)\n\ndens_df &lt;- data.frame(x = 1:1000, y = y)\nxdf &lt;- data.frame(x = x)\n\n\ng &lt;- ggplot(xdf) +\n  geom_histogram(aes(x = x, y = ..density..), bins = 100) +\n  geom_line(data = dens_df, aes(x = x, y = y), colour = \"red\") +\n  xlim(0, 1000) +\n  ylab(\"Density\") +\n  xlab(\"Distance from transect origin (m)\")\nplot(g)\n\n\n\n\n\n\n\n\n\nNow, imagine we have another set of finely spaced points along the line, and for each, we want to calculate the weight for each. The function below lets us do that:\nThe figure below shows the true value of our density function \\(f(x)\\) in red, the density of points in the simulated data along the x-axis of the ‘rug plot’, and our smoothed density in black, for a bandwidth of \\(h=10\\):\n\n\nCode\nlibrary(ggplot2)\npred_df &lt;- normal_smoother(x, h = 10)\n\ng &lt;- ggplot() +\n  geom_rug(aes(x = x)) +\n  geom_line(data = pred_df, aes(x = x, y = y)) +\n  ylab(\"Density\") +\n  geom_line(data = dens_df, aes(x = x, y = y), colour = \"red\") +\n  xlim(0, 1000)\ndens_ojs &lt;- dens_df\ndens_ojs$y &lt;- dens_ojs$y * cc\nplot(g)\n\n\n\n\n\n\n\n\n\nNow, lets see what happens if we try this for different values of \\(h\\):\n\n\nCode\nall_df &lt;- data.frame()\nfor (hv in c(5, 10, 20, 50, 100, 250)) {\n  pred_df &lt;- normal_smoother(x, h = hv)\n  pred_df$h &lt;- hv\n  all_df &lt;- rbind(all_df, pred_df)\n}\n\nall_df$h &lt;- as.factor(all_df$h)\n\ng &lt;- ggplot(all_df) +\n  geom_rug(aes(x = x)) +\n  geom_line(data = dens_df, aes(x = x, y = y), colour = \"red\") +\n  geom_line(aes(x = x, y = y)) +\n  ylab(\"Density\") +\n  facet_wrap(~h) +\n  xlim(0, 1000)\n\nplot(g)\n\n\n\n\n\n\n\n\n\n\n\nCode\nall_df &lt;- data.frame()\nhvals &lt;- seq(1, 100, by = 2)\ndistvals &lt;- seq(-100, 100, by = 1)\nall_kernvals &lt;- data.frame()\nfor (hv in hvals) {\n  pred_df &lt;- normal_smoother(x, h = hv)\n  pred_df$h &lt;- hv\n  pred_df$smoother &lt;- \"gaussian\"\n  all_df &lt;- rbind(all_df, pred_df)\n  all_kernvals &lt;- rbind(all_kernvals, data.frame(x = distvals, y = kdgaussian(distvals, bw = hv), smoother = \"gaussian\", h = hv))\n\n  pred_df &lt;- normal_smoother(x, h = hv, kern = kduniform, kernp = kpuniform)\n  pred_df$h &lt;- hv\n  pred_df$smoother &lt;- \"uniform\"\n  all_df &lt;- rbind(all_df, pred_df)\n  all_kernvals &lt;- rbind(all_kernvals, data.frame(x = distvals, y = kduniform(distvals, bw = hv), smoother = \"uniform\", h = hv))\n\n\n  pred_df &lt;- normal_smoother(x, h = hv, kern = kdtricube, kernp = kptricube)\n  pred_df$h &lt;- hv\n  pred_df$smoother &lt;- \"tricube\"\n  all_df &lt;- rbind(all_df, pred_df)\n  all_kernvals &lt;- rbind(all_kernvals, data.frame(x = distvals, y = kdtricube(distvals, bw = hv), smoother = \"tricube\", h = hv))\n\n  pred_df &lt;- normal_smoother(x, h = hv, kern = kdtriangular, kernp = kptriangular)\n  pred_df$h &lt;- hv\n  pred_df$smoother &lt;- \"triangular\"\n  all_df &lt;- rbind(all_df, pred_df)\n  all_kernvals &lt;- rbind(all_kernvals, data.frame(x = distvals, y = kdtriangular(distvals, bw = hv), smoother = \"triangular\", h = hv))\n}\nall_df$y &lt;- all_df$y * cc",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "smoothing.html#trying-different-bandwidths-and-kernels",
    "href": "smoothing.html#trying-different-bandwidths-and-kernels",
    "title": "8  Smoothing",
    "section": "9.4 Trying different bandwidths and kernels",
    "text": "9.4 Trying different bandwidths and kernels\nYou can adjust the range of the bandwidth here to get a better sense of the relationship between the smoothed curve (black) and true density (red). Adjust the bin width for the histogram of the underlying data to get a sense of the fit of the model to the underlying data.\n\nviewof h = Inputs.range([1, 100], {value: 10, step: 2, label: \"Bandwidth (m)\"})\nviewof bw = Inputs.range([5, 100], {value: 10, step: 5, label: \"Bin width (m)\"})\nviewof kern = Inputs.select([\"gaussian\", \"uniform\", \"tricube\", \"triangular\"], {value: \"gaussian\", label: \"Smoothing kernel\"})\n\nnumbins = Math.floor(1000/bw)\n\ndtrans = transpose(hvals)\nPlot.plot({\ny: {grid: true, \nlabel: \"Density\"},\nx: {\nlabel: \"Distance from transect start (m) →\"\n},\n    marks: [\n    Plot.rectY(transpose(sample),Plot.binX({y: \"count\"}, {x: \"loc\", fill: \"steelblue\", thresholds: numbins})),\n    Plot.lineY(dtrans, {filter: d =&gt; (d.h == h) && (d.smoother == kern), curve: \"linear\", x:\"x\",y: d =&gt; d.y * bw}),\n    Plot.lineY(transpose(dens), {x:\"x\", y: d =&gt; d.y * bw, curve:\"linear\", stroke: \"red\"})\n    ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure below shows the relative amount of weight placed on different points as a function of their distance from the point of interest (0, marked by the vertical red line):\n\nkv = transpose(kernvals).filter(d =&gt; d.h == h && d.smoother == kern)\nPlot.plot({\ny: {grid: true, label: \"Relative weight of point as compared to origin\"},\nx: {\nlabel: \"Distance from point of interest (m) ↔ \"\n},\nmarks: [\n//Plot.lineY(kv, {filter: d =&gt; (d.smoother == kern), x:\"x\", y: d =&gt; d.y*1000}),\nPlot.lineY(kv, Plot.normalizeY({x:\"x\", y: \"y\", basis: \"extent\"})),\nPlot.ruleX([0], {stroke: \"red\"})\n]})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions\n\nWhich of the bandwidth options seems to do the best job in capturing the value of \\(f(x)\\)? Why?\nHow does the choice of kernel impact the smoothing?\nHow do the different kernel functions encode different assumptions about distance decay?\nWhat is the relationship between the histogram of the data and the smoother? What do you see as you change the histogram bin width relative to the smoothing bandwidth?",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "smoothing.html#additional-resources",
    "href": "smoothing.html#additional-resources",
    "title": "8  Smoothing",
    "section": "9.5 Additional Resources",
    "text": "9.5 Additional Resources\nPlease see Matthew Conlen’s excellent interactive KDE tutorial",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "smoothing.html#references",
    "href": "smoothing.html#references",
    "title": "8  Smoothing",
    "section": "9.6 References",
    "text": "9.6 References",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "radon-multilevel.html",
    "href": "radon-multilevel.html",
    "title": "9  Multi-level modeling",
    "section": "",
    "text": "9.1 Modeling household-level variation in radon exposure\nIn this tutorial, we are going to replicate the analysis of household-level variation in radon exposure originally presented in (gelman2006?) (which is actually a tutorial version of (price1996?)). Our goal is to run the models described in the paper using regression models from base R as well as a Bayesian hierarchical model from the rstanarm package. Finally, we will reproduce Figures 1 & 2 from the original paper using ggplot2:",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multi-level modeling</span>"
    ]
  },
  {
    "objectID": "radon-multilevel.html#sec-radon-multilevel",
    "href": "radon-multilevel.html#sec-radon-multilevel",
    "title": "9  Multi-level modeling",
    "section": "",
    "text": "Original Fig 1 from (gelman2006?)\n\n\n\n\n\nOriginal Fig 2 from (gelman2006?)\n\n\n\nPreparation\n\nSet up the workspace\nFirst, we will load the relevant packages:\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(bayesplot)\nlibrary(rstanarm)\nlibrary(purrr)\nlibrary(tidybayes)\n\n\n\nData Preparation\nFirst, lets take the raw radon dataset from the rstanarm package and recode the floor variable to be interpretable as the basement one from the original paper: some minor modifications and additonal datasets that we’ll use for the purposes of modeling and visualizing these data.\n\nradon$basement &lt;- 1 - radon$floor\n\nNow we can see that that the dataset has all of the variables we need:\n\n\n  floor county  log_radon log_uranium basement\n1     1 AITKIN 0.83290912  -0.6890476        0\n2     0 AITKIN 0.83290912  -0.6890476        1\n3     0 AITKIN 1.09861229  -0.6890476        1\n4     0 AITKIN 0.09531018  -0.6890476        1\n5     0  ANOKA 1.16315081  -0.8473129        1\n6     0  ANOKA 0.95551145  -0.8473129        1\n\n\n\n\n\nModels\n\n🚪 Door 1: Full pooling!\nThis corresponds to a model in which we are assuming exactly no variation across locations in terms of the baseline level of radon. So, we can run a simple regression model where we assume that:\n\\[\ny_{ij} = \\alpha + \\beta x_{ij} + \\epsilon_{i}\n\\]\nWhere \\(x_{ij} = 1\\) if a house has a basement and 0 otherwise.\nIn R, we can fit this model via least squares using a single line of code:\n\nm1 &lt;- lm(log_radon ~ basement, data = radon)\n\nWe can call the summary function to get a description of the key coefficients and the goodness-of-fit:\n\n\nlm(formula = log_radon ~ basement, data = radon)\n            coef.est coef.se\n(Intercept) 0.78     0.06   \nbasement    0.59     0.07   \n---\nn = 919, k = 2\nresidual sd = 0.79, R-Squared = 0.07\n\n\n\n\n🚪 Door 2: No pooling\nThe second approach is the “No Pooling” one in which we allow the baseline intensity of radon in each county (represented by the intercept term \\(\\alpha_j\\)) to vary, but we don’t do anything to constrain that variation. In other words, we treat each county as though it was independent.\nHowever, to estimate a consistent effect of having a basement across all counties, we estimate a single \\(\\beta\\) term. This leads to a model that looks like this:\n\\[\ny_{ij} = \\alpha_j + \\beta x_{ij} + \\epsilon_{i}\n\\]\nIn R this is easy to implement, because we are implicitly asking the regression model to treat county as a categorical variable if we pass it to it as a factor datatype:\n\nno_pool_m &lt;- lm(log_radon ~ basement + log_uranium + county, data = radon)\n\n\n\nlm(formula = log_radon ~ basement + log_uranium + county, data = radon)\n                     coef.est coef.se\n(Intercept)           0.42     0.37  \nbasement              0.69     0.07  \nlog_uranium           0.32     0.60  \ncountyANOKA           0.09     0.44  \ncountyBECKER          0.48     0.53  \ncountyBELTRAMI        0.67     0.43  \ncountyBENTON          0.39     0.48  \ncountyBIGSTONE        0.31     0.68  \ncountyBLUEEARTH       0.83     0.51  \ncountyBROWN           0.80     0.60  \ncountyCARLTON         0.05     0.38  \ncountyCARVER          0.43     0.50  \ncountyCASS            0.52     0.47  \ncountyCHIPPEWA        0.56     0.60  \ncountyCHISAGO         0.21     0.48  \ncountyCLAY            0.79     0.54  \ncountyCLEARWATER      0.28     0.50  \ncountyCOOK           -0.23     0.60  \ncountyCOTTONWOOD      0.06     0.63  \ncountyCROWWING        0.26     0.40  \ncountyDAKOTA          0.27     0.36  \ncountyDODGE           0.63     0.63  \ncountyDOUGLAS         0.60     0.49  \ncountyFARIBAULT      -0.42     0.57  \ncountyFILLMORE        0.18     0.75  \ncountyFREEBORN        0.94     0.51  \ncountyGOODHUE         0.80     0.48  \ncountyHENNEPIN        0.32     0.34  \ncountyHOUSTON         0.52     0.66  \ncountyHUBBARD         0.30     0.44  \ncountyISANTI          0.22     0.57  \ncountyITASCA          0.07     0.42  \ncountyJACKSON         0.83     0.59  \ncountyKANABEC         0.18     0.50  \ncountyKANDIYOHI       0.94     0.54  \ncountyKITTSON         0.51     0.55  \ncountyKOOCHICHING     0.04     0.52  \ncountyLACQUIPARLE     1.75     0.71  \ncountyLAKE           -0.40     0.44  \ncountyLAKEOFTHEWOODS  0.99     0.51  \ncountyLESUEUR         0.60     0.55  \ncountyLINCOLN         1.08     0.67  \ncountyLYON            0.75     0.59  \ncountyMAHNOMEN        0.23     0.84  \ncountyMARSHALL        0.53     0.44  \ncountyMARTIN         -0.05     0.51  \ncountyMCLEOD          0.17     0.46  \ncountyMEEKER          0.13     0.49  \ncountyMILLELACS      -0.07     0.60  \ncountyMORRISON        0.11     0.41  \ncountyMOWER           0.54     0.51  \ncountyMURRAY          1.27     0.90  \ncountyNICOLLET        0.99     0.59  \ncountyNOBLES          0.71     0.68  \ncountyNORMAN          0.10     0.63  \ncountyOLMSTED         0.16     0.48  \ncountyOTTERTAIL       0.60     0.40  \ncountyPENNINGTON      0.11     0.54  \ncountyPINE           -0.24     0.43  \ncountyPIPESTONE       0.62     0.68  \ncountyPOLK            0.55     0.60  \ncountyPOPE            0.11     0.70  \ncountyRAMSEY          0.22     0.33  \ncountyREDWOOD         0.78     0.61  \ncountyRENVILLE        0.46     0.67  \ncountyRICE            0.70     0.49  \ncountyROCK            0.06     0.79  \ncountyROSEAU          0.64     0.36  \ncountySCOTT           0.70     0.43  \ncountySHERBURNE       0.24     0.44  \ncountySIBLEY          0.10     0.58  \ncountySTLOUIS        -0.03     0.31  \ncountySTEARNS         0.38     0.43  \ncountySTEELE          0.41     0.53  \ncountySTEVENS         0.56     0.77  \ncountySWIFT          -0.18     0.61  \ncountyTODD            0.65     0.54  \ncountyTRAVERSE        0.76     0.69  \ncountyWABASHA         0.69     0.50  \ncountyWADENA          0.43     0.48  \ncountyWASECA         -0.47     0.58  \ncountyWASHINGTON      0.31     0.34  \ncountyWATONWAN        1.54     0.60  \ncountyWILKIN          1.06     0.86  \ncountyWINONA          0.41     0.60  \ncountyWRIGHT          0.59     0.39  \n---\nn = 919, k = 86\nresidual sd = 0.73, R-Squared = 0.29\n\n\n\n\n🚪 Door 3: Partial Pooling\nFinally, we get to the partial pooling, hierarchical model in which we introduce a hierarchical prior to the model to allow our model to shrink observations from places with few observations towards the population mean. This allows us to avoid the pitfalls of overfitting associated with the no-pooling approach while not making the homogeneity assumptions associated with the full-pooling approach.\nThis works out to a multi-level model that allows random variation in household-level radon measurements as well as variation at the county level in radon levels above or below the amount predicted by the county-level soil uranium measure. Much like the no-pooling model, we can write outcomes for individuals as:\n\\[\ny_{ij} = \\alpha_j + \\beta x_{ij} + \\epsilon_{i}\n\\]\nHowever, rather than stopping there, we introduce a second level of random variation to the county-level intercepts, \\(\\alpha_j\\).\n\\[\n\\alpha_j = \\gamma_0 + \\gamma \\zeta_{j} + \\epsilon_{j}\n\\]\nWhere \\(\\epsilon_i \\sim N(0, \\sigma_i)\\) and \\(\\epsilon_j \\sim N(0, \\sigma_j)\\).\nTo fit this model, we’ll use the rstanarm package, which uses the Stan Bayesian modeling language under the hook to fit the model. This model introduces another piece of syntax to our equation, which now reads log_radon ~ basement + log_uranium + (1 | county). The interesting part of this is the (1 | county) which is a syntax used by rstanarm and other hierarchical modeling packages (such as lme4) to specify random intercepts (typically represented by a 1 in the matrix of regressors) for each of a set of clusters, in this case counties. In this model, the county-level intercept terms are implicitly assumed to be normally distributed with unknown variance \\(\\sigma_j\\) which will be estimated when the model is fit.\nWe use the stan_lmer function to fit a hierarchical linear model with a normally-distributed response variable, as follows:\n\nm2 &lt;- stan_lmer(log_radon ~ basement + log_uranium + (1 | county), data = radon)\n\nBecause this model is fit by MCMC, we can use draws from the posterior distribution to understand uncertainty in the model. For example, this visualization of the median prediction and credible intervals for the basement and uranium effects can be visualized using the mcmc_areas function from the bayesplot package:\n\nposterior &lt;- as.matrix(m2)\ng2 &lt;- mcmc_areas(posterior, pars = c(\"basement\", \"log_uranium\"))\nplot(g2)\n\n\n\n\n\n\n\n\n\n\n\nMaking the Figures\n\nFigure 1\n\nData Preparation\nSince each row of radon dataset includes an observation of a single house, we need to work backwards to obtain the county-level soil uranium measure for each individual county. This is pretty straightforward to do using the dplyr package:\n\ncounty_uranium &lt;- radon %&gt;%\n  group_by(county) %&gt;%\n  summarize(log_uranium = first(log_uranium))\n\nWe will also make a second dataset that we will use for storing the predicted radon levels for households with and without basements each for county. This contains 2 entries for each county, representing observations taken in the basement or on the first floor.\n\ncounty_uranium_tmp_1 &lt;- county_uranium\ncounty_uranium_tmp_1$basement &lt;- 1\ncounty_uranium_tmp_2 &lt;- county_uranium\ncounty_uranium_tmp_2$basement &lt;- 0\n\ncounty_dummy_df &lt;- rbind(county_uranium_tmp_1, county_uranium_tmp_2)\n\nNow, we will take each of our fitted models (fully pooled, unpooled and partially pooled) and put their predicted values into our plotting dataset\n\ncounty_dummy_df$pooled_pred &lt;- predict(m1, county_dummy_df)\ncounty_dummy_df$no_pool_pred &lt;- predict(no_pool_m, county_dummy_df)\n\nBecause the partial pooling model was fit using MCMC, we will take a slightly different approach and use the median of the posterior predictive distribution for each observation, which is analogous to (but not exactly the same as) the OLS predictions from the other models:\n\n## Gives posterior median for each prediction.\ncounty_dummy_df$partial_pred &lt;- posterior_predict(m2, county_dummy_df) %&gt;%\n  apply(2, median)\n\n\n\n\nPlotting\nTo re-create Figure 1, we will subset out the observed data and predictions for the 8 counties included in the original figure:\n\n## Place the county names in a vector we will use to keep track of them\nfig_1_counties &lt;-\n  c(\n    \"LACQUIPARLE\",\n    \"AITKIN\",\n    \"KOOCHICHING\",\n    \"DOUGLAS\",\n    \"CLAY\",\n    \"STEARNS\",\n    \"RAMSEY\",\n    \"STLOUIS\"\n  )\n\n\n# First, using the `county_dummy_df` with the basement/non-basement predictions in it,\n# subset out the relevant counties and make a new county factor variable which\n# will be used to ensure that the counties in Fig. 1 plot in the right order\n\ncounty_df_fig_1 &lt;- county_dummy_df %&gt;%\n  filter(county %in% fig_1_counties) %&gt;%\n  mutate(county2 = factor(county, levels = fig_1_counties)) %&gt;%\n  arrange(county)\n\n## Now select out the households in the original data that\n## are in each county and create another county-level factor\n## variable in the same order\n\npred_counties &lt;- radon %&gt;%\n  filter(county %in% fig_1_counties) %&gt;%\n  mutate(county2 = factor(county, levels = fig_1_counties))\n\nOnce we have the datasets together for the figure, we can begin constructing it using ggplot2:\n\ng &lt;- ggplot() +\n  ## The geom_jitter geom plots the log_radon values for each household and\n  ## jitters the points slightly to avoid overplotting.\n  geom_jitter(\n    data = pred_counties,\n    aes(x = basement, y = log_radon, group = county2),\n    height = 0,\n    width = 0.1\n  ) +\n\n  ## This superimposes the partial-pooling (α + β x_i + ϵ_i +ϵ_j) predictions\n  ## over the raw data\n  geom_line(\n    data = county_df_fig_1,\n    aes(x = basement, y = partial_pred, group = county2),\n    linetype = \"solid\",\n    colour = \"gray\"\n  ) +\n\n  ## No-pooling predictions (α_{ij} + β x_i + ϵ_i)\n  geom_line(\n    data = county_df_fig_1,\n    aes(x = basement, y = no_pool_pred, group = county2)\n  ) +\n\n  ## Full pooling predicitons (α + β x_i + ϵ_i)\n  geom_line(\n    data = county_df_fig_1,\n    aes(x = basement, y = pooled_pred, group = county2),\n    linetype = \"dashed\"\n  ) +\n\n  ## Finally, use facet_wrap to arrange the panels in two\n  ## rows of four\n  facet_wrap(vars(county2), nrow = 2) +\n  xlab(\"basement\") +\n  ylab(\"log radon level\") +\n  theme_bw() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\nplot(g)\n\n\n\n\n\n\n\n\n\n\nFigure 2\nFigure 2 reproduces the relationship between the county-level random intercepts, \\(\\alpha_j\\) and the expected level of radon at a county level as a function of county-level soil uranium.\n\n\nData Preparation\nThe following code allows us to extract predictions at the county level using our prediction dataset. To do this, we use the predicted_draws function from the tidybayes package, which lets us sample from the posterior distribution of the fitted model. The median_qi function, also from tidybayes, lets us calculate the width of a 1 standard error interval (equivalent to the range containing ~17% of the posterior probability mass around the posterior median) used in the original Figure 1 from (gelman2006?):\n\ndd &lt;- predicted_draws(m2, county_dummy_df) %&gt;%\n  median_qi(.width = 0.17) %&gt;%\n  filter(basement == 0)\n\nIn order to calculate the predicted mean radon at a county level, we need to access the coefficients corresponding to the level two model, including the intercept \\(\\gamma_0\\) and the effect of a 1-log change in log-uranium on predicted log-radon, \\(\\gamma_1\\). In order to get these values out of the model, we can use the gather_draws function from tidybayes, which allows us to access the posterior distributions for each of these parameters:\n\nuranium_coefs &lt;-\n  gather_draws(m2, c(`(Intercept)`, log_uranium)) %&gt;% median_qi()\n\nNow it is as simple as calculating the linear predictor \\(\\gamma_0 + \\gamma_1 z_j\\), where \\(z_j\\) is the log-uranium measure for the j-th county, and storing this information in a data frame we will use for plotting:\n\nlog_uranium_range &lt;-\n  seq(min(county_uranium$log_uranium) - .1,\n    max(county_uranium$log_uranium) + .1,\n    by = 0.1\n  )\n\npred_log_radon &lt;-\n  uranium_coefs$.value[1] + uranium_coefs$.value[2] * log_uranium_range\n\nmedian_radon_pred &lt;-\n  data.frame(log_uranium = log_uranium_range, .prediction = pred_log_radon)\n\n\n\nPlotting\nNow, we can build this figure up one step at a time, starting with our mean predictions:\n\ng &lt;- ggplot(dd) +\n  geom_line(data = median_radon_pred, aes(x = log_uranium, y = .prediction))\n\nplot(g)\n\n\n\n\n\n\n\n\nThe next step is to then add the median predictions (points) and 1 SE errorbars to the plot, and then fix the theme to match the original figure, et voilà!\n\ng &lt;- g + geom_point(aes(x = log_uranium, y = .prediction, group = county)) +\n  geom_errorbar(aes(\n    x = log_uranium,\n    y = .prediction,\n    ymin = .lower,\n    ymax = .upper\n  )) +\n  theme_bw() + theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  xlab(\"county-level uranium measure\") +\n  ylab(\"regression intercept\")\n\nplot(g)",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multi-level modeling</span>"
    ]
  },
  {
    "objectID": "radon-multilevel.html#sec-spatial-radon",
    "href": "radon-multilevel.html#sec-spatial-radon",
    "title": "9  Multi-level modeling",
    "section": "9.2 Taking a spatial perspective on the radon data",
    "text": "9.2 Taking a spatial perspective on the radon data\nThis tutorial is a follow-up to a prior exercise using these data. So if you haven’t already, please go back and take a look at the original multi-level modeling radon example in Section 9.1.\n\nLearning Goals\nThe primary goals of this tutorial are to introduce you to:\n\nMerging of non-spatial health exposure or outcome data with spatial metadata.\nCalculation of important spatial summary statistics, e.g. Moran’s I, from such data.\nSpatial analysis of residuals from aspatial regression models of spatially-referenced data.\n\n\n\n\n\n\n\nLook out 👀 for stretch exercises!\n\n\n\nIf you see a box with a 💡 like this, it’s in an invitation to go a bit further. This could be a conceptual question or a chance to write a bit of code to explore the data or outputs of the analysis a bit more.\n\n\n\n\nSetting up the environment\n\n\nCode\nlibrary(ggplot2)\nlibrary(arm)\nlibrary(tidycensus)\nlibrary(dplyr)\nlibrary(rstanarm)\nlibrary(stringr)\nlibrary(spdep)\nknitr::opts_chunk$set(message = FALSE, warning = FALSE, tidy = TRUE)\n\n\n\n\nData Preparation\nBefore diving into the analysis steps, there are several key things we need to do to be able to easily work with these data.\n\nDownload a shapefile for Minnesota\nFirst, we need to download a shapefile for the state of Minnesota in which each polygon represents an individual county. Thankfully, in R, this is made easy using the excellent tidycensus package:\n\noptions(tigris_use_cache = TRUE)\n\nminnesota &lt;- get_acs(\n  state = \"MN\",\n  geography = \"county\",\n  variables = \"B19013_001\",\n  geometry = TRUE,\n  year = 2020\n)\n\nTidycensus gives us the data as an sf dataframe containing a number of fields including population estimates, which we can plot straightforwardly using the plot function supplied by the sf package:\n\nplot(minnesota[\"estimate\"])\n\n\n\n\n\n\n\n\n\n\nMerge the spatial data with the radon data\nIn its raw form, this spatial dataset isn’t quite ready to merge with the radon data. If we take a peek at the county names in the shapefile, we can see that they don’t quite match the formatting of the ones in the original data:\n\nhead(sort(minnesota$NAME))\n\n[1] \"Aitkin County, Minnesota\"    \"Anoka County, Minnesota\"    \n[3] \"Becker County, Minnesota\"    \"Beltrami County, Minnesota\" \n[5] \"Benton County, Minnesota\"    \"Big Stone County, Minnesota\"\n\n\nWhereas in the radon data we see:\n\nhead(unique(as.character(radon$county)))\n\n[1] \"AITKIN\"   \"ANOKA\"    \"BECKER\"   \"BELTRAMI\" \"BENTON\"   \"BIGSTONE\"\n\n\nThe big differences here are that the shapefile uses: 1) mixed-case county names and 2) includes the name of the state in each label. To make these match the radon dataset, we can use some tools from the stringr package as well as some base R functions:\n\nminnesota &lt;-\n  minnesota %&gt;% mutate(\n    ## Since all of the original county names have the same substring \" County, Minnesota\"\n    ## we can use the str_remove function to pull them out of all of them\n    county = str_remove(NAME, \" County, Minnesota\") %&gt;%\n      ## Since some of the counties  officially have two-word names (e.g. Big Stone)\n      ## which are collapsed in the radon dataset, we will use this function to remove all spaces:\n      str_replace_all(\" \", \"\") %&gt;%\n      ## A few county names include abbreviations indicated by the presence of a '.' (e.g. St. Louis)\n      ## so we will get rid of that bit of punctuation since it is not in the original data\n      str_replace_all(\"\\\\.\", \"\") %&gt;%\n      ## Finally, convert all the county names to uppercase\n      toupper()\n  )\n\nNow, the county labels should match:\n\nhead(sort(minnesota$county))\n\n[1] \"AITKIN\"   \"ANOKA\"    \"BECKER\"   \"BELTRAMI\" \"BENTON\"   \"BIGSTONE\"\n\n\n\n\nPreparing the radon dataset\nWe will repeat the steps from the earlier tutorial in order to prepare our data for analysis:\n\nradon &lt;- radon %&gt;% mutate(basement = 1 - floor)\n\ncounty_uranium &lt;- radon %&gt;%\n  group_by(county) %&gt;%\n  summarize(\n    log_uranium = first(log_uranium),\n    mean_radon = mean(log_radon)\n  )\n\nBecause the sf dataset returned by tidycensus is a dataframe, we can then easily merge the county-level soil uranium concentrations we derived above into the shapefile. We use the left_join function from dplyr to ensure that all of the counties in the original shapefile are represented in the final dataset, even if a soil uranium measure is unavailable for them in the original data:\n\nminnesota_radon &lt;- left_join(minnesota, county_uranium)\n\nJoining with `by = join_by(county)`\n\n\nWe can then plot the log-uranium measures on the map and see that, in fact, they are quite spatially correlated. We can also see that there appear to be two counties which are missing soil uranium data in the radon dataset. To have a bit more control over our plots, we’ll switch here to using the geom_sf function of ggplot2, which makes plotting geographies from sf objects easy:\n\ng &lt;- ggplot(minnesota_radon) +\n  geom_sf(aes(fill = log_uranium)) +\n  scale_fill_viridis_c() +\n  ggtitle(\"Soil uranium by MN county\")\nplot(g)\n\n\n\n\n\n\n\n\n\n\n\nMeasuring Spatial Correlation\nTo validate our hunch that soil uranium is spatially concentrated in Minnesota, we can calculate the value of Moran’s I for these data using some functions from the spdep package. First, we use the poly2nb function to obtain the neighbors for each polygon, which will be used to calculate Moran’s I.\n\nnb &lt;- poly2nb(minnesota_radon)\n\nThis function yields an R list in which each entry is a vector with the indices for the neighbors of the i-th county. For example, this prints the neighbors of the first three counties in the dataset:\n\n\nCode\nprint(nb[1:3])\n\n\n[[1]]\n[1] 31 47 61 86\n\n[[2]]\n[1] 55 72 75\n\n[[3]]\n[1]  5 14 26 37 38 65\n\n\nWe then pass this function to the nb2listw function to obtain weights for the relationships between neighbors. Here, we use the simplest option available, “B”, for binary weights equal to 1 if the areas are neighbors and 0 otherwise:\n\nlw &lt;- nb2listw(nb, style = \"B\", zero.policy = TRUE)\nprint(lw$weights[1:3])\n\n[[1]]\n[1] 1 1 1 1\n\n[[2]]\n[1] 1 1 1\n\n[[3]]\n[1] 1 1 1 1 1 1\n\n\nFinally, we can pass these weights, along with some additional information including the outcome of interest at each location, the total number of locations, and the sum of all the weights to the moran function. The NAOK=TRUE option used here also allows the function to drop locations where data are missing:\n\nradon_i &lt;- moran(minnesota_radon$log_uranium, lw, length(nb), Szero(lw), NAOK = TRUE)$I\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\n\nWhen we do this, we find that the value of Moran’s I = 0.71, which is close to the maximum value of 1. Since we’ll be returning to the calculation of Moran’s I using our spatial data, lets pack it up into a function:\n\nmoranFromSF &lt;- function(x, sfdf, style = \"B\") {\n  nb &lt;- poly2nb(sfdf)\n  lw &lt;- nb2listw(nb, style = style, zero.policy = TRUE)\n  mi &lt;- moran(x, lw, length(nb), Szero(lw), NAOK = TRUE)$I\n  return(mi)\n}\n\nprint(moranFromSF(minnesota_radon$log_uranium, minnesota_radon))\n\n[1] 0.712615\n\n\nOf course, our key quantity of interest isn’t soil uranium but the concentration of radon at the household level. When we constructed the county_uranium dataset above, we also calculated the median radon concentration in the data for each county. When we plot it, we see something similar to the soil uranium, but perhaps a bit less clear:\n\ng &lt;- ggplot(minnesota_radon) +\n  geom_sf(aes(fill = mean_radon)) +\n  scale_fill_viridis_c() +\n  ggtitle(paste0(\"Median household radon by MN county (I=\", round(moranFromSF(minnesota_radon$mean_radon, minnesota_radon), 2), \")\"))\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\nplot(g)\n\n\n\n\n\n\n\n\nAs you can see in the figure, the value of Moran’s I is smaller than we got for log-uranium but still substantial.\n\n\n\n\n\n\nWhat’s going on?\n\n\n\nPause here and take a moment to try to figure out what might account for the difference in this intensity of clustering in radon vs. soil uranium measurements.\n\n\n\nTesting, testing\nOne way to determine whether the spatial aggregation of the radon measurements is meaningful is to compare it to a counterfactual scenario in which the distribution of radon concentrations is uncorrelated with space. This assumption, known as complete spatial randomness (or CSR), allows us to provide a benchmark against which we determine whether the value of Moran’s I we determined is highly likely to occur by chance alone. Thankfully, it is easy to generate a dataset in which the median radon values are distributed randomly across the map:\n\n## Make a new dataset representing 'random minnesota':\n## Use the sample function to resample household radon values without replacement,\n## we then recalculate county values based on these suffled values\ncounty_uranium_random &lt;- radon %&gt;%\n  mutate(log_radon = sample(log_radon, nrow(.), replace = FALSE)) %&gt;%\n  group_by(county) %&gt;%\n  summarize(\n    log_uranium = first(log_uranium),\n    mean_radon = mean(log_radon)\n  )\n\nrandom_minnesota &lt;- left_join(minnesota, county_uranium_random)\n\nJoining with `by = join_by(county)`\n\n## Plot the new randomized data\ng &lt;- ggplot(random_minnesota) +\n  geom_sf(aes(fill = mean_radon)) +\n  scale_fill_viridis_c() +\n  ggtitle(paste0(\"Spatially randomized median radon by MN county (I=\", round(moranFromSF(random_minnesota$mean_radon, random_minnesota), 2), \")\"))\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\nplot(g)\n\n\n\n\n\n\n\n\nThis yields something that looks pretty randomly distributed, which is reflected in a Moran’s I estimate closer to the null value of 0. This doesn’t necessarily tell us whether this result is meaningful rather than an artifact of random chance.\n\n\n\n\n\n\nWhat is the same? What is different?\n\n\n\nTake a minute to explore the distribution of different quantities between some random minnesotas and the observed one. For example, look at distributions of the number of observations per county, the proportion of households in each county that have basements, etc. Which are similar and which are different?\n\n\n\n\nComplete Spatial Randomness\nWhat we can do, though, is to generate a bunch of random Minnesotas in which there is no relationship between geographic location and median radon, calculate Moran’s I for each of those, and see how our observed data stack up.\n\ncsrMorans &lt;- function(radon, minnesota, trials = 1000, style = \"B\") {\n  county_uranium &lt;- radon %&gt;%\n    group_by(county) %&gt;%\n    summarize(\n      log_uranium = first(log_uranium),\n      mean_radon = mean(log_radon)\n    ) %&gt;%\n    left_join(minnesota, .)\n\n  nb &lt;- poly2nb(minnesota)\n  lw &lt;- nb2listw(nb, style = style, zero.policy = TRUE)\n  mv &lt;- moran(county_uranium$mean_radon, lw, length(nb), Szero(lw), NAOK = TRUE)$I\n\n\n  moran_vals &lt;- rep(0, trials)\n  for (i in 1:trials) {\n    county_uranium_random &lt;- radon %&gt;%\n      mutate(log_radon = sample(log_radon, nrow(.), replace = FALSE)) %&gt;%\n      group_by(county) %&gt;%\n      summarize(\n        log_uranium = first(log_uranium),\n        mean_radon = mean(log_radon)\n      )\n\n    random_minnesota &lt;- left_join(minnesota, county_uranium_random)\n    moran_vals[i] &lt;- moran(random_minnesota$mean_radon, lw, length(nb), Szero(lw), NAOK = TRUE)$I\n  }\n\n  return(list(\n    midist = moran_vals,\n    mi = mv\n  ))\n}\n\ncsr_dist &lt;- csrMorans(radon, minnesota)\n\nWe can use the distribution of Moran’s I values taken from the randomized datasets to benchmark how likely our observed value is to occur by purely random chance. The figure below shows that this is quite unlikely:\n\ng &lt;- ggplot() +\n  geom_histogram(aes(x = csr_dist$midist), bins = 50) +\n  xlab(\"Moran's I value\") +\n  geom_vline(xintercept = csr_dist$mi, colour = \"red\") +\n  geom_vline(xintercept = median(csr_dist$midist), colour = \"green\") +\n  ggtitle(\"Randomized values of Moran's I vs. observed for median household radon\")\n\nplot(g)\n\n\n\n\n\n\n\n\nAnd we can directly estimate this probability as follows:\n\nreal_moran &lt;- moranFromSF(minnesota_radon$mean_radon, minnesota_radon)\np_moran &lt;- sum(csr_dist$midist &gt;= csr_dist$mi) / length(csr_dist$midist)\nprint(p_moran)\n\n[1] 0.001\n\n\nFrom 1000 samples, it appears that none of our random datasets yielded a value of Moran’s I \\(\\ge\\) to the observed value, suggesting that it is unlikely that we would observe this value as a simple function of sampling variability.\n\n\n\n\n\n\nWhat could go wrong?\n\n\n\nBefore you move on, take a minute to think about what some of the potential flaws in our CSR-based approach to assessing the meaningfulness or signficance of this result might be.\n\n\n\n\n\nModels!\nUp to this point, we have relied on county-level summaries of the household-level radon data. For the final section of this tutorial, we are going to go back to using the full dataset and implement regression models that are able to characterize variation at the household and community level. Specifcially, we are going to first fit the full-pooling, no-pooling and partial pooling models from the original (gelman2006?) paper. We won’t go into detail on these as they have been discussed in depth in the original paper and the previous post.\n\n\n\n\n\n\nConfused?\n\n\n\nFor more detail on the implementation on interpretation of these models, please check out Section 9.1.\n\n\n\nFull-pooling model\nThe full-pooling model has the following form, in which the variable \\(x_{ij}\\) indicates whether house \\(i\\) in county \\(j\\) has a basement (1) or not (0).\n\\[\ny_{ij} = \\alpha + \\beta x_{ij} + \\epsilon_{i}\n\\]\n\nfull_pooling_model &lt;- lm(log_radon ~ basement, data = radon)\nradon$full_pooling_resid &lt;- resid(full_pooling_model)\nradon$full_pooling_pred &lt;- predict(full_pooling_model)\n\n\n\n\n\n\n\nStoring Model Predictions\n\n\n\nNote that we are storing the residuals and predictions for this model (and the ones below) as a column inside the radon dataframe.\n\n\n\n\n\nNo Pooling\nThe no-pooling model assumes essentially that each county is indepenedent, and includes a categorical variable for the county that the observed household is in:\n\\[\ny_{ij} = \\alpha_j + \\beta x_{ij} + \\epsilon_{i}\n\\]\n\nno_pooling_model &lt;- lm(log_radon ~ basement + county, data = radon)\nradon$no_pooling_resid &lt;- resid(no_pooling_model)\nradon$no_pooling_pred &lt;- predict(no_pooling_model)\n\n\nPartial pooling model\nThe partial-pooling model is the multi-level analogue to the no-pooling model. For more detail, please see the partial pooling section of the original tutorial.\n\npartial_pool_model &lt;- stan_lmer(log_radon ~ basement + log_uranium + (1 | county), data = radon)\nradon$partial_pooling_resid &lt;- resid(partial_pool_model)\nradon$partial_pooling_pred &lt;- posterior_predict(partial_pool_model) %&gt;% apply(2, mean)\n\n\n\n\nResidual Analysis\nOne thing that is important to note is that none of the regression models we are looking at directly account for spatial clustering. In other words, the spatial arrangement of the counties is not an input to the model. This doesn’t mean that they cannot adequately account for spatial correlation through the inclusion of key covariates, however.\nOne way to assess how well a model is accounting for observed and unobserved spatial hererogeneity is to examine the model residuals for evidence of spatial clustering, which is what we will do in this section.\nSince the residuals for each model are generated at the level of individual households, we will go back to working with county-level summaries of both the prediction error (residuals) and predicted household radon values:\n\nresults_by_county &lt;- radon %&gt;%\n  group_by(county) %&gt;%\n  summarize(\n    p_basement = sum(basement) / n(),\n    full_pooling_resid = mean(full_pooling_resid),\n    no_pooling_resid = mean(no_pooling_resid),\n    full_pooling_pred = mean(full_pooling_pred),\n    no_pooling_pred = mean(no_pooling_pred),\n    partial_pooling_pred = mean(partial_pooling_pred),\n    partial_pooling_resid = mean(partial_pooling_resid)\n  )\nresults_by_county &lt;- left_join(minnesota, results_by_county)\n\nJoining with `by = join_by(county)`\n\n\n\nFull Pooling Residuals\nWhen we look at the results of the full-pooling model, the residuals that still look pretty spatially clustered, and this is reflected in the value of Moran’s I &gt; 0:\n\nmi &lt;- round(moranFromSF(results_by_county$full_pooling_resid, results_by_county), 2)\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\ng &lt;- ggplot(results_by_county) +\n  geom_sf(aes(fill = full_pooling_resid)) +\n  scale_fill_viridis_c() +\n  ggtitle(paste0(\"Full pooling residuals with I=\", mi))\nplot(g)\n\n\n\n\n\n\n\n\nThis is probably intuitive: the full pooling model didn’t include any county-level information, so it might not account for all of the sptial variation. On the flipside, if we look at the predictions of the model - reflecting the expected household levels of radon in each county - should we should expect to find that they are spatially un-clustered or also clustered?\n\nmi &lt;- round(moranFromSF(results_by_county$full_pooling_pred, results_by_county), 2)\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\ng &lt;- ggplot(results_by_county) +\n  geom_sf(aes(fill = full_pooling_pred)) +\n  scale_fill_viridis_c() +\n  ggtitle(paste0(\"Full pooling predictions with I=\", mi))\nplot(g)\n\n\n\n\n\n\n\n\nWait - what? The predictions are also quite clustered, although the pattern looks a bit like a photographic negative of the residual map. It looks like our model is predicting lower values in the northwest corner of the state relative to the rest of the state. How is this possible, if our model doesn’t include contextual information?\nThis might be explained by differences in composition at the county level: maybe houses in some counties are more likely to have basements than in others? If this is the case, then those high-basement counties may have higher avg. levels of radon. So, lets just check and see if our one predictor - the presence or absence of a basement - exhibits any spatial variability?\n\nmi &lt;- round(moranFromSF(results_by_county$p_basement, results_by_county), 2)\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\ng &lt;- ggplot(results_by_county) +\n  geom_sf(aes(fill = p_basement)) +\n  scale_fill_viridis_c() +\n  ggtitle(paste0(\"Proportion of surveyed households with a basement, I=\", mi))\nplot(g)\n\n\n\n\n\n\n\n\nWhoops…that looks familiar! It seems like the pattern of spatial variation in the presence/absence of basements may be driving the clustering in our predictions and - by consequence - our residuals!\n\n\n\n\n\n\nSpatially correlated predictors → Spatially correlated predictions\n\n\n\nSometimes, it is easy to forget that the input data may be as or more correlated than the outcome data. In this example, the presence or absence of a basement in a house seems to have a spatial pattern and this impacts the spatial patterning of our predictions and model residuals!\n\n\nSo it looks like we are over-predicting risk in some areas where more surveyed households have basements and under-predicting it in other places where fewer households have basements.\n\n\nNo Pooling\nOk, so lets try this again with our no-pooling model which at least includes the counties as categorical covariates. Unless something weird is going on, this model should do a good job of explaining spatial variation:\n\n\nCode\nmi &lt;- round(moranFromSF(results_by_county$no_pooling_resid, results_by_county), 2)\n\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\n\nCode\ng &lt;- ggplot(results_by_county) +\n  geom_sf(aes(fill = no_pooling_resid)) +\n  scale_fill_viridis_c() +\n  ggtitle(paste0(\"No pooling residuals with I=\", mi))\nplot(g)\n\n\n\n\n\n\n\n\n\nWell, that’s a bit better, although it does such a good job at explaining away the overall variability in our measurments, we might be concerned that it is overfitting the model through the inclusion of the county level random effects. This is evidenced in the tiny size of the residuals and their minimal variation:\n\n\nCode\ng &lt;- ggplot() +\n  geom_histogram(aes(x = results_by_county$no_pooling_resid))\nplot(g)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nUnsurprisingly, this model does an excellent job of predicting the spatial patterns in the original data:\n\n\nCode\nmi &lt;- round(moranFromSF(results_by_county$no_pooling_pred, results_by_county), 2)\n\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\n\nCode\ng &lt;- ggplot(results_by_county) +\n  geom_sf(aes(fill = no_pooling_pred)) +\n  scale_fill_viridis_c() +\n  ggtitle(paste0(\"No pooling predictions with I=\", mi))\nplot(g)\n\n\n\n\n\n\n\n\n\nI love this model! It’s perfect! It captures almost the exact same clustering and spatial patterning of risk as the original data.\n\n\n\n\n\n\nDanger!\n\n\n\nWhat is problematic about this model? What limits its usefulness for both interpretation and prediction?\n\n\n\n\nPartial Pooling\nWhen we look at the predictions of the partial pooling model, they are notably smoother and more clustered than those of the full- and no-pooling models:\n\n\nCode\nmi &lt;- round(moranFromSF(results_by_county$partial_pooling_pred, results_by_county), 2)\n\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\n\nCode\ng &lt;- ggplot(results_by_county) +\n  geom_sf(aes(fill = partial_pooling_pred)) +\n  scale_fill_viridis_c() +\n  ggtitle(paste0(\"Partial pooling predictions with I=\", mi))\nplot(g)\n\n\n\n\n\n\n\n\n\nIf we compare this pattern and intensity of clustering to the log-uranium data, it is clear that the smoothness in the model predictions reflects the relative smoothness and clustering of the soil uranium data:\n\n\nCode\nmi &lt;- round(moranFromSF(minnesota_radon$log_uranium, minnesota_radon), 2)\n\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\n\nCode\ng &lt;- ggplot(minnesota_radon) +\n  geom_sf(aes(fill = log_uranium)) +\n  scale_fill_viridis_c() +\n  ggtitle(paste0(\"Soil uranium by MN county (I = \", mi, \")\"))\nplot(g)\n\n\n\n\n\n\n\n\n\nWhen we look at the residuals, they are still quite un-clustered - similar to the no pooling model, but their magnitude is larger, suggesting that the multi-level model is less suceptible to overfitting:\n\n\nCode\nmi &lt;- round(moranFromSF(results_by_county$partial_pooling_resid, results_by_county), 2)\n\n\nWarning in lag.listw(listw, z, zero.policy = zero.policy, NAOK = NAOK): NAs in\nlagged values\n\n\nCode\ng &lt;- ggplot(results_by_county) +\n  geom_sf(aes(fill = partial_pooling_resid)) +\n  scale_fill_viridis_c() +\n  ggtitle(paste0(\"Partial pooling residuals with I=\", mi))\nplot(g)\n\n\n\n\n\n\n\n\n\nBy contrast, the aggregated residuals at the county level are less indicative of overfitting than the no-pooling model, but are still a bit fat-tailed, suggesting that some counties may still be over- or under-fit.\n\n\nCode\ng &lt;- ggplot() +\n  geom_histogram(aes(x = results_by_county$partial_pooling_resid))\nplot(g)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s next?\nIn this tutorial, we have thoroughly reviewed the spatial implications of the three types of models reviewed in the (gelman2006?) analysis of household-level radon in Minnesota. While our results suggest that the partial-pooling model provides the most compelling explanation of spatial variability in our data, we are not done yet! In the next tutorial, we will look specifically at the predictive capabilities of each of these models and use the ability to predict risk for counties in which household-level measures are unavaialble or missing as the final guide in our odyssey of model comparison.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multi-level modeling</span>"
    ]
  },
  {
    "objectID": "radon-multilevel.html#references",
    "href": "radon-multilevel.html#references",
    "title": "9  Multi-level modeling",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multi-level modeling</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Achen, Christopher H. 2005. “Let’s Put Garbage-Can\nRegressions and Garbage-Can Probits Where They\nBelong.” Conflict Management and Peace Science 22\n(4): 327–39. https://doi.org/10.1080/07388940500339167.\n\n\nChowkwanyun, Merlin, and Adolph L. Reed. 2020. “Racial\nHealth Disparities and Covid-19 —\nCaution and Context.” New England\nJournal of Medicine 383 (3): 201–3. https://doi.org/10.1056/NEJMp2012910.\n\n\nGetis, Arthur, and J. K. Ord. 2010. “The Analysis of\nSpatial Association by Use of Distance\nStatistics.” Geographical Analysis 24 (3):\n189–206. https://doi.org/10.1111/j.1538-4632.1992.tb00261.x.\n\n\nGoodchild, Michael F. 2004. “The Validity and\nUsefulness of Laws in Geographic\nInformation Science and Geography.”\nAnnals of the Association of American Geographers 94 (2):\n300–303. https://doi.org/10.1111/j.1467-8306.2004.09402008.x.\n\n\nGoodreau, Steven M, James A Kitts, and Martina Morris. 2009.\n“Birds of a Feather, or Friend of a Friend? Using\nExponential Random Graph Models to Investigate Adolescent Social\nNetworks.” Demography 46 (1): 103–25. https://doi.org/10.1353/dem.0.0045.\n\n\nGriffith, Daniel A. 2023. “Spatial Autocorrelation Mixtures in\nGeospatial Disease Data: An Important Global\nEpidemiologic/Public Health Assessment Ingredient?”\nTransactions in GIS 27 (3): 730–51. https://doi.org/10.1111/tgis.13042.\n\n\nKershaw, Kiarri N., Whitney R. Robinson, Penny Gordon-Larsen, Margaret\nT. Hicken, David C. Goff, Mercedes R. Carnethon, Catarina I. Kiefe,\nStephen Sidney, and Ana V. Diez Roux. 2017. “Association of\nChanges in Neighborhood-Level Racial Residential\nSegregation With Changes in Blood Pressure Among Black\nAdults: The CARDIA Study.” JAMA Internal\nMedicine 177 (7): 996–1002. https://doi.org/10.1001/jamainternmed.2017.1226.\n\n\nLegendre, Pierre. 1993. “Spatial Autocorrelation:\nTrouble or New Paradigm?”\nEcology 74 (6): 1659–73. https://doi.org/10.2307/1939924.\n\n\nMiller, Harvey J. 2004. “Tobler’s First Law and\nSpatial Analysis.” Annals of the Association of\nAmerican Geographers 94 (2): 284–89. https://doi.org/10.1111/j.1467-8306.2004.09402005.x.\n\n\nRoberts, Samuel K. 2009. Infectious Fear:\nPolitics, Disease, and the Health\nEffects of Segregation. University of North\nCarolina Press.\n\n\nRosenberg, Charles E., ed. 1992. “Explaining Epidemics.” In\nExplaining Epidemics, 293–304. Cambridge:\nCambridge University Press. https://doi.org/10.1017/CBO9780511666865.015.\n\n\nSayer, Andrew. 2010. Method in Social Science:\nRevised 2nd Edition. 2nd ed. London:\nRoutledge. https://doi.org/10.4324/9780203850374.\n\n\nSusewind, Raphael. 2017. “Muslims in Indian Cities:\nDegrees of Segregation and the Elusive Ghetto.”\nEnvironment and Planning A: Economy and Space 49 (6):\n1286–1307. https://doi.org/10.1177/0308518X17696071.\n\n\nTobler, W. R. 1970. “A Computer Movie Simulating Urban\nGrowth in the Detroit Region.” Economic\nGeography 46: 234–40. https://doi.org/10.2307/143141.\n\n\nWacquant, Loïc. 2007. “Territorial Stigmatization in\nthe Age of Advanced Marginality.”\nThesis Eleven 91 (1): 66–77. https://doi.org/10.1177/0725513607082003.",
    "crumbs": [
      "Tools",
      "References"
    ]
  }
]